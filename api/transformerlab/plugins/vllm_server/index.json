{
  "name": "vLLM Server",
  "uniqueId": "vllm_server",
  "description": "vLLM is a fast and easy-to-use library for LLM inference and serving.",
  "plugin-format": "python",
  "type": "loader",
  "version": "1.0.26",
  "supports": [
    "chat",
    "completion",
    "rag",
    "tools",
    "template",
    "embeddings",
    "batched",
    "multimodal"
  ],
  "model_architectures": [
    "CohereForCausalLM",
    "Exaone4ForCausalLM",
    "FalconForCausalLM",
    "GemmaForCausalLM",
    "GPTBigCodeForCausalLM",
    "LlamaForCausalLM",
    "MistralForCausalLM",
    "MixtralForCausalLM",
    "PhiForCausalLM",
    "Phi3ForCausalLM",
    "Qwen2ForCausalLM",
    "Qwen2_5_VLForConditionalGeneration",
    "Qwen2VLForConditionalGeneration",
    "Gemma3ForConditionalGeneration",
    "PaliGemmaForConditionalGeneration",
    "DeepseekV2ForCausalLM",
    "LlavaLlamaForCausalLM",
    "LlavaQwenForCausalLM",
    "Qwen3VLForConditionalGeneration",
    "Qwen3ForCausalLM",
    "Glm4MoeForCausalLM"
  ],
  "supported_hardware_architectures": ["cuda"],
  "files": ["main.py", "setup.sh"],
  "setup-script": "setup.sh",
  "parameters": {
    "port": {
      "title": "Server Port",
      "type": "integer",
      "default": 8000
    },
    "max_model_len": {
      "title": "Max Model Length",
      "type": "integer",
      "default": 0,
      "minimum": 0
    },
    "pipeline_parallel_size": {
      "title": "Pipeline Parallel Size",
      "type": "integer",
      "default": 1,
      "minimum": 1
    }
  },
  "parameters_ui": {
    "max_model_len": {
      "ui:help": "Max model length to use for the model. If 0, the model will use the default max length."
    },
    "pipeline_parallel_size": {
      "ui:help": "Number of chunks of model layers divided across gpus/nodes for parallel inference. Default is 1 (no pipeline parallelism)."
    }
  }
}
