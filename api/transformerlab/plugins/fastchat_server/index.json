{
  "name": "Fastchat Server",
  "uniqueId": "fastchat_server",
  "description": "Fastchat loads models for inference using Huggingface Transformers for generation.",
  "plugin-format": "python",
  "type": "loader",
  "version": "1.0.31",
  "supports": [
    "chat",
    "completion",
    "visualize_model",
    "model_layers",
    "rag",
    "tools",
    "template",
    "embeddings",
    "tokenize",
    "logprobs",
    "batched"
  ],
  "model_architectures": [
    "CohereForCausalLM",
    "FalconForCausalLM",
    "GemmaForCausalLM",
    "Gemma2ForCausalLM",
    "GPTBigCodeForCausalLM",
    "GraniteForCausalLM",
    "LlamaForCausalLM",
    "MistralForCausalLM",
    "MixtralForCausalLM",
    "PhiForCausalLM",
    "Phi3ForCausalLM",
    "Qwen2ForCausalLM",
    "ExaoneForCausalLM",
    "T5ForConditionalGeneration",
    "Gemma3ForCausalLM",
    "Gemma3ForConditionalGeneration",
    "Gemma3nForConditionalGeneration",
    "AprielForCausalLM",
    "GPTNeoXForCausalLM",
    "Qwen3ForCausalLM",
    "Qwen3MoeForCausalLM",
    "Ernie4_5_MoeForCausalLM",
    "Ernie4_5_ForCausalLM",
    "SmolLM3ForCausalLM",
    "GptOssForCausalLM",
    "RForConditionalGeneration",
    "Lfm2ForCausalLM"
  ],
  "supported_hardware_architectures": ["cpu", "cuda", "mlx", "amd"],
  "files": ["main.py", "setup.sh"],
  "setup-script": "setup.sh",
  "parameters": {
    "gpu_ids": {
      "title": "GPU IDs to use for Inference. Leaving blank will use all available GPUs",
      "type": "string",
      "default": ""
    },
    "load_compressed": {
      "title": "Load compressed model",
      "type": "string",
      "default": "None",
      "enum": ["None", "8-bit", "4-bit"]
    },
    "model_dtype": {
      "title": "Select a specific data type for the model",
      "type": "string",
      "enum": ["auto", "float16", "bfloat16", "float32"]
    },
    "gpu_split_strategy": {
      "title": "GPU split strategy",
      "type": "string",
      "default": "Auto (VRAM)",
      "enum": ["Auto (VRAM)", "Split evenly", "Custom max GPU memory"]
    },
    "max_gpu_memory": {
      "title": "Max GPU memory per GPU (e.g. 20GiB)",
      "type": "string",
      "default": ""
    }
  },
  "parameters_ui": {
    "gpu_ids": {
      "ui:help": "Specify a comma-separated list of GPU IDs to use for inference. The IDs for each GPU can be found in the Computer tab. For example: 0,1,2,3"
    },
    "model_dtype": {
      "ui:help": "Select a specific data type for the model. This might help with older GPUs that do not support bfloat16"
    },
    "gpu_split_strategy": {
      "ui:help": "Auto uses available VRAM. Split evenly caps each GPU to the smallest VRAM. Custom uses the Max GPU memory value."
    },
    "max_gpu_memory": {
      "ui:help": "Only used when GPU split strategy is Custom. Example: 20GiB"
    }
  }
}
