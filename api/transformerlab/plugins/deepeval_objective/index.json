{
  "name": "Objective Metrics",
  "uniqueId": "deepeval_objective",
  "description": "Evaluating outputs of LLMs using objective metrics",
  "plugin-format": "python",
  "type": "evaluator",
  "evalsType": "dataset",
  "version": "0.2.14",
  "git": "https://github.com/confident-ai/deepeval",
  "url": "https://github.com/confident-ai/deepeval",
  "files": [
    "main.py",
    "setup.sh"
  ],
  "supported_hardware_architectures": [
    "cpu",
    "cuda",
    "mlx",
    "amd"
  ],
  "_dataset": true,
  "_dataset_display_message": "Please upload a dataset file with columns: 'input', 'output', 'expected_output'. The context column is optional if using metrics which don't require it.",
  "setup-script": "setup.sh",
  "parameters": {
    "tasks": {
      "title": "Evaluation Metric",
      "type": "string",
      "enum": [
        "Rouge",
        "BLEU",
        "Exact Match",
        "Quasi Exact Match",
        "Quasi Contains",
        "BERT Score"
      ]
    },
    "limit": {
      "title": "Fraction of samples to evaluate",
      "type": [
        "number"
      ],
      "minimum": 0.0,
      "default": 1.0,
      "maximum": 1.0,
      "multipleOf": 0.1
    },
    "dataset_split": {
      "title": "Dataset Split",
      "type": "string",
      "default": "train"
    }
  },
  "parameters_ui": {
    "metrics": {
      "ui:help": "Select an evaluation metric from the drop-down list"
    },
    "limit": {
      "ui:help": "Enter a fraction of samples to evaluate from your data. Set as 1 to get all samples",
      "ui:widget": "RangeWidget"
    },
    "tasks": {
      "ui:help": "Select an evaluation metric from the drop-down list",
      "ui:widget": "AutoCompleteWidget"
    }
  }
}
