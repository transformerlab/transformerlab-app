[
    {
        "name": "exaone",
        "description": "Multimodal foundation, designed for enterprise-scale reasoning across language, vision, and code domains.",
        "tags": [
            "Multimodal",
            "Multilingual",
            "Reasoning"
        ],
        "models": [
            {
                "uniqueID": "LGAI-EXAONE/EXAONE-Deep-2.4B",
                "name": "LGAI EXAONE Deep 2.4B",
                "description": "EXAONE Deep exhibits superior capabilities in various reasoning tasks including math and coding benchmarks, ranging from 2.4B to 32B parameters developed and released by LG AI Research",
                "parameters": "2.4B",
                "context": "32768",
                "architecture": "ExaoneForCausalLM",
                "added": "2025-03-18",
                "tags": [
                    "Reasoning"
                ],
                "formats": [
                    "Safetensors"
                ],
                "huggingface_repo": "LGAI-EXAONE/EXAONE-Deep-2.4B",
                "transformers_version": "4.43.1",
                "gated": false,
                "license": "other",
                "logo": "https://huggingface.co/LGAI-EXAONE/EXAONE-Deep-2.4B/resolve/main/assets/EXAONE_Symbol+BI_3d.png",
                "size_of_model_in_mb": 4595.726956367493,
                "author": {
                    "name": "LGAI-EXAONE",
                    "url": "https://huggingface.co/LGAI-EXAONE/EXAONE-Deep-2.4B",
                    "blurb": ""
                },
                "resources": {
                    "canonicalUrl": "https://huggingface.co/LGAI-EXAONE/EXAONE-Deep-2.4B",
                    "downloadUrl": "https://huggingface.co/LGAI-EXAONE/EXAONE-Deep-2.4B",
                    "paperUrl": "?"
                },
                "model_group": "exaone"
            },
            {
                "uniqueID": "LGAI-EXAONE/EXAONE-Deep-7.8B",
                "name": "LGAI EXAONE Deep 7.8B",
                "description": "EXAONE Deep exhibits superior capabilities in various reasoning tasks including math and coding benchmarks, ranging from 2.4B to 32B parameters developed and released by LG AI Research",
                "parameters": "",
                "context": "32768",
                "architecture": "ExaoneForCausalLM",
                "added": "2025-03-18",
                "tags": [
                    "Reasoning"
                ],
                "formats": [
                    "Safetensors"
                ],
                "huggingface_repo": "LGAI-EXAONE/EXAONE-Deep-7.8B",
                "transformers_version": "4.43.1",
                "gated": false,
                "license": "other",
                "logo": "https://huggingface.co/LGAI-EXAONE/EXAONE-Deep-2.4B/resolve/main/assets/EXAONE_Symbol+BI_3d.png",
                "size_of_model_in_mb": 14920.461790084839,
                "author": {
                    "name": "LGAI-EXAONE",
                    "url": "https://huggingface.co/LGAI-EXAONE/EXAONE-Deep-7.8B",
                    "blurb": ""
                },
                "resources": {
                    "canonicalUrl": "https://huggingface.co/LGAI-EXAONE/EXAONE-Deep-7.8B",
                    "downloadUrl": "https://huggingface.co/LGAI-EXAONE/EXAONE-Deep-7.8B",
                    "paperUrl": "?"
                },
                "model_group": "exaone"
            },
            {
                "uniqueID": "LGAI-EXAONE/EXAONE-Deep-32B",
                "name": "LGAI EXAONE Deep 32B",
                "description": "EXAONE Deep exhibits superior capabilities in various reasoning tasks including math and coding benchmarks, ranging from 2.4B to 32B parameters developed and released by LG AI Research",
                "parameters": "32B",
                "context": "32768",
                "architecture": "ExaoneForCausalLM",
                "added": "2025-03-18",
                "tags": [
                    "Reasoning"
                ],
                "formats": [
                    "Safetensors"
                ],
                "huggingface_repo": "LGAI-EXAONE/EXAONE-Deep-32B",
                "transformers_version": "4.43.1",
                "gated": false,
                "license": "other",
                "logo": "https://huggingface.co/LGAI-EXAONE/EXAONE-Deep-2.4B/resolve/main/assets/EXAONE_Symbol+BI_3d.png",
                "size_of_model_in_mb": 61049.268359184265,
                "author": {
                    "name": "LGAI-EXAONE",
                    "url": "https://huggingface.co/LGAI-EXAONE/EXAONE-Deep-32B",
                    "blurb": ""
                },
                "resources": {
                    "canonicalUrl": "https://huggingface.co/LGAI-EXAONE/EXAONE-Deep-32B",
                    "downloadUrl": "https://huggingface.co/LGAI-EXAONE/EXAONE-Deep-32B",
                    "paperUrl": "?"
                },
                "model_group": "exaone"
            }
        ]
    },
    {
        "name": "vicuna",
        "description": "Fine-tuned and conversational. Derived from the Alpaca and LLaMA line.",
        "tags": [],
        "models": [
            {
                "uniqueID": "lmsys/vicuna-7b-v1.5",
                "name": "Vicuna 7b",
                "description": "Vicuna is a state-of-the-art chat assistant developed by LMSYS, leveraging the transformative power of an auto-regressive language model based on the transformer architecture. Trained via fine-tuning the Llama 2 model, Vicuna is used primarily for research in large language models and chatbots, catering to the needs of AI enthusiasts, machine learning researchers, and hobbyists. The model operates under the Llama 2 CLA. The latest version, Vicuna v1.5 (16k), has been fine-tuned using supervised instruction and linear RoPE scaling, with training data comprising around 125K conversations collected from ShareGPT.com, packed into sequences of 16K tokens each. A comprehensive explanation of the training details can be found in the appendix to the linked paper titled \"Training Details of Vicuna Models.\"",
                "parameters": "7B",
                "context": "4k",
                "architecture": "LlamaForCausalLM",
                "formats": [
                    "PyTorch"
                ],
                "huggingface_repo": "lmsys/vicuna-7b-v1.5",
                "transformers_version": "4.31.0",
                "gated": false,
                "license": "Llama 2 CLA",
                "logo": "https://lmsys.org/images/blog/vicuna/vicuna.jpeg",
                "size_of_model_in_mb": 12853.13,
                "author": {
                    "name": "LMSYS Org",
                    "url": "https://lmsys.org/",
                    "blurb": "Large Model Systems Organization (LMSYS Org) is an open research organization founded by students and faculty from UC Berkeley in collaboration with UCSD and CMU."
                },
                "resources": {
                    "canonicalUrl": "https://huggingface.co/lmsys/vicuna-7b-v1.5",
                    "downloadUrl": "https://huggingface.co/lmsys/vicuna-7b-v1.5",
                    "paperUrl": "https://arxiv.org/abs/2306.05685"
                },
                "model_group": "vicuna"
            },
            {
                "uniqueID": "lmsys/vicuna-13b-v1.5",
                "name": "Vicuna 13b",
                "description": "Vicuna is a state-of-the-art chat assistant developed by LMSYS, leveraging the transformative power of an auto-regressive language model based on the transformer architecture. Trained via fine-tuning the Llama 2 model, Vicuna is used primarily for research in large language models and chatbots, catering to the needs of AI enthusiasts, machine learning researchers, and hobbyists. The model operates under the Llama 2 CLA. The latest version, Vicuna v1.5 (16k), has been fine-tuned using supervised instruction and linear RoPE scaling, with training data comprising around 125K conversations collected from ShareGPT.com, packed into sequences of 16K tokens each. A comprehensive explanation of the training details can be found in the appendix to the linked paper titled \"Training Details of Vicuna Models.\"",
                "parameters": "13B",
                "context": "4k",
                "architecture": "LlamaForCausalLM",
                "formats": [
                    "PyTorch"
                ],
                "huggingface_repo": "lmsys/vicuna-13b-v1.5",
                "transformers_version": "4.31.0",
                "gated": false,
                "license": "Llama 2 CLA",
                "logo": "https://lmsys.org/images/blog/vicuna/vicuna.jpeg",
                "size_of_model_in_mb": 24826.44,
                "author": {
                    "name": "LMSYS Org",
                    "url": "https://lmsys.org/",
                    "blurb": "Large Model Systems Organization (LMSYS Org) is an open research organization founded by students and faculty from UC Berkeley in collaboration with UCSD and CMU."
                },
                "resources": {
                    "canonicalUrl": "https://huggingface.co/lmsys/vicuna-13b-v1.5",
                    "downloadUrl": "https://huggingface.co/lmsys/vicuna-13b-v1.5",
                    "paperUrl": "https://arxiv.org/abs/2306.05685"
                },
                "model_group": "vicuna"
            },
            {
                "uniqueID": "lmsys/vicuna-7b-v1.5-16k",
                "name": "Vicuna 7b - 16k",
                "description": "Vicuna is a state-of-the-art chat assistant developed by LMSYS, leveraging the transformative power of an auto-regressive language model based on the transformer architecture. Trained via fine-tuning the Llama 2 model, Vicuna is used primarily for research in large language models and chatbots, catering to the needs of AI enthusiasts, machine learning researchers, and hobbyists. The model operates under the Llama 2 CLA. The latest version, Vicuna v1.5 (16k), has been fine-tuned using supervised instruction and linear RoPE scaling, with training data comprising around 125K conversations collected from ShareGPT.com, packed into sequences of 16K tokens each. A comprehensive explanation of the training details can be found in the appendix to the linked paper titled \"Training Details of Vicuna Models.\"",
                "parameters": "7B",
                "context": "16k",
                "architecture": "LlamaForCausalLM",
                "formats": [
                    "PyTorch"
                ],
                "huggingface_repo": "lmsys/vicuna-7b-v1.5-16k",
                "transformers_version": "4.31.0",
                "gated": false,
                "license": "Llama 2 CLA",
                "logo": "https://lmsys.org/images/blog/vicuna/vicuna.jpeg",
                "size_of_model_in_mb": 12853.13,
                "author": {
                    "name": "LMSYS Org",
                    "url": "https://lmsys.org/",
                    "blurb": "Large Model Systems Organization (LMSYS Org) is an open research organization founded by students and faculty from UC Berkeley in collaboration with UCSD and CMU."
                },
                "resources": {
                    "canonicalUrl": "https://huggingface.co/lmsys/vicuna-7b-v1.5-16k",
                    "downloadUrl": "https://huggingface.co/lmsys/vicuna-7b-v1.5-16k",
                    "paperUrl": "https://arxiv.org/abs/2306.05685"
                },
                "model_group": "vicuna"
            }
        ]
    },
    {
        "name": "watt",
        "description": "Specialized for tool use and dynamic plugin-based reasoning tasks.",
        "tags": [
            "Tool Use"
        ],
        "models": [
            {
                "uniqueID": "watt-ai/watt-tool-8B",
                "name": "Watt Tool Use 8B",
                "description": "Leading tool use model based on Llama architecture.",
                "parameters": "8B",
                "added": "2025-01-28",
                "tags": [
                    "Tool Use"
                ],
                "context": "131072",
                "architecture": "LlamaForCausalLM",
                "formats": [
                    "Safetensors"
                ],
                "huggingface_repo": "watt-ai/watt-tool-8B",
                "transformers_version": "4.45.2",
                "gated": false,
                "license": "apache-2.0",
                "logo": "",
                "size_of_model_in_mb": 15325.28,
                "author": {
                    "name": "watt-ai",
                    "url": "https://huggingface.co/watt-ai/watt-tool-8B",
                    "blurb": ""
                },
                "resources": {
                    "canonicalUrl": "https://huggingface.co/watt-ai/watt-tool-8B",
                    "downloadUrl": "https://huggingface.co/watt-ai/watt-tool-8B",
                    "paperUrl": "?"
                },
                "model_group": "watt"
            }
        ]
    },
    {
        "name": "aya",
        "description": "Instruction-tuned and multilingual, supporting 23 languages with strong general-purpose capabilities.",
        "tags": [
            "Multilingual"
        ],
        "models": [
            {
                "uniqueID": "CohereForAI/aya-23-8B",
                "name": "Aya 23 8B",
                "description": "Aya 23 is an open weights research release of an instruction fine-tuned model with highly advanced multilingual capabilities. Aya 23 focuses on pairing a highly performant pre-trained Command family of models with the recently released Aya Collection. The result is a powerful multilingual large language model serving 23 languages.",
                "parameters": "8B",
                "context": "8192",
                "architecture": "CohereForCausalLM",
                "formats": [
                    "PyTorch"
                ],
                "huggingface_repo": "CohereForAI/aya-23-8B",
                "transformers_version": "4.40.0.dev0",
                "gated": true,
                "license": "CC-By-NC-4.0",
                "logo": "https://cdn.sanity.io/images/rjtqmwfu/production/5bc885878e9f0f1e202b742c86f543c7fe765376-409x144.svg",
                "size_of_model_in_mb": 15328.09,
                "model_group": "aya"
            }
        ]
    },
    {
        "name": "openllama",
        "description": "Community-built open version of the LLaMA architecture for research and development.",
        "tags": [],
        "models": [
            {
                "uniqueID": "openlm-research/open_llama_3b_v2",
                "name": "Open LLama 3b v2",
                "archived": true,
                "description": " we are releasing our public preview of OpenLLaMA, a permissively licensed open source reproduction of Meta AI\u2019s LLaMA. We are releasing a series of 3B, 7B and 13B models trained on different data mixtures. Our model weights can serve as the drop in replacement of LLaMA in existing implementations.",
                "parameters": "3B",
                "context": "4k",
                "architecture": "LlamaForCausalLM",
                "formats": [
                    "PyTorch"
                ],
                "huggingface_repo": "openlm-research/open_llama_3b_v2",
                "transformers_version": "4.31.0.dev0",
                "gated": false,
                "license": "Apache 2.0",
                "logo": "https://avatars.githubusercontent.com/u/132110378?s=48&v=4",
                "size_of_model_in_mb": 6536.06,
                "author": {
                    "name": "OpenLLaMA",
                    "url": "https://github.com/openlm-research/open_llama",
                    "blurb": ""
                },
                "resources": {
                    "canonicalUrl": "https://huggingface.co/openlm-research/open_llama_3b",
                    "downloadUrl": "https://huggingface.co/openlm-research/open_llama_3b",
                    "paperUrl": "?"
                },
                "model_group": "openllama"
            },
            {
                "uniqueID": "openlm-research/open_llama_7b_v2",
                "name": "Open LLama 7b v2",
                "archived": true,
                "description": " we are releasing our public preview of OpenLLaMA, a permissively licensed open source reproduction of Meta AI\u2019s LLaMA. We are releasing a series of 3B, 7B and 13B models trained on different data mixtures. Our model weights can serve as the drop in replacement of LLaMA in existing implementations.",
                "parameters": "7B",
                "context": "4k",
                "architecture": "LlamaForCausalLM",
                "formats": [
                    "PyTorch"
                ],
                "huggingface_repo": "openlm-research/open_llama_7b_v2",
                "transformers_version": "4.31.0.dev0",
                "gated": false,
                "license": "Apache 2.0",
                "logo": "https://avatars.githubusercontent.com/u/132110378?s=48&v=4",
                "size_of_model_in_mb": 12853.14,
                "author": {
                    "name": "OpenLLaMA",
                    "url": "https://github.com/openlm-research/open_llama",
                    "blurb": ""
                },
                "resources": {
                    "canonicalUrl": "https://huggingface.co/openlm-research/open_llama_3b",
                    "downloadUrl": "https://huggingface.co/openlm-research/open_llama_3b",
                    "paperUrl": "?"
                },
                "model_group": "openllama"
            }
        ]
    },
    {
        "name": "gemma",
        "description": "Compact, research-focused language model developed for experimental scalability.",
        "tags": [],
        "models": [
            {
                "uniqueID": "unsloth/gemma-3-1b-it-GGUF",
                "name": "Gemma 3 1B Instruct (GGUF Q8_0)",
                "description": "GGUF-formatted 8-bit quantization of Google's Gemma 3 1B instruction-tuned model.",
                "added": "2025-03-25",
                "parameters": "1B",
                "context": "32768",
                "architecture": "GGUF",
                "formats": [
                    "GGUF"
                ],
                "tags": [
                    "Tool Use"
                ],
                "huggingface_repo": "unsloth/gemma-3-1b-it-GGUF",
                "huggingface_filename": "gemma-3-1b-it-Q8_0.gguf",
                "transformers_version": "4.50.0.dev0",
                "gated": "manual",
                "license": "gemma",
                "logo": "https://storage.googleapis.com/gweb-uniblog-publish-prod/images/gemma-header.width-1200.format-webp.webp",
                "size_of_model_in_mb": 1095.68,
                "author": {
                    "name": "unsloth",
                    "url": "https://huggingface.co/unsloth/gemma-3-1b-it-GGUF",
                    "blurb": ""
                },
                "resources": {
                    "canonicalUrl": "https://huggingface.co/unsloth/gemma-3-1b-it-GGUF",
                    "downloadUrl": "https://huggingface.co/unsloth/gemma-3-1b-it-GGUF",
                    "paperUrl": "?"
                },
                "model_group": "gemma"
            },
            {
                "uniqueID": "unsloth/gemma-3-27b-it-GGUF",
                "name": "Gemma 3 27B Instruct (GGUF Q4_K_M)",
                "description": "GGUF-formatted 4-bit quantization of Google's Gemma 3 27B instruction-tuned model.",
                "added": "2025-03-25",
                "tags": [
                    "Tool Use"
                ],
                "parameters": "27B",
                "context": "2048",
                "architecture": "GGUF",
                "formats": [
                    "GGUF"
                ],
                "huggingface_repo": "unsloth/gemma-3-27b-it-GGUF",
                "huggingface_filename": "gemma-3-27b-it-Q4_K_M.gguf",
                "transformers_version": "4.50.0.dev0",
                "gated": false,
                "license": "gemma",
                "logo": "https://storage.googleapis.com/gweb-uniblog-publish-prod/images/gemma-header.width-1200.format-webp.webp",
                "size_of_model_in_mb": 16896.0,
                "author": {
                    "name": "unsloth",
                    "url": "https://huggingface.co/unsloth/gemma-3-27b-it-GGUF",
                    "blurb": ""
                },
                "resources": {
                    "canonicalUrl": "https://huggingface.co/unsloth/gemma-3-27b-it-GGUF",
                    "downloadUrl": "https://huggingface.co/unsloth/gemma-3-27b-it-GGUF",
                    "paperUrl": "?"
                },
                "model_group": "gemma"
            },
            {
                "uniqueID": "google/gemma-7b",
                "name": "Gemma 7B",
                "archived": true,
                "description": "Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models. They are text-to-text, decoder-only large language models, available in English, with open weights, pre-trained variants, and instruction-tuned variants. Gemma models are well-suited for a variety of text generation tasks, including question answering, summarization, and reasoning. Their relatively small size makes it possible to deploy them in environments with limited resources such as a laptop, desktop or your own cloud infrastructure, democratizing access to state of the art AI models and helping foster innovation for everyone.",
                "parameters": "7B",
                "context": "8192",
                "architecture": "GemmaForCausalLM",
                "formats": [
                    "Safetensors"
                ],
                "huggingface_repo": "google/gemma-7b",
                "transformers_version": "4.38.0.dev0",
                "license": "Gemma",
                "gated": "manual",
                "logo": "https://storage.googleapis.com/gweb-uniblog-publish-prod/images/gemma-header.width-1200.format-webp.webp",
                "size_of_model_in_mb": 16305.17,
                "author": {
                    "name": "Google",
                    "url": "https://huggingface.co/google/gemma-7b",
                    "blurb": ""
                },
                "resources": {
                    "canonicalUrl": "https://huggingface.co/google/gemma-7b",
                    "downloadUrl": "https://huggingface.co/google/gemma-7b",
                    "paperUrl": "?"
                },
                "model_group": "gemma"
            },
            {
                "uniqueID": "google/gemma-7b-it",
                "name": "Gemma 7B Instruct",
                "archived": true,
                "description": "Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models. They are text-to-text, decoder-only large language models, available in English, with open weights, pre-trained variants, and instruction-tuned variants. Gemma models are well-suited for a variety of text generation tasks, including question answering, summarization, and reasoning. Their relatively small size makes it possible to deploy them in environments with limited resources such as a laptop, desktop or your own cloud infrastructure, democratizing access to state of the art AI models and helping foster innovation for everyone.",
                "parameters": "7B",
                "context": "8192",
                "architecture": "GemmaForCausalLM",
                "formats": [
                    "Safetensors"
                ],
                "huggingface_repo": "google/gemma-7b-it",
                "transformers_version": "4.38.0.dev0",
                "license": "Gemma",
                "gated": "manual",
                "logo": "https://storage.googleapis.com/gweb-uniblog-publish-prod/images/gemma-header.width-1200.format-webp.webp",
                "size_of_model_in_mb": 16305.17,
                "author": {
                    "name": "Google",
                    "url": "https://huggingface.co/google/gemma-7b-it",
                    "blurb": ""
                },
                "resources": {
                    "canonicalUrl": "https://huggingface.co/google/gemma-7b-it",
                    "downloadUrl": "https://huggingface.co/google/gemma-7b-it",
                    "paperUrl": "?"
                },
                "model_group": "gemma"
            },
            {
                "uniqueID": "google/gemma-2b-it",
                "name": "Gemma 2B Instruct",
                "archived": true,
                "description": "Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models. They are text-to-text, decoder-only large language models, available in English, with open weights, pre-trained variants, and instruction-tuned variants. Gemma models are well-suited for a variety of text generation tasks, including question answering, summarization, and reasoning. Their relatively small size makes it possible to deploy them in environments with limited resources such as a laptop, desktop or your own cloud infrastructure, democratizing access to state of the art AI models and helping foster innovation for everyone.",
                "parameters": "2B",
                "context": "8192",
                "architecture": "GemmaForCausalLM",
                "formats": [
                    "Safetensors"
                ],
                "huggingface_repo": "google/gemma-2b-it",
                "transformers_version": "4.38.0.dev0",
                "license": "Gemma",
                "gated": "manual",
                "logo": "https://storage.googleapis.com/gweb-uniblog-publish-prod/images/gemma-header.width-1200.format-webp.webp",
                "size_of_model_in_mb": 4800.96,
                "author": {
                    "name": "Google",
                    "url": "https://huggingface.co/google/gemma-2b-it",
                    "blurb": ""
                },
                "resources": {
                    "canonicalUrl": "https://huggingface.co/google/gemma-2b-it",
                    "downloadUrl": "https://huggingface.co/google/gemma-2b-it",
                    "paperUrl": "?"
                },
                "model_group": "gemma"
            },
            {
                "uniqueID": "google/gemma-2b",
                "name": "Gemma 2B",
                "archived": true,
                "description": "Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models. They are text-to-text, decoder-only large language models, available in English, with open weights, pre-trained variants, and instruction-tuned variants. Gemma models are well-suited for a variety of text generation tasks, including question answering, summarization, and reasoning. Their relatively small size makes it possible to deploy them in environments with limited resources such as a laptop, desktop or your own cloud infrastructure, democratizing access to state of the art AI models and helping foster innovation for everyone.",
                "parameters": "2B",
                "context": "8192",
                "architecture": "GemmaForCausalLM",
                "formats": [
                    "Safetensors"
                ],
                "huggingface_repo": "google/gemma-2b",
                "transformers_version": "4.38.0.dev0",
                "license": "Gemma",
                "gated": "manual",
                "logo": "https://storage.googleapis.com/gweb-uniblog-publish-prod/images/gemma-header.width-1200.format-webp.webp",
                "size_of_model_in_mb": 4800.96,
                "author": {
                    "name": "Google",
                    "url": "https://huggingface.co/google/gemma-2b",
                    "blurb": ""
                },
                "resources": {
                    "canonicalUrl": "https://huggingface.co/google/gemma-2b",
                    "downloadUrl": "https://huggingface.co/google/gemma-2b",
                    "paperUrl": "?"
                },
                "model_group": "gemma"
            },
            {
                "uniqueID": "google/gemma-2-9b-it",
                "name": "Gemma2 9B Instruct",
                "archived": true,
                "description": "Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models. They are text-to-text, decoder-only large language models, available in English, with open weights for both pre-trained variants and instruction-tuned variants. Gemma models are well-suited for a variety of text generation tasks, including question answering, summarization, and reasoning. Their relatively small size makes it possible to deploy them in environments with limited resources such as a laptop, desktop or your own cloud infrastructure, democratizing access to state of the art AI models and helping foster innovation for everyone.",
                "parameters": "9B",
                "context": "8192",
                "architecture": "Gemma2ForCausalLM",
                "formats": [
                    "Safetensors"
                ],
                "huggingface_repo": "google/gemma-2-9b-it",
                "transformers_version": "4.42.0.dev0",
                "license": "Gemma",
                "gated": "manual",
                "logo": "https://storage.googleapis.com/gweb-uniblog-publish-prod/images/gemma-header.width-1200.format-webp.webp",
                "size_of_model_in_mb": 17648.05,
                "author": {
                    "name": "Google",
                    "url": "https://huggingface.co/google/gemma-2-9b-it",
                    "blurb": ""
                },
                "resources": {
                    "canonicalUrl": "https://huggingface.co/google/gemma-2-9b-it",
                    "downloadUrl": "https://huggingface.co/google/gemma-2-9b-it",
                    "paperUrl": "?"
                },
                "model_group": "gemma"
            },
            {
                "uniqueID": "google/gemma-2-27b-it",
                "name": "Gemma2 27B Instruct",
                "archived": true,
                "description": "Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models. They are text-to-text, decoder-only large language models, available in English, with open weights, pre-trained variants, and instruction-tuned variants. Gemma models are well-suited for a variety of text generation tasks, including question answering, summarization, and reasoning. Their relatively small size makes it possible to deploy them in environments with limited resources such as a laptop, desktop or your own cloud infrastructure, democratizing access to state of the art AI models and helping foster innovation for everyone.",
                "parameters": "27B",
                "context": "8192",
                "architecture": "Gemma2ForCausalLM",
                "formats": [
                    "Safetensors"
                ],
                "huggingface_repo": "google/gemma-2-27b-it",
                "gated": "manual",
                "transformers_version": "4.42.0.dev0",
                "license": "Gemma",
                "logo": "https://storage.googleapis.com/gweb-uniblog-publish-prod/images/gemma-header.width-1200.format-webp.webp",
                "size_of_model_in_mb": 51952.53,
                "author": {
                    "name": "Google",
                    "url": "https://huggingface.co/google/gemma-2-27b-it",
                    "blurb": ""
                },
                "resources": {
                    "canonicalUrl": "https://huggingface.co/google/gemma-2-27b-it",
                    "downloadUrl": "https://huggingface.co/google/gemma-2-27b-it",
                    "paperUrl": "?"
                },
                "model_group": "gemma"
            },
            {
                "uniqueID": "google/gemma-2-2b-it",
                "name": "Gemma2 2B Instruct",
                "archived": true,
                "description": "Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models. They are text-to-text, decoder-only large language models, available in English, with open weights for both pre-trained variants and instruction-tuned variants. Gemma models are well-suited for a variety of text generation tasks, including question answering, summarization, and reasoning. Their relatively small size makes it possible to deploy them in environments with limited resources such as a laptop, desktop or your own cloud infrastructure, democratizing access to state of the art AI models and helping foster innovation for everyone.",
                "parameters": "2B",
                "context": "8192",
                "architecture": "Gemma2ForCausalLM",
                "formats": [
                    "Safetensors"
                ],
                "huggingface_repo": "google/gemma-2-2b-it",
                "transformers_version": "4.42.4",
                "license": "Gemma",
                "gated": "manual",
                "logo": "https://storage.googleapis.com/gweb-uniblog-publish-prod/images/gemma-header.width-1200.format-webp.webp",
                "size_of_model_in_mb": 5007.32,
                "author": {
                    "name": "Google",
                    "url": "https://huggingface.co/google/gemma-2-2b-it",
                    "blurb": ""
                },
                "resources": {
                    "canonicalUrl": "https://huggingface.co/google/gemma-2-2b-it",
                    "downloadUrl": "https://huggingface.co/google/gemma-2-2b-it",
                    "paperUrl": "?"
                },
                "model_group": "gemma"
            },
            {
                "uniqueID": "mlx-community/gemma-3-1b-it-4bit",
                "name": "Gemma 3 1B Instruct (4bit MLX)",
                "description": "MLX formatted version of Google's Gemma 3 1B parameter instruction-trained model quantized to 4 bits.",
                "parameters": "1B",
                "context": "32768",
                "added": "2025-03-19",
                "architecture": "Gemma3ForCausalLM",
                "formats": [
                    "Safetensors"
                ],
                "tags": [
                    "Tool Use"
                ],
                "huggingface_repo": "mlx-community/gemma-3-1b-it-4bit",
                "transformers_version": "4.50.0.dev0",
                "gated": "manual",
                "license": "gemma",
                "logo": "https://ml-explore.github.io/mlx/build/html/_static/mlx_logo.png",
                "size_of_model_in_mb": 736.1,
                "author": {
                    "name": "mlx-community",
                    "url": "https://huggingface.co/mlx-community/gemma-3-1b-it-4bit",
                    "blurb": ""
                },
                "resources": {
                    "canonicalUrl": "https://huggingface.co/mlx-community/gemma-3-1b-it-4bit",
                    "downloadUrl": "https://huggingface.co/mlx-community/gemma-3-1b-it-4bit",
                    "paperUrl": "?"
                },
                "model_group": "gemma"
            },
            {
                "uniqueID": "mlx-community/gemma-3-12b-it-4bit",
                "name": "Gemma 3 12B Instruct (4bit MLX)",
                "description": "MLX formatted version of Google's Gemma 3 12B parameter instruction-trained model quantized to 4 bits.",
                "parameters": "12B",
                "context": "32768",
                "added": "2025-03-19",
                "architecture": "Gemma3ForConditionalGeneration",
                "formats": [
                    "Safetensors"
                ],
                "tags": [
                    "Tool Use"
                ],
                "huggingface_repo": "mlx-community/gemma-3-12b-it-4bit",
                "transformers_version": "4.50.0.dev0",
                "gated": "manual",
                "license": "gemma",
                "logo": "https://ml-explore.github.io/mlx/build/html/_static/mlx_logo.png",
                "size_of_model_in_mb": 7694.3,
                "author": {
                    "name": "mlx-community",
                    "url": "https://huggingface.co/mlx-community/gemma-3-12b-it-4bit",
                    "blurb": ""
                },
                "resources": {
                    "canonicalUrl": "https://huggingface.co/mlx-community/gemma-3-12b-it-4bit",
                    "downloadUrl": "https://huggingface.co/mlx-community/gemma-3-12b-it-4bit",
                    "paperUrl": "?"
                },
                "model_group": "gemma"
            },
            {
                "uniqueID": "google/gemma-3-1b-pt",
                "name": "Gemma 3 1B Base",
                "description": "",
                "parameters": "1B",
                "context": "128000",
                "added": "2025-03-19",
                "architecture": "Gemma3ForCausalLM",
                "formats": [
                    "Safetensors"
                ],
                "tags": [
                    "Tool Use"
                ],
                "huggingface_repo": "google/gemma-3-1b-pt",
                "transformers_version": "4.50.0.dev0",
                "gated": "manual",
                "license": "gemma",
                "logo": "",
                "size_of_model_in_mb": 1944.6060609817505,
                "author": {
                    "name": "google",
                    "url": "https://huggingface.co/google/gemma-3-1b-pt",
                    "blurb": ""
                },
                "resources": {
                    "canonicalUrl": "https://huggingface.co/google/gemma-3-1b-pt",
                    "downloadUrl": "https://huggingface.co/google/gemma-3-1b-pt",
                    "paperUrl": "?"
                },
                "model_group": "gemma"
            },
            {
                "uniqueID": "google/gemma-3-1b-it",
                "name": "Gemma 3 1B Instruct",
                "description": "",
                "parameters": "1B",
                "context": "128000",
                "added": "2025-03-19",
                "architecture": "Gemma3ForCausalLM",
                "formats": [
                    "Safetensors"
                ],
                "tags": [
                    "Tool Use"
                ],
                "huggingface_repo": "google/gemma-3-1b-it",
                "transformers_version": "4.50.0.dev0",
                "gated": "manual",
                "license": "gemma",
                "logo": "",
                "size_of_model_in_mb": 1944.6083211898804,
                "author": {
                    "name": "google",
                    "url": "https://huggingface.co/google/gemma-3-1b-it",
                    "blurb": ""
                },
                "resources": {
                    "canonicalUrl": "https://huggingface.co/google/gemma-3-1b-it",
                    "downloadUrl": "https://huggingface.co/google/gemma-3-1b-it",
                    "paperUrl": "?"
                },
                "model_group": "gemma"
            },
            {
                "uniqueID": "google/gemma-3-4b-pt",
                "name": "Gemma 3 4B Base",
                "description": "",
                "parameters": "",
                "context": "128000",
                "added": "2025-03-19",
                "architecture": "Gemma3ForConditionalGeneration",
                "formats": [
                    "Safetensors"
                ],
                "tags": [
                    "Tool Use"
                ],
                "huggingface_repo": "google/gemma-3-4b-pt",
                "transformers_version": "4.50.0.dev0",
                "gated": "manual",
                "license": "gemma",
                "logo": "",
                "size_of_model_in_mb": 8239.389072418213,
                "author": {
                    "name": "google",
                    "url": "https://huggingface.co/google/gemma-3-4b-pt",
                    "blurb": ""
                },
                "resources": {
                    "canonicalUrl": "https://huggingface.co/google/gemma-3-4b-pt",
                    "downloadUrl": "https://huggingface.co/google/gemma-3-4b-pt",
                    "paperUrl": "?"
                },
                "model_group": "gemma"
            },
            {
                "uniqueID": "google/gemma-3-4b-it",
                "name": "Gemma 3 4B Instruct",
                "description": "",
                "parameters": "4B",
                "context": "128000",
                "added": "2025-03-19",
                "architecture": "Gemma3ForConditionalGeneration",
                "formats": [
                    "Safetensors"
                ],
                "tags": [
                    "Tool Use"
                ],
                "huggingface_repo": "google/gemma-3-4b-it",
                "transformers_version": "4.50.0.dev0",
                "gated": "manual",
                "license": "gemma",
                "logo": "",
                "size_of_model_in_mb": 8239.393244743347,
                "author": {
                    "name": "google",
                    "url": "https://huggingface.co/google/gemma-3-4b-it",
                    "blurb": ""
                },
                "resources": {
                    "canonicalUrl": "https://huggingface.co/google/gemma-3-4b-it",
                    "downloadUrl": "https://huggingface.co/google/gemma-3-4b-it",
                    "paperUrl": "?"
                },
                "model_group": "gemma"
            },
            {
                "uniqueID": "google/gemma-3-12b-pt",
                "name": "Gemma 3 12B Base",
                "description": "",
                "parameters": "12B",
                "context": "128000",
                "added": "2025-03-19",
                "architecture": "Gemma3ForConditionalGeneration",
                "formats": [
                    "Safetensors"
                ],
                "tags": [
                    "Tool Use"
                ],
                "huggingface_repo": "google/gemma-3-12b-pt",
                "transformers_version": "4.50.0.dev0",
                "gated": "manual",
                "license": "gemma",
                "logo": "",
                "size_of_model_in_mb": 23283.15651702881,
                "author": {
                    "name": "google",
                    "url": "https://huggingface.co/google/gemma-3-12b-pt",
                    "blurb": ""
                },
                "resources": {
                    "canonicalUrl": "https://huggingface.co/google/gemma-3-12b-pt",
                    "downloadUrl": "https://huggingface.co/google/gemma-3-12b-pt",
                    "paperUrl": "?"
                },
                "model_group": "gemma"
            },
            {
                "uniqueID": "google/gemma-3-12b-it",
                "name": "Gemma 3 12B Instruct",
                "description": "",
                "parameters": "12B",
                "context": "128000",
                "added": "2025-03-19",
                "architecture": "Gemma3ForConditionalGeneration",
                "formats": [
                    "Safetensors"
                ],
                "tags": [
                    "Tool Use"
                ],
                "huggingface_repo": "google/gemma-3-12b-it",
                "transformers_version": "4.50.0.dev0",
                "gated": "manual",
                "license": "gemma",
                "logo": "",
                "size_of_model_in_mb": 23283.160661697388,
                "author": {
                    "name": "google",
                    "url": "https://huggingface.co/google/gemma-3-12b-it",
                    "blurb": ""
                },
                "resources": {
                    "canonicalUrl": "https://huggingface.co/google/gemma-3-12b-it",
                    "downloadUrl": "https://huggingface.co/google/gemma-3-12b-it",
                    "paperUrl": "?"
                },
                "model_group": "gemma"
            },
            {
                "uniqueID": "google/gemma-3-27b-pt",
                "name": "Gemma 3 27B Base",
                "description": "",
                "parameters": "27B",
                "context": "128000",
                "added": "2025-03-19",
                "architecture": "Gemma3ForConditionalGeneration",
                "formats": [
                    "Safetensors"
                ],
                "tags": [
                    "Tool Use"
                ],
                "huggingface_repo": "google/gemma-3-27b-pt",
                "transformers_version": "4.50.0.dev0",
                "gated": "manual",
                "license": "gemma",
                "logo": "",
                "size_of_model_in_mb": 52360.884380340576,
                "author": {
                    "name": "google",
                    "url": "https://huggingface.co/google/gemma-3-27b-pt",
                    "blurb": ""
                },
                "resources": {
                    "canonicalUrl": "https://huggingface.co/google/gemma-3-27b-pt",
                    "downloadUrl": "https://huggingface.co/google/gemma-3-27b-pt",
                    "paperUrl": "?"
                },
                "model_group": "gemma"
            },
            {
                "uniqueID": "google/gemma-3-27b-it",
                "name": "Gemma 3 27B Instruct",
                "description": "",
                "parameters": "27B",
                "context": "128000",
                "added": "2025-03-19",
                "architecture": "Gemma3ForConditionalGeneration",
                "formats": [
                    "Safetensors"
                ],
                "tags": [
                    "Tool Use"
                ],
                "huggingface_repo": "google/gemma-3-27b-it",
                "transformers_version": "4.50.0.dev0",
                "gated": "manual",
                "license": "gemma",
                "logo": "",
                "size_of_model_in_mb": 52360.88702392578,
                "author": {
                    "name": "google",
                    "url": "https://huggingface.co/google/gemma-3-27b-it",
                    "blurb": ""
                },
                "resources": {
                    "canonicalUrl": "https://huggingface.co/google/gemma-3-27b-it",
                    "downloadUrl": "https://huggingface.co/google/gemma-3-27b-it",
                    "paperUrl": "?"
                },
                "model_group": "gemma"
            },
            {
                "uniqueID": "google/shieldgemma-2-4b-it",
                "name": "ShieldGemma 2 4B Instruct",
                "description": "",
                "parameters": "4B",
                "context": "128000",
                "added": "2025-03-19",
                "architecture": "ShieldGemmaForImageClassification",
                "formats": [
                    "Safetensors"
                ],
                "tags": [
                    "Tool Use"
                ],
                "huggingface_repo": "google/shieldgemma-2-4b-it",
                "transformers_version": "4.50.0.dev0",
                "gated": "manual",
                "license": "gemma",
                "logo": "",
                "size_of_model_in_mb": 8239.389739990234,
                "author": {
                    "name": "google",
                    "url": "https://huggingface.co/google/shieldgemma-2-4b-it",
                    "blurb": ""
                },
                "resources": {
                    "canonicalUrl": "https://huggingface.co/google/shieldgemma-2-4b-it",
                    "downloadUrl": "https://huggingface.co/google/shieldgemma-2-4b-it",
                    "paperUrl": "?"
                },
                "model_group": "gemma"
            },
            {
                "uniqueID": "mlx-community/gemma-3-27b-it-qat-4bit",
                "name": "Gemma 3 QAT 27B (MLX 4-bit)",
                "description": "This 27B MLX formatted version of Google's Gemma 3 model is built using Quantization Aware Training (QAT), which allows the model to preserve similar quality as bfloat16 while significantly reducing the memory requirements.",
                "added": "2025-04-22",
                "tags": [],
                "parameters": "27B",
                "context": "128000",
                "architecture": "Gemma3ForConditionalGeneration",
                "formats": [
                    "Safetensors"
                ],
                "huggingface_repo": "mlx-community/gemma-3-27b-it-qat-4bit",
                "transformers_version": "4.51.3",
                "gated": false,
                "license": "other",
                "logo": "https://storage.googleapis.com/gweb-uniblog-publish-prod/images/gemma-header.width-1200.format-webp.webp",
                "size_of_model_in_mb": 16091.71697807312,
                "author": {
                    "name": "mlx-community",
                    "url": "https://huggingface.co/mlx-community/gemma-3-27b-it-qat-4bit",
                    "blurb": ""
                },
                "resources": {
                    "canonicalUrl": "https://huggingface.co/mlx-community/gemma-3-27b-it-qat-4bit",
                    "downloadUrl": "https://huggingface.co/mlx-community/gemma-3-27b-it-qat-4bit",
                    "paperUrl": "?"
                },
                "model_group": "gemma"
            },
            {
                "uniqueID": "mlx-community/gemma-3-12b-it-qat-4bit",
                "name": "Gemma 3 QAT 12B (MLX 4-bit)",
                "description": "This 12B MLX formatted version of Google's Gemma 3 model is built using Quantization Aware Training (QAT), which allows the model to preserve similar quality as bfloat16 while significantly reducing the memory requirements.",
                "added": "2025-04-22",
                "tags": [],
                "parameters": "12B",
                "context": "128000",
                "architecture": "Gemma3ForConditionalGeneration",
                "formats": [
                    "Safetensors"
                ],
                "huggingface_repo": "mlx-community/gemma-3-12b-it-qat-4bit",
                "transformers_version": "4.51.3",
                "gated": false,
                "license": "other",
                "logo": "https://storage.googleapis.com/gweb-uniblog-publish-prod/images/gemma-header.width-1200.format-webp.webp",
                "size_of_model_in_mb": 7694.268117904663,
                "author": {
                    "name": "mlx-community",
                    "url": "https://huggingface.co/mlx-community/gemma-3-12b-it-qat-4bit",
                    "blurb": ""
                },
                "resources": {
                    "canonicalUrl": "https://huggingface.co/mlx-community/gemma-3-12b-it-qat-4bit",
                    "downloadUrl": "https://huggingface.co/mlx-community/gemma-3-12b-it-qat-4bit",
                    "paperUrl": "?"
                },
                "model_group": "gemma"
            },
            {
                "uniqueID": "mlx-community/gemma-3-1b-it-qat-4bit",
                "name": "Gemma 3 QAT 1B (MLX 4-bit)",
                "description": "This 1B MLX formatted version of Google's Gemma 3 model is built using Quantization Aware Training (QAT), which allows the model to preserve similar quality as bfloat16 while significantly reducing the memory requirements.",
                "added": "2025-04-22",
                "tags": [],
                "parameters": "1B",
                "context": "128000",
                "architecture": "Gemma3ForCausalLM",
                "formats": [
                    "Safetensors"
                ],
                "huggingface_repo": "mlx-community/gemma-3-1b-it-qat-4bit",
                "transformers_version": "4.52.0.dev0",
                "gated": false,
                "license": "gemma",
                "logo": "https://storage.googleapis.com/gweb-uniblog-publish-prod/images/gemma-header.width-1200.format-webp.webp",
                "size_of_model_in_mb": 736.1032953262329,
                "author": {
                    "name": "mlx-community",
                    "url": "https://huggingface.co/mlx-community/gemma-3-1b-it-qat-4bit",
                    "blurb": ""
                },
                "resources": {
                    "canonicalUrl": "https://huggingface.co/mlx-community/gemma-3-1b-it-qat-4bit",
                    "downloadUrl": "https://huggingface.co/mlx-community/gemma-3-1b-it-qat-4bit",
                    "paperUrl": "?"
                },
                "model_group": "gemma"
            },
            {
                "uniqueID": "google/gemma-3-1b-it-qat-q4_0-gguf",
                "name": "Gemma 3 QAT 1B IT (q4_0 GGUF)",
                "description": "This 1B GGUF formatted version of Google's Gemma 3 model is built using Quantization Aware Training (QAT), which allows the model to preserve similar quality as bfloat16 while significantly reducing the memory requirements.",
                "added": "2025-04-22",
                "tags": [],
                "parameters": "1B",
                "context": "128000",
                "architecture": "GGUF",
                "formats": [
                    "GGUF"
                ],
                "huggingface_repo": "google/gemma-3-1b-it-qat-q4_0-gguf",
                "huggingface_filename": "gemma-3-1b-it-q4_0.gguf",
                "transformers_version": "4.51.3",
                "gated": false,
                "license": "other",
                "logo": "https://storage.googleapis.com/gweb-uniblog-publish-prod/images/gemma-header.width-1200.format-webp.webp",
                "size_of_model_in_mb": 1024.0,
                "author": {
                    "name": "google",
                    "url": "https://huggingface.co/google/gemma-3-1b-it-qat-q4_0-gguf",
                    "blurb": ""
                },
                "resources": {
                    "canonicalUrl": "https://huggingface.co/google/gemma-3-1b-it-qat-q4_0-gguf",
                    "downloadUrl": "https://huggingface.co/google/gemma-3-1b-it-qat-q4_0-gguf",
                    "paperUrl": "?"
                },
                "model_group": "gemma"
            }
        ]
    },
    {
        "name": "llama",
        "description": "Foundational language models known for versatility and strong pretraining baseline by Meta.",
        "tags": [],
        "models": [
            {
                "uniqueID": "mlx-community/Llama-2-7b-mlx",
                "name": "Llama 2 7b MLX",
                "archived": true,
                "description": "Llama 2 is a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. This is the repository for the 7B fine-tuned model, in npz format suitable for use in Apple's MLX framework.",
                "parameters": "7B",
                "context": "4k",
                "architecture": "MLX",
                "formats": [
                    "NPZ"
                ],
                "huggingface_repo": "mlx-community/Llama-2-7b-mlx",
                "transformers_version": "4.34.0.dev0",
                "gated": "auto",
                "license": "Apache 2.0",
                "logo": "https://ml-explore.github.io/mlx/build/html/_static/mlx_logo.png",
                "size_of_model_in_mb": 12853.07,
                "author": {
                    "name": "",
                    "url": "",
                    "blurb": ""
                },
                "resources": {
                    "canonicalUrl": "",
                    "downloadUrl": "",
                    "paperUrl": "?"
                },
                "model_group": "llama"
            },
            {
                "uniqueID": "mlx-community/Llama-2-7b-chat-4-bit",
                "name": "Llama 2 7B Chat 4-bit MLX",
                "archived": true,
                "description": "Llama 2 is a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. This is the repository for the 7B fine-tuned model, in npz format suitable for use in Apple's MLX framework.",
                "parameters": "7B",
                "context": "4k",
                "architecture": "MLX",
                "formats": [
                    "NPZ"
                ],
                "huggingface_repo": "mlx-community/Llama-2-7b-chat-4-bit",
                "transformers_version": "4.34.0.dev0",
                "gated": "auto",
                "license": "Apache 2.0",
                "logo": "https://ml-explore.github.io/mlx/build/html/_static/mlx_logo.png",
                "size_of_model_in_mb": 3795.51,
                "author": {
                    "name": "",
                    "url": "",
                    "blurb": ""
                },
                "resources": {
                    "canonicalUrl": "",
                    "downloadUrl": "",
                    "paperUrl": "?"
                },
                "model_group": "llama"
            },
            {
                "uniqueID": "allenai/Llama-3.1-Tulu-3-8B",
                "name": "Llama 3.1 Tulu 3 8B",
                "description": "T\u00fclu3 is a leading instruction following model family, offering fully open-source data, code, and recipes designed to serve as a comprehensive guide for modern post-training techniques. T\u00fclu3 is designed for state-of-the-art performance on a diversity of tasks in addition to chat, such as MATH, GSM8K, and IFEval.",
                "parameters": "8B",
                "added": "2025-01-31",
                "context": "131072",
                "architecture": "LlamaForCausalLM",
                "formats": [
                    "Safetensors"
                ],
                "huggingface_repo": "allenai/Llama-3.1-Tulu-3-8B",
                "transformers_version": "4.43.4",
                "gated": false,
                "license": "llama3.1",
                "logo": "https://pbs.twimg.com/profile_images/1819065604370051072/UrIOp6zI_400x400.png",
                "size_of_model_in_mb": 15325.4,
                "author": {
                    "name": "allenai",
                    "url": "https://huggingface.co/allenai/Llama-3.1-Tulu-3-8B",
                    "blurb": ""
                },
                "resources": {
                    "canonicalUrl": "https://huggingface.co/allenai/Llama-3.1-Tulu-3-8B",
                    "downloadUrl": "https://huggingface.co/allenai/Llama-3.1-Tulu-3-8B",
                    "paperUrl": "?"
                },
                "model_group": "llama"
            },
            {
                "uniqueID": "mlx-community/Llama-3.2-1B-Instruct-4bit",
                "name": "Llama 3.2 1B Instruct (MLX 4bit)",
                "description": "The Llama 3.2 instruction-tuned text only models are optimized for multilingual dialogue use cases, including agentic retrieval and summarization tasks. They outperform many of the available open source and closed chat models on common industry benchmarks.",
                "added": "2024-09-27",
                "parameters": "1B",
                "context": "131072",
                "architecture": "MLX",
                "formats": [
                    "Safetensors"
                ],
                "huggingface_repo": "mlx-community/Llama-3.2-1B-Instruct-4bit",
                "transformers_version": "4.45.0.dev0",
                "gated": false,
                "license": "llama3.2",
                "logo": "",
                "size_of_model_in_mb": 671.82,
                "author": {
                    "name": "mlx-community",
                    "url": "https://huggingface.co/mlx-community/Llama-3.2-1B-Instruct-4bit",
                    "blurb": ""
                },
                "resources": {
                    "canonicalUrl": "https://huggingface.co/mlx-community/Llama-3.2-1B-Instruct-4bit",
                    "downloadUrl": "https://huggingface.co/mlx-community/Llama-3.2-1B-Instruct-4bit",
                    "paperUrl": "?"
                },
                "model_group": "llama"
            },
            {
                "uniqueID": "Llama-3.2-1B-Instruct-Q6_K.gguf",
                "name": "LLama 3.2 1B Instruct (GGUF Q6_K)",
                "description": "GGUF export of Llama 3.2 1B Instruct model using 6-bit K-quantization. Should maintain near perfect results to original model.",
                "added": "2024-11-21",
                "parameters": "1B",
                "context": "131072",
                "architecture": "GGUF",
                "formats": [
                    "GGUF"
                ],
                "huggingface_repo": "bartowski/Llama-3.2-1B-Instruct-GGUF",
                "huggingface_filename": "Llama-3.2-1B-Instruct-Q6_K.gguf",
                "gated": false,
                "license": "llama3.2",
                "logo": "https://upload.wikimedia.org/wikipedia/commons/a/ab/Meta-Logo.png",
                "size_of_model_in_mb": 1044,
                "author": {
                    "name": "Meta",
                    "url": "https://huggingface.co/meta-llama/",
                    "blurb": ""
                },
                "resources": {
                    "canonicalUrl": "https://huggingface.co/bartowski/Llama-3.2-1B-Instruct-GGUF",
                    "downloadUrl": "https://huggingface.co/bartowski/Llama-3.2-1B-Instruct-GGUF",
                    "paperUrl": "?"
                },
                "model_group": "llama"
            },
            {
                "uniqueID": "unsloth/Llama-3.2-1B-Instruct",
                "name": "Llama 3.2 1B Instruct",
                "description": "The Llama 3.2 instruction-tuned text only models are optimized for multilingual dialogue use cases, including agentic retrieval and summarization tasks. They outperform many of the available open source and closed chat models on common industry benchmarks.",
                "added": "2024-09-27",
                "parameters": "1B",
                "context": "131072",
                "architecture": "LlamaForCausalLM",
                "formats": [
                    "Safetensors"
                ],
                "huggingface_repo": "unsloth/Llama-3.2-1B-Instruct",
                "transformers_version": "4.45.0.dev0",
                "gated": false,
                "license": "llama3.2",
                "logo": "",
                "size_of_model_in_mb": 2365.86,
                "author": {
                    "name": "meta-llama",
                    "url": "https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct",
                    "blurb": ""
                },
                "resources": {
                    "canonicalUrl": "https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct",
                    "downloadUrl": "https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct",
                    "paperUrl": "?"
                },
                "model_group": "llama"
            },
            {
                "uniqueID": "meta-llama/Llama-3.2-1B",
                "name": "Llama 3.2 1B",
                "description": "Llama 3.2 is an auto-regressive language model that uses an optimized transformer architecture. The tuned versions use supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align with human preferences for helpfulness and safety.",
                "added": "2024-09-27",
                "parameters": "1B",
                "context": "131072",
                "architecture": "LlamaForCausalLM",
                "formats": [
                    "Safetensors"
                ],
                "huggingface_repo": "meta-llama/Llama-3.2-1B",
                "transformers_version": "4.45.0.dev0",
                "gated": "manual",
                "license": "llama3.2",
                "logo": "",
                "size_of_model_in_mb": 2365.86,
                "author": {
                    "name": "meta-llama",
                    "url": "https://huggingface.co/meta-llama/Llama-3.2-1B",
                    "blurb": ""
                },
                "resources": {
                    "canonicalUrl": "https://huggingface.co/meta-llama/Llama-3.2-1B",
                    "downloadUrl": "https://huggingface.co/meta-llama/Llama-3.2-1B",
                    "paperUrl": "?"
                },
                "model_group": "llama"
            },
            {
                "uniqueID": "mlx-community/Llama-3.3-70B-Instruct-4bit",
                "name": "Llama 3.3 70B Instruct (MLX 4bit)",
                "description": "MLX export of Llama 3.3 70B Instruct model quantized to 4 bit. Loses some quality but useful for systems with lower RAM.",
                "parameters": "70B",
                "context": "131072",
                "architecture": "MLX",
                "formats": [
                    "Safetensors"
                ],
                "huggingface_repo": "mlx-community/Llama-3.3-70B-Instruct-4bit",
                "transformers_version": "4.47.0.dev0",
                "gated": false,
                "license": "llama3.3",
                "logo": "https://upload.wikimedia.org/wikipedia/commons/a/ab/Meta-Logo.png",
                "size_of_model_in_mb": 37866.6,
                "author": {
                    "name": "mlx-community",
                    "url": "https://huggingface.co/mlx-community/Llama-3.3-70B-Instruct-4bit",
                    "blurb": ""
                },
                "resources": {
                    "canonicalUrl": "https://huggingface.co/mlx-community/Llama-3.3-70B-Instruct-4bit",
                    "downloadUrl": "https://huggingface.co/mlx-community/Llama-3.3-70B-Instruct-4bit",
                    "paperUrl": "?"
                },
                "model_group": "llama"
            },
            {
                "uniqueID": "Llama-3.3-70B-Instruct-GGUF",
                "name": "LLama 3.2 70B Instruct (GGUF IQ2_M)",
                "description": "GGUF export of Llama 3.3 70B Instruct model IQ2_M variant. Loses some quality but useful for systems with lower RAM.",
                "added": "2024-12-12",
                "parameters": "70B",
                "context": "131072",
                "architecture": "GGUF",
                "formats": [
                    "GGUF"
                ],
                "huggingface_repo": "bartowski/Llama-3.3-70B-Instruct-GGUF",
                "huggingface_filename": "Llama-3.3-70B-Instruct-IQ2_M.gguf",
                "gated": false,
                "license": "llama3.3",
                "logo": "https://upload.wikimedia.org/wikipedia/commons/a/ab/Meta-Logo.png",
                "size_of_model_in_mb": 24678.4,
                "author": {
                    "name": "Meta",
                    "url": "https://huggingface.co/meta-llama/",
                    "blurb": ""
                },
                "resources": {
                    "canonicalUrl": "https://huggingface.co/bartowski/Llama-3.3-70B-Instruct-GGUF",
                    "downloadUrl": "https://huggingface.co/bartowski/Llama-3.3-70B-Instruct-GGUF",
                    "paperUrl": "?"
                },
                "model_group": "llama"
            },
            {
                "uniqueID": "huggyllama/llama-7b",
                "name": "LLama 7B",
                "archived": true,
                "description": "",
                "parameters": "7B",
                "context": "4k",
                "architecture": "LlamaForCausalLM",
                "formats": [
                    "Safetensors",
                    "PyTorch"
                ],
                "huggingface_repo": "huggyllama/llama-7b",
                "transformers_version": "4.27.4",
                "gated": false,
                "license": "Meta Custom",
                "logo": "https://upload.wikimedia.org/wikipedia/commons/a/ab/Meta-Logo.png",
                "size_of_model_in_mb": 25707.46,
                "author": {
                    "name": "Meta",
                    "url": "https://huggingface.co/meta-llama/",
                    "blurb": ""
                },
                "resources": {
                    "canonicalUrl": "https://huggingface.co/huggyllama/llama-7b",
                    "downloadUrl": "https://huggingface.co/huggyllama/llama-7b",
                    "paperUrl": "?"
                },
                "model_group": "llama"
            },
            {
                "uniqueID": "llama-2-7b-chat.Q4_0.gguf",
                "name": "LLama 2 - 7B chat - GGUF - Q4_0",
                "archived": true,
                "description": "Llama 2 is a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. This is the repository for the 7B fine-tuned model, optimized for dialogue use cases and converted for the Hugging Face Transformers format.",
                "parameters": "7B",
                "context": "4k",
                "architecture": "GGUF",
                "formats": [
                    "GGUF"
                ],
                "huggingface_repo": "TheBloke/Llama-2-7B-Chat-GGUF",
                "huggingface_filename": "llama-2-7b-chat.Q4_0.gguf",
                "gated": false,
                "license": "Meta Custom",
                "logo": "https://upload.wikimedia.org/wikipedia/commons/a/ab/Meta-Logo.png",
                "size_of_model_in_mb": 3648.6,
                "author": {
                    "name": "Meta",
                    "url": "https://huggingface.co/meta-llama/",
                    "blurb": ""
                },
                "resources": {
                    "canonicalUrl": "https://huggingface.co/meta-llama/Llama-2-7b-chat",
                    "downloadUrl": "https://huggingface.co/meta-llama/Llama-2-7b-chat",
                    "paperUrl": "?"
                },
                "model_group": "llama"
            },
            {
                "uniqueID": "llama-2-7b.Q4_0.gguf",
                "name": "LLama 2 - 7B - GGUF - Q4_0",
                "archived": true,
                "description": "Llama 2 is a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. This is the repository for the 7B fine-tuned model, optimized for dialogue use cases and converted for the Hugging Face Transformers format.",
                "parameters": "7B",
                "context": "4k",
                "architecture": "GGUF",
                "formats": [
                    "GGUF"
                ],
                "huggingface_repo": "TheBloke/Llama-2-7B-GGUF",
                "huggingface_filename": "llama-2-7b.Q4_0.gguf",
                "gated": false,
                "license": "Meta Custom",
                "logo": "https://upload.wikimedia.org/wikipedia/commons/a/ab/Meta-Logo.png",
                "size_of_model_in_mb": 3648.6,
                "author": {
                    "name": "Meta",
                    "url": "https://huggingface.co/meta-llama/",
                    "blurb": ""
                },
                "resources": {
                    "canonicalUrl": "https://huggingface.co/meta-llama/Llama-2-7b-chat",
                    "downloadUrl": "https://huggingface.co/meta-llama/Llama-2-7b-chat",
                    "paperUrl": "?"
                },
                "model_group": "llama"
            },
            {
                "uniqueID": "meta-llama/Llama-2-7b-chat-hf",
                "name": "LLama 2 7B - Chat",
                "archived": true,
                "description": "Llama 2 is a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. This is the repository for the 7B fine-tuned model, optimized for dialogue use cases and converted for the Hugging Face Transformers format.",
                "parameters": "7B",
                "context": "4k",
                "architecture": "LlamaForCausalLM",
                "huggingface_repo": "meta-llama/Llama-2-7b-chat-hf",
                "formats": [
                    "PyTorch"
                ],
                "transformers_version": "4.31.0.dev0",
                "gated": "manual",
                "license": "Meta Custom",
                "logo": "https://upload.wikimedia.org/wikipedia/commons/a/ab/Meta-Logo.png",
                "size_of_model_in_mb": 25707.46,
                "author": {
                    "name": "Meta",
                    "url": "https://huggingface.co/meta-llama/",
                    "blurb": ""
                },
                "resources": {
                    "canonicalUrl": "https://huggingface.co/meta-llama/Llama-2-7b-chat",
                    "downloadUrl": "https://huggingface.co/meta-llama/Llama-2-7b-chat",
                    "paperUrl": "?"
                },
                "model_group": "llama"
            },
            {
                "uniqueID": "meta-llama/Llama-2-7b-hf",
                "name": "LLama 2 7B",
                "archived": true,
                "description": "Llama 2 is a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. This is the repository for the 7B pretrained model, converted for the Hugging Face Transformers format",
                "parameters": "7B",
                "context": "4k",
                "architecture": "LlamaForCausalLM",
                "formats": [
                    "PyTorch"
                ],
                "huggingface_repo": "meta-llama/Llama-2-7b-hf",
                "transformers_version": "4.31.0.dev0",
                "gated": "manual",
                "license": "Meta Custom",
                "logo": "https://upload.wikimedia.org/wikipedia/commons/a/ab/Meta-Logo.png",
                "size_of_model_in_mb": 25707.46,
                "author": {
                    "name": "Meta",
                    "url": "https://huggingface.co/meta-llama/",
                    "blurb": ""
                },
                "resources": {
                    "canonicalUrl": "https://huggingface.co/meta-llama/Llama-2-7b-hf",
                    "downloadUrl": "https://huggingface.co/meta-llama/Llama-2-7b-hf",
                    "paperUrl": "?"
                },
                "model_group": "llama"
            },
            {
                "uniqueID": "meta-llama/Llama-2-13b-hf",
                "name": "LLama 2 13B",
                "archived": true,
                "description": "Llama 2 is a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. This is the repository for the 7B pretrained model, converted for the Hugging Face Transformers format",
                "parameters": "13B",
                "context": "4k",
                "architecture": "LlamaForCausalLM",
                "formats": [
                    "PyTorch"
                ],
                "huggingface_repo": "meta-llama/Llama-2-13b-hf",
                "transformers_version": "4.31.0.dev0",
                "gated": "manual",
                "license": "Meta Custom",
                "logo": "https://upload.wikimedia.org/wikipedia/commons/a/ab/Meta-Logo.png",
                "size_of_model_in_mb": 49654.08,
                "author": {
                    "name": "Meta",
                    "url": "https://huggingface.co/meta-llama/",
                    "blurb": ""
                },
                "resources": {
                    "canonicalUrl": "https://huggingface.co/meta-llama/Llama-2-13b-hf",
                    "downloadUrl": "https://huggingface.co/meta-llama/Llama-2-13b-hf",
                    "paperUrl": "?"
                },
                "model_group": "llama"
            },
            {
                "uniqueID": "meta-llama/Llama-2-13b-chat-hf",
                "name": "LLama 2 13B - Chat",
                "archived": true,
                "description": "Llama 2 is a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. This is the repository for the 7B pretrained model, converted for the Hugging Face Transformers format",
                "parameters": "7B",
                "context": "4k",
                "architecture": "LlamaForCausalLM",
                "formats": [
                    "PyTorch"
                ],
                "huggingface_repo": "meta-llama/Llama-2-13b-chat-hf",
                "transformers_version": "4.31.0.dev0",
                "gated": "manual",
                "license": "Meta Custom",
                "logo": "https://upload.wikimedia.org/wikipedia/commons/a/ab/Meta-Logo.png",
                "size_of_model_in_mb": 49654.08,
                "author": {
                    "name": "Meta",
                    "url": "https://huggingface.co/meta-llama/",
                    "blurb": ""
                },
                "resources": {
                    "canonicalUrl": "https://huggingface.co/meta-llama/Llama-2-13b-chat-hf",
                    "downloadUrl": "https://huggingface.co/meta-llama/Llama-2-13b-chat-hf",
                    "paperUrl": "?"
                },
                "model_group": "llama"
            },
            {
                "uniqueID": "meta-llama/Meta-Llama-3-8B-Instruct",
                "name": "LLama 3 8B - Instruct",
                "archived": true,
                "description": "Meta developed and released the Meta Llama 3 family of large language models (LLMs), a collection of pretrained and instruction tuned generative text models in 8 and 70B sizes. The Llama 3 instruction tuned models are optimized for dialogue use cases and outperform many of the available open source chat models on common industry benchmarks. Further, in developing these models, we took great care to optimize helpfulness and safety.",
                "parameters": "8B",
                "context": "8k",
                "architecture": "LlamaForCausalLM",
                "formats": [
                    "Safetensors"
                ],
                "huggingface_repo": "meta-llama/Meta-Llama-3-8B-Instruct",
                "transformers_version": "4.38",
                "gated": "manual",
                "license": "Meta Custom",
                "logo": "https://upload.wikimedia.org/wikipedia/commons/a/ab/Meta-Logo.png",
                "size_of_model_in_mb": 15325.28,
                "author": {
                    "name": "Meta",
                    "url": "https://huggingface.co/meta-llama/",
                    "blurb": ""
                },
                "resources": {
                    "canonicalUrl": "https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct",
                    "downloadUrl": "https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct",
                    "paperUrl": "?"
                },
                "model_group": "llama"
            },
            {
                "uniqueID": "meta-llama/Meta-Llama-3-8B",
                "name": "LLama 3 8B",
                "archived": true,
                "description": "Meta developed and released the Meta Llama 3 family of large language models (LLMs), a collection of pretrained and instruction tuned generative text models in 8 and 70B sizes. The Llama 3 instruction tuned models are optimized for dialogue use cases and outperform many of the available open source chat models on common industry benchmarks. Further, in developing these models, we took great care to optimize helpfulness and safety.",
                "parameters": "8B",
                "context": "8k",
                "architecture": "LlamaForCausalLM",
                "formats": [
                    "Safetensors"
                ],
                "huggingface_repo": "meta-llama/Meta-Llama-3-8B",
                "transformers_version": "4.38",
                "gated": "manual",
                "license": "Meta Custom",
                "logo": "https://upload.wikimedia.org/wikipedia/commons/a/ab/Meta-Logo.png",
                "size_of_model_in_mb": 15325.28,
                "author": {
                    "name": "Meta",
                    "url": "https://huggingface.co/meta-llama/",
                    "blurb": ""
                },
                "resources": {
                    "canonicalUrl": "https://huggingface.co/meta-llama/Meta-Llama-3-8B",
                    "downloadUrl": "https://huggingface.co/meta-llama/Meta-Llama-3-8B",
                    "paperUrl": "?"
                },
                "model_group": "llama"
            },
            {
                "uniqueID": "mlx-community/Meta-Llama-3-8B-Instruct-4bit",
                "name": "Llama 3 8B Instruct (MLX 4-bit)",
                "archived": true,
                "description": "Meta developed and released the Meta Llama 3 family of large language models (LLMs), a collection of pretrained and instruction tuned generative text models in 8 and 70B sizes. The Llama 3 instruction tuned models are optimized for dialogue use cases and outperform many of the available open source chat models on common industry benchmarks. Further, in developing these models, we took great care to optimize helpfulness and safety.",
                "parameters": "8B",
                "context": "8k",
                "architecture": "MLX",
                "formats": [
                    "Safetensors"
                ],
                "huggingface_repo": "mlx-community/Meta-Llama-3-8B-Instruct-4bit",
                "transformers_version": "4.38",
                "gated": false,
                "license": "Meta Custom",
                "logo": "https://upload.wikimedia.org/wikipedia/commons/a/ab/Meta-Logo.png",
                "size_of_model_in_mb": 5037.17,
                "author": {
                    "name": "Meta",
                    "url": "https://huggingface.co/meta-llama/",
                    "blurb": ""
                },
                "resources": {
                    "canonicalUrl": "https://huggingface.co/mlx-community/Meta-Llama-3-8B-Instruct-4bit",
                    "downloadUrl": "https://huggingface.co/mlx-community/Meta-Llama-3-8B-Instruct-4bit",
                    "paperUrl": "?"
                },
                "model_group": "llama"
            },
            {
                "uniqueID": "mlx-community/Meta-Llama-3-70B-Instruct-4bit",
                "name": "Llama 3 70B Instruct (MLX 4-bit)",
                "archived": true,
                "description": "Meta developed and released the Meta Llama 3 family of large language models (LLMs), a collection of pretrained and instruction tuned generative text models in 8 and 70B sizes. The Llama 3 instruction tuned models are optimized for dialogue use cases and outperform many of the available open source chat models on common industry benchmarks. Further, in developing these models, we took great care to optimize helpfulness and safety.",
                "parameters": "8B",
                "context": "8k",
                "architecture": "MLX",
                "formats": [
                    "Safetensors"
                ],
                "huggingface_repo": "mlx-community/Meta-Llama-3-70B-Instruct-4bit",
                "transformers_version": "4.38",
                "gated": false,
                "license": "Meta Custom",
                "logo": "https://upload.wikimedia.org/wikipedia/commons/a/ab/Meta-Logo.png",
                "size_of_model_in_mb": 37858.83,
                "author": {
                    "name": "Meta",
                    "url": "https://huggingface.co/meta-llama/",
                    "blurb": ""
                },
                "resources": {
                    "canonicalUrl": "https://huggingface.co/mlx-community/Meta-Llama-3-70B-Instruct-4bit",
                    "downloadUrl": "https://huggingface.co/mlx-community/Meta-Llama-3-70B-Instruct-4bit",
                    "paperUrl": "?"
                },
                "model_group": "llama"
            },
            {
                "uniqueID": "meta-llama/Meta-Llama-3.1-8B-Instruct",
                "name": "LLama 3.1 8B Instruct",
                "description": "The Meta Llama 3.1 collection of multilingual large language models (LLMs) is a collection of pretrained and instruction tuned generative models in 8B, 70B and 405B sizes (text in/text out). The Llama 3.1 instruction tuned text only models (8B, 70B, 405B) are optimized for multilingual dialogue use cases and outperform many of the available open source and closed chat models on common industry benchmarks.",
                "parameters": "8B",
                "context": "128k",
                "architecture": "LlamaForCausalLM",
                "formats": [
                    "Safetensors"
                ],
                "huggingface_repo": "meta-llama/Meta-Llama-3.1-8B-Instruct",
                "transformers_version": "4.42.3",
                "gated": "manual",
                "license": "Meta Custom",
                "logo": "https://upload.wikimedia.org/wikipedia/commons/a/ab/Meta-Logo.png",
                "size_of_model_in_mb": 15325.28,
                "author": {
                    "name": "Meta",
                    "url": "https://huggingface.co/meta-llama/",
                    "blurb": ""
                },
                "resources": {
                    "canonicalUrl": "https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct",
                    "downloadUrl": "https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct",
                    "paperUrl": "?"
                },
                "model_group": "llama"
            },
            {
                "uniqueID": "meta-llama/Meta-Llama-3.1-8B",
                "name": "LLama 3.1 8B",
                "description": "The Meta Llama 3.1 collection of multilingual large language models (LLMs) is a collection of pretrained and instruction tuned generative models in 8B, 70B and 405B sizes (text in/text out). The Llama 3.1 instruction tuned text only models (8B, 70B, 405B) are optimized for multilingual dialogue use cases and outperform many of the available open source and closed chat models on common industry benchmarks.",
                "parameters": "8B",
                "context": "128k",
                "architecture": "LlamaForCausalLM",
                "formats": [
                    "Safetensors"
                ],
                "huggingface_repo": "meta-llama/Meta-Llama-3.1-8B",
                "transformers_version": "4.42.3",
                "gated": "manual",
                "license": "Meta Custom",
                "logo": "https://upload.wikimedia.org/wikipedia/commons/a/ab/Meta-Logo.png",
                "size_of_model_in_mb": 15325.28,
                "author": {
                    "name": "Meta",
                    "url": "https://huggingface.co/meta-llama/",
                    "blurb": ""
                },
                "resources": {
                    "canonicalUrl": "https://huggingface.co/meta-llama/Meta-Llama-3.1-8B",
                    "downloadUrl": "https://huggingface.co/meta-llama/Meta-Llama-3.1-8B",
                    "paperUrl": "?"
                },
                "model_group": "llama"
            }
        ]
    },
    {
        "name": "llava",
        "description": "Vision-language model capable of multimodal understanding and visual question answering.",
        "tags": [
            "Multimodal"
        ],
        "models": [
            {
                "uniqueID": "bczhou/tiny-llava-v1-hf",
                "name": "TinyLlava v1",
                "description": "Small scale large multimodal model. Able to take in image input and assess web links.",
                "parameters": "1.41B",
                "context": "4k",
                "architecture": "LlavaForConditionalGeneration",
                "tags": [
                    "Multimodal"
                ],
                "formats": [
                    "Safetensors",
                    "PyTorch"
                ],
                "huggingface_repo": "bczhou/tiny-llava-v1-hf",
                "transformers_version": "4.42.4",
                "gated": false,
                "license": "Apache 2.0",
                "logo": "https://media.datadriveninvestor.com/uploads/2020/10/llava_logo.png",
                "size_of_model_in_mb": 5381.05,
                "author": {
                    "name": "bczhou",
                    "url": "https://huggingface.co/bczhou",
                    "blurb": ""
                },
                "resources": {
                    "canonicalUrl": "https://huggingface.co/bczhou/tiny-llava-v1-hf",
                    "downloadUrl": "https://huggingface.co/bczhou/tiny-llava-v1-hf",
                    "paperUrl": "?"
                },
                "model_group": "llava"
            },
            {
                "uniqueID": "llava-hf/llava-1.5-7b-hf",
                "name": "LLaVA 1.5 7B",
                "description": "LLaVA is an open-source chatbot trained by fine-tuning LLaMA/Vicuna on GPT-generated multimodal instruction-following data.",
                "parameters": "7B",
                "context": "4k",
                "architecture": "LlavaForConditionalGeneration",
                "tags": [
                    "Multimodal"
                ],
                "formats": [
                    "Safetensors",
                    "PyTorch"
                ],
                "huggingface_repo": "llava-hf/llava-1.5-7b-hf",
                "transformers_version": "4.42.4",
                "gated": false,
                "license": "Apache 2.0",
                "logo": "https://media.datadriveninvestor.com/uploads/2020/10/llava_logo.png",
                "size_of_model_in_mb": 13476.51,
                "author": {
                    "name": "llava-hf",
                    "url": "https://huggingface.co/llava-hf",
                    "blurb": ""
                },
                "resources": {
                    "canonicalUrl": "https://huggingface.co/llava-hf/llava-1.5-7b-hf",
                    "downloadUrl": "https://huggingface.co/llava-hf/llava-1.5-7b-hf",
                    "paperUrl": "?"
                },
                "model_group": "llava"
            }
        ]
    },
    {
        "name": "intellect",
        "description": "Reasoning-focused. Designed for analytical and structured language tasks.",
        "tags": [
            "Reasoning"
        ],
        "models": [
            {
                "uniqueID": "PrimeIntellect/INTELLECT-2",
                "name": "INTELLECT-2",
                "description": "INTELLECT-2 is a 32 billion parameter language model trained through a reinforcement learning run leveraging globally distributed, permissionless GPU resources contributed by the community.",
                "added": "2025-05-13",
                "tags": [],
                "parameters": "32B",
                "context": "40960",
                "architecture": "Qwen2ForCausalLM",
                "formats": [
                    "Safetensors"
                ],
                "huggingface_repo": "PrimeIntellect/INTELLECT-2",
                "transformers_version": "4.51.3",
                "gated": false,
                "license": "apache-2.0",
                "logo": "https://cdn-avatars.huggingface.co/v1/production/uploads/62088594a5943c8a8fc94560/y5SEKiE8TkjBKs9xfjCx5.png",
                "size_of_model_in_mb": 124997.96606826782,
                "author": {
                    "name": "PrimeIntellect",
                    "url": "https://huggingface.co/PrimeIntellect/INTELLECT-2",
                    "blurb": ""
                },
                "resources": {
                    "canonicalUrl": "https://huggingface.co/PrimeIntellect/INTELLECT-2",
                    "downloadUrl": "https://huggingface.co/PrimeIntellect/INTELLECT-2",
                    "paperUrl": "?"
                },
                "model_group": "intellect"
            }
        ]
    },
    {
        "name": "nemotron",
        "description": "Reasoning models that are post trained for reasoning, human chat preferences, and tasks, such as RAG and tool calling.",
        "tags": [
            "reasoning",
            "chat",
            "rag"
        ],
        "models": []
    },
    {
        "name": "smollm",
        "description": "Tiny instruct models designed for experimentation and lightweight tasks.",
        "tags": [
            "Tiny"
        ],
        "models": [
            {
                "uniqueID": "mlx-community/SmolLM-135M-4bit",
                "name": "SmolLM 135M MLX 4bit",
                "description": "Tiny SLM created by HuggingFace in 4-bit MLX format.",
                "added": "2025-01-28",
                "parameters": "135M",
                "context": "2048",
                "architecture": "MLX",
                "formats": [
                    "Safetensors"
                ],
                "huggingface_repo": "mlx-community/SmolLM-135M-4bit",
                "transformers_version": "4.41.2",
                "gated": false,
                "license": "apache-2.0",
                "logo": "https://webml-community-smollm-webgpu.static.hf.space/logo.png",
                "size_of_model_in_mb": 75.1,
                "author": {
                    "name": "mlx-community",
                    "url": "https://huggingface.co/mlx-community/SmolLM-135M-4bit",
                    "blurb": ""
                },
                "resources": {
                    "canonicalUrl": "https://huggingface.co/mlx-community/SmolLM-135M-4bit",
                    "downloadUrl": "https://huggingface.co/mlx-community/SmolLM-135M-4bit",
                    "paperUrl": "https://huggingface.co/blog/smollm"
                },
                "model_group": "smollm"
            },
            {
                "uniqueID": "HuggingFaceTB/SmolLM-135M-Instruct",
                "name": "SmolLM 135M Instruct",
                "description": "The SMOLest version of HuggingFace's SLM trained on publicly available datasets.",
                "added": "2025-01-28",
                "parameters": "135M",
                "context": "2048",
                "architecture": "LlamaForCausalLM",
                "formats": [
                    "Safetensors"
                ],
                "huggingface_repo": "HuggingFaceTB/SmolLM-135M-Instruct",
                "transformers_version": "4.42.3",
                "gated": false,
                "license": "apache-2.0",
                "logo": "https://webml-community-smollm-webgpu.static.hf.space/logo.png",
                "size_of_model_in_mb": 259.37,
                "author": {
                    "name": "HuggingFaceTB",
                    "url": "https://huggingface.co/HuggingFaceTB/SmolLM-135M-Instruct",
                    "blurb": ""
                },
                "resources": {
                    "canonicalUrl": "https://huggingface.co/HuggingFaceTB/SmolLM-135M-Instruct",
                    "downloadUrl": "https://huggingface.co/HuggingFaceTB/SmolLM-135M-Instruct",
                    "paperUrl": "https://huggingface.co/blog/smollm"
                },
                "model_group": "smollm"
            },
            {
                "uniqueID": "HuggingFaceTB/SmolLM2-1.7B-Instruct",
                "name": "SmolLM2 1.7B Instruct",
                "description": "1.7B parameter version of HuggingFace's SLM trained on publicly available datasets.",
                "added": "2025-01-28",
                "parameters": "1.7B",
                "context": "8192",
                "architecture": "LlamaForCausalLM",
                "formats": [
                    "Safetensors"
                ],
                "huggingface_repo": "HuggingFaceTB/SmolLM2-1.7B-Instruct",
                "transformers_version": "4.42.3",
                "gated": false,
                "license": "apache-2.0",
                "logo": "https://webml-community-smollm-webgpu.static.hf.space/logo.png",
                "size_of_model_in_mb": 3267.08,
                "author": {
                    "name": "HuggingFaceTB",
                    "url": "https://huggingface.co/HuggingFaceTB/SmolLM2-1.7B-Instruct",
                    "blurb": ""
                },
                "resources": {
                    "canonicalUrl": "https://huggingface.co/HuggingFaceTB/SmolLM2-1.7B-Instruct",
                    "downloadUrl": "https://huggingface.co/HuggingFaceTB/SmolLM2-1.7B-Instruct",
                    "paperUrl": "https://huggingface.co/blog/smollm"
                },
                "model_group": "smollm"
            }
        ]
    },
    {
        "name": "cogito",
        "description": "Hybrid reasoning capability. Outperforming peers across standard benchmarks in its size class.",
        "tags": [
            "Reasoning"
        ],
        "models": [
            {
                "uniqueID": "deepcogito/cogito-v1-preview-llama-3B",
                "name": "Cogito 3B",
                "description": "Cogito v1 Preview is a family of hybrid reasoning models by Deep Cogito that outperform the best available open models of the same size, including counterparts from LLaMA, DeepSeek, and Qwen across most standard benchmarks.",
                "added": "2025-04-09",
                "tags": [],
                "parameters": "3B",
                "context": "131072",
                "architecture": "LlamaForCausalLM",
                "formats": [
                    "Safetensors"
                ],
                "huggingface_repo": "deepcogito/cogito-v1-preview-llama-3B",
                "transformers_version": "4.49.0",
                "gated": false,
                "license": "llama3.2",
                "logo": "https://upload.wikimedia.org/wikipedia/commons/a/ab/Meta-Logo.png",
                "size_of_model_in_mb": 6895.848543167114,
                "author": {
                    "name": "deepcogito",
                    "url": "https://huggingface.co/deepcogito/cogito-v1-preview-llama-3B",
                    "blurb": ""
                },
                "resources": {
                    "canonicalUrl": "https://huggingface.co/deepcogito/cogito-v1-preview-llama-3B",
                    "downloadUrl": "https://huggingface.co/deepcogito/cogito-v1-preview-llama-3B",
                    "paperUrl": "?"
                },
                "model_group": "cogito"
            },
            {
                "uniqueID": "deepcogito/cogito-v1-preview-llama-8B",
                "name": "Cogito 8B",
                "description": "Cogito v1 Preview is a family of hybrid reasoning models by Deep Cogito that outperform the best available open models of the same size, including counterparts from LLaMA, DeepSeek, and Qwen across most standard benchmarks.",
                "added": "2025-04-09",
                "tags": [],
                "parameters": "8B",
                "context": "131072",
                "architecture": "LlamaForCausalLM",
                "formats": [
                    "Safetensors"
                ],
                "huggingface_repo": "deepcogito/cogito-v1-preview-llama-8B",
                "transformers_version": "4.49.0",
                "gated": false,
                "license": "llama3.1",
                "logo": "https://upload.wikimedia.org/wikipedia/commons/a/ab/Meta-Logo.png",
                "size_of_model_in_mb": 15333.029312133789,
                "author": {
                    "name": "deepcogito",
                    "url": "https://huggingface.co/deepcogito/cogito-v1-preview-llama-8B",
                    "blurb": ""
                },
                "resources": {
                    "canonicalUrl": "https://huggingface.co/deepcogito/cogito-v1-preview-llama-8B",
                    "downloadUrl": "https://huggingface.co/deepcogito/cogito-v1-preview-llama-8B",
                    "paperUrl": "?"
                },
                "model_group": "cogito"
            },
            {
                "uniqueID": "deepcogito/cogito-v1-preview-qwen-14B",
                "name": "Cogito 14B",
                "description": "Cogito v1 Preview is a family of hybrid reasoning models by Deep Cogito that outperform the best available open models of the same size, including counterparts from LLaMA, DeepSeek, and Qwen across most standard benchmarks.",
                "added": "2025-04-09",
                "tags": [],
                "parameters": "14B",
                "context": "131072",
                "architecture": "Qwen2ForCausalLM",
                "formats": [
                    "Safetensors"
                ],
                "huggingface_repo": "deepcogito/cogito-v1-preview-qwen-14B",
                "transformers_version": "4.49.0",
                "gated": false,
                "license": "apache-2.0",
                "logo": "https://cdn-avatars.huggingface.co/v1/production/uploads/62088594a5943c8a8fc94560/y5SEKiE8TkjBKs9xfjCx5.png",
                "size_of_model_in_mb": 28177.47069168091,
                "author": {
                    "name": "deepcogito",
                    "url": "https://huggingface.co/deepcogito/cogito-v1-preview-qwen-14B",
                    "blurb": ""
                },
                "resources": {
                    "canonicalUrl": "https://huggingface.co/deepcogito/cogito-v1-preview-qwen-14B",
                    "downloadUrl": "https://huggingface.co/deepcogito/cogito-v1-preview-qwen-14B",
                    "paperUrl": "?"
                },
                "model_group": "cogito"
            },
            {
                "uniqueID": "deepcogito/cogito-v1-preview-qwen-32B",
                "name": "Cogito 32B",
                "description": "Cogito v1 Preview is a family of hybrid reasoning models by Deep Cogito that outperform the best available open models of the same size, including counterparts from LLaMA, DeepSeek, and Qwen across most standard benchmarks.",
                "added": "2025-04-09",
                "tags": [],
                "parameters": "32B",
                "context": "131072",
                "architecture": "Qwen2ForCausalLM",
                "formats": [
                    "Safetensors"
                ],
                "huggingface_repo": "deepcogito/cogito-v1-preview-qwen-32B",
                "transformers_version": "4.49.0",
                "gated": false,
                "license": "apache-2.0",
                "logo": "https://cdn-avatars.huggingface.co/v1/production/uploads/62088594a5943c8a8fc94560/y5SEKiE8TkjBKs9xfjCx5.png",
                "size_of_model_in_mb": 62498.03526878357,
                "author": {
                    "name": "deepcogito",
                    "url": "https://huggingface.co/deepcogito/cogito-v1-preview-qwen-32B",
                    "blurb": ""
                },
                "resources": {
                    "canonicalUrl": "https://huggingface.co/deepcogito/cogito-v1-preview-qwen-32B",
                    "downloadUrl": "https://huggingface.co/deepcogito/cogito-v1-preview-qwen-32B",
                    "paperUrl": "?"
                },
                "model_group": "cogito"
            },
            {
                "uniqueID": "deepcogito/cogito-v1-preview-llama-70B",
                "name": "Cogito 70B",
                "description": "Cogito v1 Preview is a family of hybrid reasoning models by Deep Cogito that outperform the best available open models of the same size, including counterparts from LLaMA, DeepSeek, and Qwen across most standard benchmarks.",
                "added": "2025-04-09",
                "tags": [],
                "parameters": "70B",
                "context": "131072",
                "architecture": "LlamaForCausalLM",
                "formats": [
                    "Safetensors"
                ],
                "huggingface_repo": "deepcogito/cogito-v1-preview-llama-70B",
                "transformers_version": "4.49.0",
                "gated": false,
                "license": "llama3.1",
                "logo": "https://upload.wikimedia.org/wikipedia/commons/a/ab/Meta-Logo.png",
                "size_of_model_in_mb": 134587.11736679077,
                "author": {
                    "name": "deepcogito",
                    "url": "https://huggingface.co/deepcogito/cogito-v1-preview-llama-70B",
                    "blurb": ""
                },
                "resources": {
                    "canonicalUrl": "https://huggingface.co/deepcogito/cogito-v1-preview-llama-70B",
                    "downloadUrl": "https://huggingface.co/deepcogito/cogito-v1-preview-llama-70B",
                    "paperUrl": "?"
                },
                "model_group": "cogito"
            }
        ]
    },
    {
        "name": "mistral",
        "description": "Fast, open-weight transformer optimized for low-latency inference.",
        "tags": [],
        "models": [
            {
                "uniqueID": "mlx-community/Mistral-7B-Instruct-v0.1-4bit-mlx",
                "name": "Mistral 7B Instruct v0.1 (MLX 4bit)",
                "archived": true,
                "description": "The Mistral-7B-Instruct-v0.1 Large Language Model (LLM) is a instruct fine-tuned version of the Mistral-7B-v0.1 generative text model using a variety of publicly available conversation datasets.",
                "parameters": "7B",
                "context": "4k",
                "architecture": "MLX",
                "formats": [
                    "Safetensors"
                ],
                "huggingface_repo": "mlx-community/Mistral-7B-Instruct-v0.1-4bit-mlx",
                "transformers_version": "4.34.0.dev0",
                "gated": "auto",
                "license": "Apache 2.0",
                "logo": "https://ml-explore.github.io/mlx/build/html/_static/mlx_logo.png",
                "size_of_model_in_mb": 4066.62,
                "author": {
                    "name": "Mistral AI",
                    "url": "https://docs.mistral.ai/",
                    "blurb": ""
                },
                "resources": {
                    "canonicalUrl": "https://huggingface.co/mistralai/Mistral-7B-v0.1",
                    "downloadUrl": "https://huggingface.co/mistralai/Mistral-7B-v0.1",
                    "paperUrl": "?"
                },
                "model_group": "mistral"
            },
            {
                "uniqueID": "mlx-community/Mistral-7B-Instruct-v0.2-4bit",
                "name": "Mistral-7B-Instruct-v0.2 4 bit MLX",
                "archived": true,
                "description": "The Mistral-7B-Instruct-v0.2 Large Language Model (LLM) is an improved instruct fine-tuned version of Mistral-7B-Instruct-v0.1.",
                "parameters": "7B",
                "context": "4k",
                "architecture": "MLX",
                "formats": [
                    "Safetensors"
                ],
                "huggingface_repo": "mlx-community/Mistral-7B-Instruct-v0.2-4bit",
                "transformers_version": "4.39.0.dev0",
                "gated": "auto",
                "license": "Apache 2.0",
                "logo": "https://ml-explore.github.io/mlx/build/html/_static/mlx_logo.png",
                "size_of_model_in_mb": 4067.14,
                "author": {
                    "name": "",
                    "url": "",
                    "blurb": ""
                },
                "resources": {
                    "canonicalUrl": "",
                    "downloadUrl": "",
                    "paperUrl": "?"
                },
                "model_group": "mistral"
            },
            {
                "uniqueID": "mlx-community/Mistral-7B-Instruct-v0.2",
                "name": "Mistral-7B-Instruct-v0.2 MLX",
                "archived": true,
                "description": "The Mistral-7B-Instruct-v0.2 Large Language Model (LLM) is an improved instruct fine-tuned version of Mistral-7B-Instruct-v0.1.",
                "parameters": "7B",
                "context": "4k",
                "architecture": "MLX",
                "formats": [
                    "NPZ"
                ],
                "huggingface_repo": "mlx-community/Mistral-7B-Instruct-v0.2",
                "transformers_version": "4.34.0.dev0",
                "gated": "auto",
                "license": "Apache 2.0",
                "logo": "https://ml-explore.github.io/mlx/build/html/_static/mlx_logo.png",
                "size_of_model_in_mb": 13813.07,
                "author": {
                    "name": "",
                    "url": "",
                    "blurb": ""
                },
                "resources": {
                    "canonicalUrl": "",
                    "downloadUrl": "",
                    "paperUrl": "?"
                },
                "model_group": "mistral"
            },
            {
                "uniqueID": "mistralai/Devstral-Small-2505",
                "name": "Devstral-Small-2505",
                "description": "Agentic LLM for software engineering tasks",
                "added": "2025-05-21",
                "tags": [
                    "Coding"
                ],
                "parameters": "",
                "context": "131072",
                "architecture": "MistralForCausalLM",
                "formats": [
                    "Safetensors"
                ],
                "huggingface_repo": "mistralai/Devstral-Small-2505",
                "transformers_version": "4.51.3",
                "gated": false,
                "license": "apache-2.0",
                "logo": "https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503",
                "size_of_model_in_mb": 89935.80525779724,
                "author": {
                    "name": "mistralai",
                    "url": "https://huggingface.co/mistralai/Devstral-Small-2505",
                    "blurb": ""
                },
                "resources": {
                    "canonicalUrl": "https://huggingface.co/mistralai/Devstral-Small-2505",
                    "downloadUrl": "https://huggingface.co/mistralai/Devstral-Small-2505",
                    "paperUrl": "?"
                },
                "model_group": "mistral"
            },
            {
                "uniqueID": "mlx-community/Mistral-Small-24B-Instruct-2501-4bit",
                "name": "Mistral Small 24B Instruct 2501 (MLX 4-bit)",
                "description": "Mistral Small 3 ( 2501 ) sets a new benchmark in the \"small\" Large Language Models category below 70B, boasting 24B parameters and achieving state-of-the-art capabilities comparable to larger models!",
                "parameters": "24B",
                "added": "2025-01-31",
                "archived": true,
                "context": "32768",
                "architecture": "MistralForCausalLM",
                "formats": [
                    "MLX"
                ],
                "huggingface_repo": "mlx-community/Mistral-Small-24B-Instruct-2501-4bit",
                "transformers_version": "4.49.0.dev0",
                "gated": false,
                "license": "apache-2.0",
                "logo": "https://docs.mistral.ai/img/logo.svg",
                "size_of_model_in_mb": 12662.46,
                "author": {
                    "name": "mlx-community",
                    "url": "https://huggingface.co/mlx-community/Mistral-Small-24B-Instruct-2501-4bit",
                    "blurb": ""
                },
                "resources": {
                    "canonicalUrl": "https://huggingface.co/mlx-community/Mistral-Small-24B-Instruct-2501-4bit",
                    "downloadUrl": "https://huggingface.co/mlx-community/Mistral-Small-24B-Instruct-2501-4bit",
                    "paperUrl": "?"
                },
                "model_group": "mistral"
            },
            {
                "uniqueID": "mistralai/Mistral-Small-24B-Instruct-2501",
                "name": "Mistral Small 24B Instruct 2501",
                "description": "Mistral Small 3 ( 2501 ) sets a new benchmark in the \"small\" Large Language Models category below 70B, boasting 24B parameters and achieving state-of-the-art capabilities comparable to larger models!",
                "parameters": "24B",
                "added": "2025-01-31",
                "archived": true,
                "context": "32768",
                "architecture": "MistralForCausalLM",
                "formats": [
                    "Safetensors"
                ],
                "huggingface_repo": "mistralai/Mistral-Small-24B-Instruct-2501",
                "transformers_version": "4.49.0.dev0",
                "gated": "auto",
                "license": "apache-2.0",
                "logo": "https://docs.mistral.ai/img/logo.svg",
                "size_of_model_in_mb": 89952.3,
                "author": {
                    "name": "mistralai",
                    "url": "https://huggingface.co/mistralai/Mistral-Small-24B-Instruct-2501",
                    "blurb": ""
                },
                "resources": {
                    "canonicalUrl": "https://huggingface.co/mistralai/Mistral-Small-24B-Instruct-2501",
                    "downloadUrl": "https://huggingface.co/mistralai/Mistral-Small-24B-Instruct-2501",
                    "paperUrl": "?"
                },
                "model_group": "mistral"
            },
            {
                "uniqueID": "mistralai/Mistral-Small-3.1-24B-Instruct-2503",
                "name": "Mistral Small 3.1 24B Instruct 2503",
                "description": "Building upon Mistral Small 3 (2501), Mistral Small 3.1 (2503) adds state-of-the-art vision understanding and enhances long context capabilities up to 128k tokens without compromising text performance.",
                "parameters": "24B",
                "context": "128000",
                "architecture": "Mistral3ForConditionalGeneration",
                "added": "2025-03-18",
                "formats": [
                    "Safetensors"
                ],
                "huggingface_repo": "mistralai/Mistral-Small-3.1-24B-Instruct-2503",
                "transformers_version": "4.50.0.dev0",
                "gated": false,
                "license": "apache-2.0",
                "logo": "https://docs.mistral.ai/img/logo.svg",
                "size_of_model_in_mb": 91627,
                "author": {
                    "name": "mistralai",
                    "url": "https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503",
                    "blurb": ""
                },
                "resources": {
                    "canonicalUrl": "https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503",
                    "downloadUrl": "https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503",
                    "paperUrl": "?"
                },
                "model_group": "mistral"
            },
            {
                "uniqueID": "mistralai/Mistral-Small-3.1-24B-Base-2503",
                "name": "Mistral Small 3.1 24B Base 2503",
                "description": "Building upon Mistral Small 3 (2501), Mistral Small 3.1 (2503) adds state-of-the-art vision understanding and enhances long context capabilities up to 128k tokens without compromising text performance.",
                "parameters": "24B",
                "context": "128000",
                "architecture": "Mistral3ForConditionalGeneration",
                "added": "2025-03-18",
                "formats": [
                    "Safetensors"
                ],
                "huggingface_repo": "mistralai/Mistral-Small-3.1-24B-Base-2503",
                "transformers_version": "4.50.0.dev0",
                "gated": false,
                "license": "apache-2.0",
                "logo": "https://docs.mistral.ai/img/logo.svg",
                "size_of_model_in_mb": 91627,
                "author": {
                    "name": "mistralai",
                    "url": "https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Base-2503",
                    "blurb": ""
                },
                "resources": {
                    "canonicalUrl": "https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Base-2503",
                    "downloadUrl": "https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Base-2503",
                    "paperUrl": "?"
                },
                "model_group": "mistral"
            },
            {
                "uniqueID": "mlx-community/Mistral-Small-3.1-24B-Instruct-2503-4bit",
                "name": "Mistral Small 3.1 24B Instruct 2503 (4bit MLX)",
                "description": "MLX formatted 4-bit quantization of the 2503 release of Mistral's Small 3.1 model.",
                "added": "2025-03-25",
                "tags": [],
                "parameters": "24B",
                "context": "2048",
                "architecture": "Mistral3ForConditionalGeneration",
                "formats": [
                    "Safetensors"
                ],
                "huggingface_repo": "mlx-community/Mistral-Small-3.1-24B-Instruct-2503-4bit",
                "transformers_version": "4.50.0.dev0",
                "gated": false,
                "license": "apache-2.0",
                "logo": "https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503",
                "size_of_model_in_mb": 13464.98053264618,
                "author": {
                    "name": "mlx-community",
                    "url": "https://huggingface.co/mlx-community/Mistral-Small-3.1-24B-Instruct-2503-4bit",
                    "blurb": ""
                },
                "resources": {
                    "canonicalUrl": "https://huggingface.co/mlx-community/Mistral-Small-3.1-24B-Instruct-2503-4bit",
                    "downloadUrl": "https://huggingface.co/mlx-community/Mistral-Small-3.1-24B-Instruct-2503-4bit",
                    "paperUrl": "?"
                },
                "model_group": "mistral"
            },
            {
                "uniqueID": "unsloth/Mistral-Small-3.1-24B-Instruct-2503-GGUF",
                "name": "Mistral Small 3.1 24B Instruct 2503 (GGUF Q4_K_M)",
                "description": "GGUF-formatted 4-bit quantization of the 2503 release of Mistral's Small 3.1 model.",
                "added": "2025-03-25",
                "tags": [],
                "parameters": "24B",
                "context": "2048",
                "architecture": "Mistral3ForConditionalGeneration",
                "formats": [
                    "Safetensors"
                ],
                "huggingface_repo": "unsloth/Mistral-Small-3.1-24B-Instruct-2503-GGUF",
                "huggingface_filename": "Mistral-Small-3.1-24B-Instruct-2503-Q4_K_M.gguf",
                "transformers_version": "4.50.0.dev0",
                "gated": false,
                "license": "apache-2.0",
                "logo": "https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503",
                "size_of_model_in_mb": 14643.2,
                "author": {
                    "name": "unsloth",
                    "url": "https://huggingface.co/unsloth/Mistral-Small-3.1-24B-Instruct-2503-GGUF",
                    "blurb": ""
                },
                "resources": {
                    "canonicalUrl": "https://huggingface.co/unsloth/Mistral-Small-3.1-24B-Instruct-2503-GGUF",
                    "downloadUrl": "https://huggingface.co/unsloth/Mistral-Small-3.1-24B-Instruct-2503-GGUF",
                    "paperUrl": "?"
                },
                "model_group": "mistral"
            },
            {
                "uniqueID": "mistralai/Mistral-7B-v0.3",
                "name": "Mistral-7B-v0.3",
                "description": "The Mistral-7B-v0.3 Large Language Model (LLM) is a Mistral-7B-v0.2 with extended vocabulary. Mistral-7B-v0.3 has the following changes compared to Mistral-7B-v0.2: Extended vocabulary to 32768",
                "parameters": "7B",
                "context": "32768",
                "architecture": "MistralForCausalLM",
                "formats": [
                    "Safetensors",
                    "PyTorch"
                ],
                "huggingface_repo": "mistralai/Mistral-7B-v0.3",
                "allow_patterns": [
                    "tokenizer.model.v3",
                    "*.json",
                    "*.safetensors",
                    "*.py",
                    "tokenizer.model",
                    "*.tiktoken",
                    "*.npz",
                    "*.bin"
                ],
                "transformers_version": "4.42.0.dev0",
                "gated": "auto",
                "license": "Apache 2.0",
                "logo": "https://docs.mistral.ai/img/logo.svg",
                "size_of_model_in_mb": 27652.22,
                "model_group": "mistral"
            },
            {
                "uniqueID": "mistralai/Mistral-7B-Instruct-v0.3",
                "name": "Mistral-7B-Instruct-v0.3",
                "description": "The Mistral-7B-Instruct-v0.3 Large Language Model (LLM) is an instruct fine-tuned version of the Mistral-7B-v0.3. Mistral-7B-v0.3 has the following changes compared to Mistral-7B-v0.2 Extended vocabulary to 32768. Supports v3 Tokenizer. Supports function calling",
                "parameters": "7B",
                "context": "32768",
                "architecture": "MistralForCausalLM",
                "formats": [
                    "Safetensors",
                    "PyTorch"
                ],
                "huggingface_repo": "mistralai/Mistral-7B-Instruct-v0.3",
                "allow_patterns": [
                    "tokenizer.model.v3",
                    "*.json",
                    "*.safetensors",
                    "*.py",
                    "tokenizer.model",
                    "*.tiktoken",
                    "*.npz",
                    "*.bin"
                ],
                "transformers_version": "4.42.0.dev0",
                "gated": "auto",
                "license": "Apache 2.0",
                "logo": "https://docs.mistral.ai/img/logo.svg",
                "size_of_model_in_mb": 27652.23,
                "model_group": "mistral"
            },
            {
                "uniqueID": "mistralai/Mistral-7B-v0.1",
                "name": "Mistral 7B v0.1",
                "archived": true,
                "description": "The Mistral-7B-v0.1 Large Language Model (LLM) is a pretrained generative text model with 7 billion parameters. Mistral-7B-v0.1 outperforms Llama 2 13B on all benchmarks we tested.",
                "parameters": "7B",
                "context": "4k",
                "architecture": "MistralForCausalLM",
                "formats": [
                    "Safetensors",
                    "PyTorch"
                ],
                "huggingface_repo": "mistralai/Mistral-7B-v0.1",
                "transformers_version": "4.34.0.dev0",
                "gated": "auto",
                "license": "Apache 2.0",
                "logo": "https://docs.mistral.ai/img/logo.svg",
                "size_of_model_in_mb": 28127.37,
                "author": {
                    "name": "Mistral AI",
                    "url": "https://docs.mistral.ai/",
                    "blurb": ""
                },
                "resources": {
                    "canonicalUrl": "https://huggingface.co/mistralai/Mistral-7B-v0.1",
                    "downloadUrl": "https://huggingface.co/mistralai/Mistral-7B-v0.1",
                    "paperUrl": "?"
                },
                "model_group": "mistral"
            },
            {
                "uniqueID": "mistralai/Mistral-7B-Instruct-v0.2",
                "name": "Mistral 7B Instruct v0.2",
                "archived": true,
                "description": "The Mistral-7B-Instruct-v0.2 Large Language Model (LLM) is a instruct fine-tuned version of the Mistral-7B-v0.1 generative text model using a variety of publicly available conversation datasets.",
                "parameters": "7B",
                "context": "4k",
                "architecture": "MistralForCausalLM",
                "formats": [
                    "Safetensors",
                    "PyTorch"
                ],
                "huggingface_repo": "mistralai/Mistral-7B-Instruct-v0.2",
                "transformers_version": "4.36.0",
                "gated": "auto",
                "license": "Apache 2.0",
                "logo": "https://docs.mistral.ai/img/logo.svg",
                "size_of_model_in_mb": 28127.38,
                "author": {
                    "name": "Mistral AI",
                    "url": "https://docs.mistral.ai/",
                    "blurb": ""
                },
                "resources": {
                    "canonicalUrl": "https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2",
                    "downloadUrl": "https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2",
                    "paperUrl": "?"
                },
                "model_group": "mistral"
            },
            {
                "uniqueID": "Mixtral-8x7B-Instruct-v0.1",
                "name": "Mixtral 8x7B Instruct",
                "description": "The Mixtral-8x7B Large Language Model (LLM) is a pretrained generative Sparse Mixture of Experts. The Mistral-8x7B outperforms Llama 2 70B on most benchmarks we tested.",
                "parameters": "7B",
                "context": "4k",
                "architecture": "MixtralForCausalLM",
                "formats": [
                    "Safetensors",
                    "PyTorch"
                ],
                "huggingface_repo": "mistralai/Mixtral-8x7B-Instruct-v0.1",
                "transformers_version": "4.36.0.dev0",
                "gated": "auto",
                "license": "Apache 2.0",
                "logo": "https://mistral.ai/images/product/models/mistral-8x7b-v0.1.jpg",
                "size_of_model_in_mb": 56254.8,
                "author": {
                    "name": "Mistral AI",
                    "url": "https://mistral.ai/news/mixtral-of-experts/",
                    "blurb": ""
                },
                "resources": {
                    "canonicalUrl": "https://mistral.ai/news/mixtral-of-experts/",
                    "downloadUrl": "https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1",
                    "paperUrl": "?"
                },
                "model_group": "mistral"
            },
            {
                "uniqueID": "mistralai/Mistral-Nemo-Base-2407",
                "name": "Mistral NeMo Base 2407",
                "description": "The Mistral-Nemo-Base-2407 Large Language Model (LLM) is a pretrained generative text model of 12B parameters trained jointly by Mistral AI and NVIDIA, it significantly outperforms existing models smaller or similar in size.",
                "parameters": "12.2B",
                "context": "128k",
                "architecture": "MistralForCausalLM",
                "formats": [
                    "Safetensors"
                ],
                "huggingface_repo": "mistralai/Mistral-Nemo-Base-2407",
                "transformers_version": "4.43.0.dev0",
                "gated": "auto",
                "license": "Apache 2.0",
                "logo": "https://docs.mistral.ai/img/logo.svg",
                "size_of_model_in_mb": 46747.18,
                "author": {
                    "name": "Mistral AI",
                    "url": "https://docs.mistral.ai/",
                    "blurb": ""
                },
                "resources": {
                    "canonicalUrl": "https://huggingface.co/mistralai/Mistral-Nemo-Base-2407",
                    "downloadUrl": "https://huggingface.co/mistralai/Mistral-Nemo-Base-2407",
                    "paperUrl": "?"
                },
                "model_group": "mistral"
            },
            {
                "uniqueID": "mistralai/Mistral-Nemo-Instruct-2407",
                "name": "Mistral NeMo Instruct 2407",
                "description": "The Mistral-Nemo-Instruct-2407 Large Language Model (LLM) is an instruct fine-tuned version of the Mistral-Nemo-Base-2407. Trained jointly by Mistral AI and NVIDIA, it significantly outperforms existing models smaller or similar in size.",
                "parameters": "12.2B",
                "context": "128k",
                "architecture": "MistralForCausalLM",
                "formats": [
                    "Safetensors"
                ],
                "huggingface_repo": "mistralai/Mistral-Nemo-Instruct-2407",
                "transformers_version": "4.43.0.dev0",
                "gated": "auto",
                "license": "Apache 2.0",
                "logo": "https://docs.mistral.ai/img/logo.svg",
                "size_of_model_in_mb": 46747.18,
                "author": {
                    "name": "Mistral AI",
                    "url": "https://docs.mistral.ai/",
                    "blurb": ""
                },
                "resources": {
                    "canonicalUrl": "https://huggingface.co/mistralai/Mistral-Nemo-Instruct-2407",
                    "downloadUrl": "https://huggingface.co/mistralai/Mistral-Nemo-Instruct-2407",
                    "paperUrl": "?"
                },
                "model_group": "mistral"
            }
        ]
    },
    {
        "name": "falcon",
        "description": "Open-weight transformer with competitive performance across diverse language tasks.",
        "tags": [],
        "models": [
            {
                "uniqueID": "tiiuae/falcon-7b",
                "name": "Falcon 7B",
                "archived": true,
                "description": "Best overall smaller model. Fast responses. Instruction based. Trained by TII. Licensed for commercial use.",
                "parameters": "7B",
                "context": "4k",
                "architecture": "FalconForCausalLM",
                "formats": [
                    "PyTorch"
                ],
                "huggingface_repo": "tiiuae/falcon-7b",
                "transformers_version": "4.27.4",
                "gated": false,
                "license": "Apache 2.0",
                "logo": "https://falconllm.tii.ae/assets/images/logo.svg",
                "size_of_model_in_mb": 27534.19,
                "author": {
                    "name": "TII",
                    "url": "https://falconllm.tii.ae/index.html",
                    "blurb": ""
                },
                "resources": {
                    "canonicalUrl": "https://huggingface.co/tiiuae/falcon-7b",
                    "downloadUrl": "https://huggingface.co/tiiuae/falcon-7b",
                    "paperUrl": "?"
                },
                "model_group": "falcon"
            },
            {
                "uniqueID": "tiiuae/falcon-7b-instruct",
                "name": "Falcon 7B Instruct",
                "archived": true,
                "description": "Best overall smaller model. Fast responses. Instruction based. Trained by TII. Licensed for commercial use.",
                "parameters": "7B",
                "context": "4k",
                "architecture": "FalconForCausalLM",
                "formats": [
                    "PyTorch"
                ],
                "huggingface_repo": "tiiuae/falcon-7b-instruct",
                "transformers_version": "4.27.4",
                "gated": false,
                "license": "Apache 2.0",
                "logo": "https://falconllm.tii.ae/assets/images/logo.svg",
                "size_of_model_in_mb": 27534.19,
                "author": {
                    "name": "TII",
                    "url": "https://falconllm.tii.ae/index.html",
                    "blurb": ""
                },
                "resources": {
                    "canonicalUrl": "https://huggingface.co/tiiuae/falcon-7b-instruct",
                    "downloadUrl": "https://huggingface.co/tiiuae/falcon-7b-instruct",
                    "paperUrl": "?"
                },
                "model_group": "falcon"
            }
        ]
    },
    {
        "name": "flan",
        "description": "Versatile instruction-tuned variant of T5 excelling across multitask benchmarks.",
        "tags": [],
        "models": [
            {
                "uniqueID": "google/flan-t5-small",
                "name": "Google Flan T5 Small",
                "description": "",
                "parameters": "80M",
                "context": "2048",
                "architecture": "T5ForConditionalGeneration",
                "formats": [
                    "Safetensors",
                    "PyTorch"
                ],
                "huggingface_repo": "google/flan-t5-small",
                "transformers_version": "4.23.1",
                "gated": false,
                "license": "Apache 2.0",
                "logo": "",
                "size_of_model_in_mb": 589.56,
                "author": {
                    "name": "Google",
                    "url": "https://github.com/google-research/FLAN",
                    "blurb": ""
                },
                "resources": {
                    "canonicalUrl": "https://huggingface.co/google/flan-t5-small",
                    "downloadUrl": "https://huggingface.co/google/flan-t5-small",
                    "paperUrl": "?"
                },
                "model_group": "flan"
            }
        ]
    },
    {
        "name": "qwen",
        "description": "Multilingual, high-accuracy transformer excelling in reasoning and benchmark tasks.",
        "tags": [
            "Reasoning",
            "Multilingual"
        ],
        "models": [
            {
                "uniqueID": "Qwen/Qwen2-7B-Instruct",
                "name": "Qwen2 7B Instruct",
                "archived": true,
                "description": "Qwen2 has generally surpassed most opensource models and demonstrated competitiveness against proprietary models across a series of benchmarks targeting for language understanding, language generation, multilingual capability, coding, mathematics, reasoning, etc. Qwen2-7B-Instruct supports a context length of up to 131,072 tokens, enabling the processing of extensive inputs. This repo contains the instruction-tuned 7B Qwen2 model.",
                "parameters": "7B",
                "context": "32768",
                "architecture": "Qwen2ForCausalLM",
                "formats": [
                    "Safetensors"
                ],
                "huggingface_repo": "Qwen/Qwen2-7B-Instruct",
                "transformers_version": "4.41.2",
                "gated": false,
                "license": "Apache 2.0",
                "logo": "https://cdn-avatars.huggingface.co/v1/production/uploads/62088594a5943c8a8fc94560/y5SEKiE8TkjBKs9xfjCx5.png",
                "size_of_model_in_mb": 14535.05,
                "author": {
                    "name": "Alibaba Cloud",
                    "url": "",
                    "blurb": ""
                },
                "resources": {
                    "canonicalUrl": "https://huggingface.co/Qwen/Qwen2-7B-Instruct",
                    "downloadUrl": "https://huggingface.co/Qwen/Qwen2-7B-Instruct",
                    "paperUrl": "?"
                },
                "model_group": "qwen"
            },
            {
                "uniqueID": "Qwen/Qwen2-7B",
                "name": "Qwen2 7B",
                "archived": true,
                "description": "Qwen2 has generally surpassed most opensource models and demonstrated competitiveness against proprietary models across a series of benchmarks targeting for language understanding, language generation, multilingual capability, coding, mathematics, reasoning, etc. This repo contains the 7B Qwen2 base language model.",
                "parameters": "7B",
                "context": "32768",
                "architecture": "Qwen2ForCausalLM",
                "formats": [
                    "Safetensors"
                ],
                "huggingface_repo": "Qwen/Qwen2-7B",
                "transformers_version": "4.41.2",
                "gated": false,
                "license": "Apache 2.0",
                "logo": "https://cdn-avatars.huggingface.co/v1/production/uploads/62088594a5943c8a8fc94560/y5SEKiE8TkjBKs9xfjCx5.png",
                "size_of_model_in_mb": 14535.05,
                "author": {
                    "name": "Alibaba Cloud",
                    "url": "",
                    "blurb": ""
                },
                "resources": {
                    "canonicalUrl": "https://huggingface.co/Qwen/Qwen2-7B",
                    "downloadUrl": "https://huggingface.co/Qwen/Qwen2-7B",
                    "paperUrl": "?"
                },
                "model_group": "qwen"
            },
            {
                "uniqueID": "Qwen/Qwen2-1.5B-Instruct",
                "name": "Qwen2 1.5B Instruct",
                "archived": true,
                "description": "Qwen2 has generally surpassed most opensource models and demonstrated competitiveness against proprietary models across a series of benchmarks targeting for language understanding, language generation, multilingual capability, coding, mathematics, reasoning, etc. This repo contains the instruction-tuned 1.5B Qwen2 model.",
                "parameters": "1.5B",
                "context": "32768",
                "architecture": "Qwen2ForCausalLM",
                "formats": [
                    "Safetensors"
                ],
                "huggingface_repo": "Qwen/Qwen2-1.5B-Instruct",
                "transformers_version": "4.41.2",
                "gated": false,
                "license": "Apache 2.0",
                "logo": "https://cdn-avatars.huggingface.co/v1/production/uploads/62088594a5943c8a8fc94560/y5SEKiE8TkjBKs9xfjCx5.png",
                "size_of_model_in_mb": 2953.79,
                "author": {
                    "name": "Alibaba Cloud",
                    "url": "",
                    "blurb": ""
                },
                "resources": {
                    "canonicalUrl": "https://huggingface.co/Qwen/Qwen2-1.5B-Instruct",
                    "downloadUrl": "https://huggingface.co/Qwen/Qwen2-1.5B-Instruct",
                    "paperUrl": "?"
                },
                "model_group": "qwen"
            },
            {
                "uniqueID": "mlx-community/Liberated-Qwen1.5-72B-4bit",
                "name": "Liberated Qwen1.5 72B",
                "archived": true,
                "description": "Brought to you by AbacusAI and Eric Hartford. This model is based on Qwen/Qwen1.5-72B and subject to the tongyi-qianwen license. Liberated consists of open source datasets, including SystemChat a new dataset I created, designed to teach the model compliance to the system prompt, over long multiturn conversations, even with unusual or mechanical system prompts. These are tasks that Open Source Models have been lacking in thus far. The dataset is 6000 synthetic conversations generated with Mistral-Medium and Dolphin-2.7-mixtral-8x7b. There are no guardrails or censorship added to the dataset. You are advised to implement your own alignment layer before exposing the model as a service. Please read my blog post about uncensored models. https://erichartford.com/uncensored-models.",
                "parameters": "72B",
                "context": "32768",
                "architecture": "MLX",
                "formats": [
                    "Safetensors"
                ],
                "huggingface_repo": "mlx-community/Liberated-Qwen1.5-72B-4bit",
                "transformers_version": "4.39.3",
                "gated": false,
                "license": "CC-BY-NC-4.0",
                "logo": "https://cdn-uploads.huggingface.co/production/uploads/63111b2d88942700629f5771/xCWGByXr8YNwGxKVh_x9H.png",
                "size_of_model_in_mb": 40500.27,
                "author": {
                    "name": "AbacusAI",
                    "url": "",
                    "blurb": ""
                },
                "resources": {
                    "canonicalUrl": "",
                    "downloadUrl": "",
                    "paperUrl": "?"
                },
                "model_group": "qwen"
            },
            {
                "uniqueID": "Qwen/Qwen2.5-1.5B-Instruct",
                "name": "Qwen2.5 1.5B Instruct",
                "description": "Qwen2.5 is the latest series of Qwen large language models.",
                "parameters": "1.5B",
                "context": "32768",
                "architecture": "Qwen2ForCausalLM",
                "formats": [
                    "Safetensors"
                ],
                "huggingface_repo": "Qwen/Qwen2.5-1.5B-Instruct",
                "transformers_version": "4.43.1",
                "gated": false,
                "license": "apache-2.0",
                "logo": "https://cdn-avatars.huggingface.co/v1/production/uploads/62088594a5943c8a8fc94560/y5SEKiE8TkjBKs9xfjCx5.png",
                "size_of_model_in_mb": 2953.8,
                "author": {
                    "name": "Qwen",
                    "url": "https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct",
                    "blurb": ""
                },
                "resources": {
                    "canonicalUrl": "https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct",
                    "downloadUrl": "https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct",
                    "paperUrl": "?"
                },
                "model_group": "qwen"
            },
            {
                "uniqueID": "mlx-community/Qwen2.5-14B-Instruct-1M-8bit",
                "name": "Qwen2.5 14B Instruct - 1M context (MLX 8bit)",
                "description": "MLX 8-bit version of the long-context version of Qwen's 2.5 model.",
                "parameters": "14B",
                "added": "2025-01-28",
                "context": "1010000",
                "architecture": "MLX",
                "formats": [
                    "Safetensors"
                ],
                "huggingface_repo": "mlx-community/Qwen2.5-14B-Instruct-1M-8bit",
                "transformers_version": "4.45.2",
                "gated": false,
                "license": "apache-2.0",
                "logo": "https://cdn-avatars.huggingface.co/v1/production/uploads/62088594a5943c8a8fc94560/y5SEKiE8TkjBKs9xfjCx5.png",
                "size_of_model_in_mb": 14980.7,
                "author": {
                    "name": "mlx-community",
                    "url": "https://huggingface.co/mlx-community/Qwen2.5-14B-Instruct-1M-8bit",
                    "blurb": ""
                },
                "resources": {
                    "canonicalUrl": "https://huggingface.co/mlx-community/Qwen2.5-14B-Instruct-1M-8bit",
                    "downloadUrl": "https://huggingface.co/mlx-community/Qwen2.5-14B-Instruct-1M-8bit",
                    "paperUrl": "?"
                },
                "model_group": "qwen"
            },
            {
                "uniqueID": "Qwen/Qwen2.5-7B-Instruct-1M",
                "name": "Qwen2.5 7B Instruct - 1M context",
                "description": "Long-context version of the Qwen2.5 series models, supporting a context length of up to 1M tokens. Compared to the Qwen2.5 128K version, Qwen2.5-1M demonstrates significantly improved performance in handling long-context tasks while maintaining its capability in short tasks.",
                "parameters": "7B",
                "added": "2025-01-28",
                "context": "1010000",
                "architecture": "Qwen2ForCausalLM",
                "formats": [
                    "Safetensors"
                ],
                "huggingface_repo": "Qwen/Qwen2.5-7B-Instruct-1M",
                "transformers_version": "4.47.1",
                "gated": false,
                "license": "apache-2.0",
                "logo": "https://cdn-avatars.huggingface.co/v1/production/uploads/62088594a5943c8a8fc94560/y5SEKiE8TkjBKs9xfjCx5.png",
                "size_of_model_in_mb": 14535.11,
                "author": {
                    "name": "Qwen",
                    "url": "https://huggingface.co/Qwen/Qwen2.5-7B-Instruct-1M",
                    "blurb": ""
                },
                "resources": {
                    "canonicalUrl": "https://huggingface.co/Qwen/Qwen2.5-7B-Instruct-1M",
                    "downloadUrl": "https://huggingface.co/Qwen/Qwen2.5-7B-Instruct-1M",
                    "paperUrl": "?"
                },
                "model_group": "qwen"
            },
            {
                "uniqueID": "mlx-community/Qwen2.5-7B-Instruct-4bit",
                "name": "Qwen2.5 7B Instruct 4bit MLX",
                "description": "Qwen2.5 is the latest series of Qwen large language models.",
                "added": "2024-11-19",
                "parameters": "7B",
                "context": "32768",
                "architecture": "MLX",
                "formats": [
                    "Safetensors"
                ],
                "huggingface_repo": "mlx-community/Qwen2.5-7B-Instruct-4bit",
                "transformers_version": "4.43.1",
                "gated": false,
                "license": "apache-2.0",
                "logo": "https://cdn-avatars.huggingface.co/v1/production/uploads/62088594a5943c8a8fc94560/y5SEKiE8TkjBKs9xfjCx5.png",
                "size_of_model_in_mb": 4095.28,
                "author": {
                    "name": "mlx-community",
                    "url": "https://huggingface.co/mlx-community/Qwen2.5-7B-Instruct-4bit",
                    "blurb": ""
                },
                "resources": {
                    "canonicalUrl": "https://huggingface.co/mlx-community/Qwen2.5-7B-Instruct-4bit",
                    "downloadUrl": "https://huggingface.co/mlx-community/Qwen2.5-7B-Instruct-4bit",
                    "paperUrl": "?"
                },
                "model_group": "qwen"
            },
            {
                "uniqueID": "Qwen/Qwen2.5-7B-Instruct",
                "name": "Qwen2.5 7B Instruct",
                "description": "Qwen2.5 is the latest series of Qwen large language models",
                "added": "2024-11-19",
                "parameters": "7B",
                "context": "32768",
                "architecture": "Qwen2ForCausalLM",
                "formats": [
                    "Safetensors"
                ],
                "huggingface_repo": "Qwen/Qwen2.5-7B-Instruct",
                "transformers_version": "4.43.1",
                "gated": false,
                "license": "apache-2.0",
                "logo": "https://cdn-avatars.huggingface.co/v1/production/uploads/62088594a5943c8a8fc94560/y5SEKiE8TkjBKs9xfjCx5.png",
                "size_of_model_in_mb": 14535.06,
                "author": {
                    "name": "Qwen",
                    "url": "https://huggingface.co/Qwen/Qwen2.5-7B-Instruct",
                    "blurb": ""
                },
                "resources": {
                    "canonicalUrl": "https://huggingface.co/Qwen/Qwen2.5-7B-Instruct",
                    "downloadUrl": "https://huggingface.co/Qwen/Qwen2.5-7B-Instruct",
                    "paperUrl": "?"
                },
                "model_group": "qwen"
            },
            {
                "uniqueID": "Qwen/Qwen2.5-7B",
                "name": "Qwen2.5 7B",
                "description": "Qwen2.5 is the latest series of Qwen large language models",
                "added": "2024-11-19",
                "parameters": "7B",
                "context": "131072",
                "architecture": "Qwen2ForCausalLM",
                "formats": [
                    "Safetensors"
                ],
                "huggingface_repo": "Qwen/Qwen2.5-7B",
                "transformers_version": "4.40.1",
                "gated": false,
                "license": "apache-2.0",
                "logo": "https://cdn-avatars.huggingface.co/v1/production/uploads/62088594a5943c8a8fc94560/y5SEKiE8TkjBKs9xfjCx5.png",
                "size_of_model_in_mb": 14535.06,
                "author": {
                    "name": "Qwen",
                    "url": "https://huggingface.co/Qwen/Qwen2.5-7B",
                    "blurb": ""
                },
                "resources": {
                    "canonicalUrl": "https://huggingface.co/Qwen/Qwen2.5-7B",
                    "downloadUrl": "https://huggingface.co/Qwen/Qwen2.5-7B",
                    "paperUrl": "?"
                },
                "model_group": "qwen"
            },
            {
                "uniqueID": "mlx-community/Qwen2.5-Coder-14B-Instruct-4bit",
                "name": "Qwen2.5 Coder 14B Instruct (MLX 4bit)",
                "description": "Qwen2.5-Coder is the latest series of Code-Specific Qwen large language models (formerly known as CodeQwen) and performs at a level comparable to the best proprietary models.",
                "added": "2024-11-19",
                "parameters": "14B",
                "tags": [
                    "Coding"
                ],
                "context": "32768",
                "architecture": "MLX",
                "formats": [
                    "Safetensors"
                ],
                "huggingface_repo": "mlx-community/Qwen2.5-Coder-14B-Instruct-4bit",
                "transformers_version": "4.43.1",
                "gated": false,
                "license": "apache-2.0",
                "logo": "https://cdn-avatars.huggingface.co/v1/production/uploads/62088594a5943c8a8fc94560/y5SEKiE8TkjBKs9xfjCx5.png",
                "size_of_model_in_mb": 7934.02,
                "author": {
                    "name": "mlx-community",
                    "url": "https://huggingface.co/mlx-community/Qwen2.5-Coder-14B-Instruct-4bit",
                    "blurb": ""
                },
                "resources": {
                    "canonicalUrl": "https://huggingface.co/mlx-community/Qwen2.5-Coder-14B-Instruct-4bit",
                    "downloadUrl": "https://huggingface.co/mlx-community/Qwen2.5-Coder-14B-Instruct-4bit",
                    "paperUrl": "?"
                },
                "model_group": "qwen"
            },
            {
                "uniqueID": "Qwen/Qwen2.5-Coder-7B-Instruct",
                "name": "Qwen2.5 Coder 7B Instruct",
                "description": "Qwen2.5-Coder is the latest series of Code-Specific Qwen large language models (formerly known as CodeQwen) and performs at a level comparable to the best proprietary models.",
                "added": "2024-11-19",
                "parameters": "7B",
                "tags": [
                    "Coding"
                ],
                "context": "32768",
                "architecture": "Qwen2ForCausalLM",
                "formats": [
                    "Safetensors"
                ],
                "huggingface_repo": "Qwen/Qwen2.5-Coder-7B-Instruct",
                "transformers_version": "4.44.0",
                "gated": false,
                "license": "apache-2.0",
                "logo": "https://cdn-avatars.huggingface.co/v1/production/uploads/62088594a5943c8a8fc94560/y5SEKiE8TkjBKs9xfjCx5.png",
                "size_of_model_in_mb": 14535.06,
                "author": {
                    "name": "Qwen",
                    "url": "https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct",
                    "blurb": ""
                },
                "resources": {
                    "canonicalUrl": "https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct",
                    "downloadUrl": "https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct",
                    "paperUrl": "?"
                },
                "model_group": "qwen"
            },
            {
                "uniqueID": "Qwen/Qwen3-30B-A3B",
                "name": "Qwen3 30B A3B",
                "description": "",
                "added": "2025-04-28",
                "tags": [],
                "parameters": "30B",
                "context": "40960",
                "architecture": "Qwen3MoeForCausalLM",
                "formats": [
                    "Safetensors"
                ],
                "huggingface_repo": "Qwen/Qwen3-30B-A3B",
                "transformers_version": "4.51.0",
                "gated": false,
                "license": "apache-2.0",
                "logo": "https://cdn-avatars.huggingface.co/v1/production/uploads/62088594a5943c8a8fc94560/y5SEKiE8TkjBKs9xfjCx5.png",
                "size_of_model_in_mb": 58252.79786109924,
                "author": {
                    "name": "Qwen",
                    "url": "https://huggingface.co/Qwen/Qwen3-30B-A3B",
                    "blurb": ""
                },
                "resources": {
                    "canonicalUrl": "https://huggingface.co/Qwen/Qwen3-30B-A3B",
                    "downloadUrl": "https://huggingface.co/Qwen/Qwen3-30B-A3B",
                    "paperUrl": "?"
                },
                "model_group": "qwen"
            },
            {
                "uniqueID": "Qwen/Qwen3-32B",
                "name": "Qwen3 32B",
                "description": "",
                "added": "2025-04-28",
                "tags": [],
                "parameters": "32B",
                "context": "40960",
                "architecture": "Qwen3ForCausalLM",
                "formats": [
                    "Safetensors"
                ],
                "huggingface_repo": "Qwen/Qwen3-32B",
                "transformers_version": "4.51.0",
                "gated": false,
                "license": "apache-2.0",
                "logo": "https://cdn-avatars.huggingface.co/v1/production/uploads/62088594a5943c8a8fc94560/y5SEKiE8TkjBKs9xfjCx5.png",
                "size_of_model_in_mb": 62502.47671127319,
                "author": {
                    "name": "Qwen",
                    "url": "https://huggingface.co/Qwen/Qwen3-32B",
                    "blurb": ""
                },
                "resources": {
                    "canonicalUrl": "https://huggingface.co/Qwen/Qwen3-32B",
                    "downloadUrl": "https://huggingface.co/Qwen/Qwen3-32B",
                    "paperUrl": "?"
                },
                "model_group": "qwen"
            },
            {
                "uniqueID": "Qwen/Qwen3-14B",
                "name": "Qwen3 14B",
                "description": "",
                "added": "2025-04-28",
                "tags": [],
                "parameters": "14B",
                "context": "40960",
                "architecture": "Qwen3ForCausalLM",
                "formats": [
                    "Safetensors"
                ],
                "huggingface_repo": "Qwen/Qwen3-14B",
                "transformers_version": "4.51.0",
                "gated": false,
                "license": "apache-2.0",
                "logo": "https://cdn-avatars.huggingface.co/v1/production/uploads/62088594a5943c8a8fc94560/y5SEKiE8TkjBKs9xfjCx5.png",
                "size_of_model_in_mb": 28181.94607067108,
                "author": {
                    "name": "Qwen",
                    "url": "https://huggingface.co/Qwen/Qwen3-14B",
                    "blurb": ""
                },
                "resources": {
                    "canonicalUrl": "https://huggingface.co/Qwen/Qwen3-14B",
                    "downloadUrl": "https://huggingface.co/Qwen/Qwen3-14B",
                    "paperUrl": "?"
                },
                "model_group": "qwen"
            },
            {
                "uniqueID": "Qwen/Qwen3-8B",
                "name": "Qwen3 8B",
                "description": "",
                "added": "2025-04-28",
                "tags": [],
                "parameters": "8B",
                "context": "40960",
                "architecture": "Qwen3ForCausalLM",
                "formats": [
                    "Safetensors"
                ],
                "huggingface_repo": "Qwen/Qwen3-8B",
                "transformers_version": "4.51.0",
                "gated": false,
                "license": "apache-2.0",
                "logo": "https://cdn-avatars.huggingface.co/v1/production/uploads/62088594a5943c8a8fc94560/y5SEKiE8TkjBKs9xfjCx5.png",
                "size_of_model_in_mb": 15636.21500301361,
                "author": {
                    "name": "Qwen",
                    "url": "https://huggingface.co/Qwen/Qwen3-8B",
                    "blurb": ""
                },
                "resources": {
                    "canonicalUrl": "https://huggingface.co/Qwen/Qwen3-8B",
                    "downloadUrl": "https://huggingface.co/Qwen/Qwen3-8B",
                    "paperUrl": "?"
                },
                "model_group": "qwen"
            },
            {
                "uniqueID": "Qwen/Qwen3-4B",
                "name": "Qwen3 4B",
                "description": "",
                "added": "2025-04-28",
                "tags": [],
                "parameters": "4B",
                "context": "40960",
                "architecture": "Qwen3ForCausalLM",
                "formats": [
                    "Safetensors"
                ],
                "huggingface_repo": "Qwen/Qwen3-4B",
                "transformers_version": "4.51.0",
                "gated": false,
                "license": "apache-2.0",
                "logo": "https://cdn-avatars.huggingface.co/v1/production/uploads/62088594a5943c8a8fc94560/y5SEKiE8TkjBKs9xfjCx5.png",
                "size_of_model_in_mb": 7685.875841140747,
                "author": {
                    "name": "Qwen",
                    "url": "https://huggingface.co/Qwen/Qwen3-4B",
                    "blurb": ""
                },
                "resources": {
                    "canonicalUrl": "https://huggingface.co/Qwen/Qwen3-4B",
                    "downloadUrl": "https://huggingface.co/Qwen/Qwen3-4B",
                    "paperUrl": "?"
                },
                "model_group": "qwen"
            },
            {
                "uniqueID": "Qwen/Qwen3-1.7B",
                "name": "Qwen3 1.7B",
                "description": "",
                "added": "2025-04-28",
                "tags": [],
                "parameters": "1.7B",
                "context": "40960",
                "architecture": "Qwen3ForCausalLM",
                "formats": [
                    "Safetensors"
                ],
                "huggingface_repo": "Qwen/Qwen3-1.7B",
                "transformers_version": "4.51.0",
                "gated": false,
                "license": "apache-2.0",
                "logo": "https://cdn-avatars.huggingface.co/v1/production/uploads/62088594a5943c8a8fc94560/y5SEKiE8TkjBKs9xfjCx5.png",
                "size_of_model_in_mb": 3888.846706390381,
                "author": {
                    "name": "Qwen",
                    "url": "https://huggingface.co/Qwen/Qwen3-1.7B",
                    "blurb": ""
                },
                "resources": {
                    "canonicalUrl": "https://huggingface.co/Qwen/Qwen3-1.7B",
                    "downloadUrl": "https://huggingface.co/Qwen/Qwen3-1.7B",
                    "paperUrl": "?"
                },
                "model_group": "qwen"
            },
            {
                "uniqueID": "Qwen/Qwen3-0.6B",
                "name": "Qwen3 0.6B",
                "description": "",
                "added": "2025-04-28",
                "tags": [],
                "parameters": "0.6B",
                "context": "40960",
                "architecture": "Qwen3ForCausalLM",
                "formats": [
                    "Safetensors"
                ],
                "huggingface_repo": "Qwen/Qwen3-0.6B",
                "transformers_version": "4.51.0",
                "gated": false,
                "license": "apache-2.0",
                "logo": "https://cdn-avatars.huggingface.co/v1/production/uploads/62088594a5943c8a8fc94560/y5SEKiE8TkjBKs9xfjCx5.png",
                "size_of_model_in_mb": 1447.2107467651367,
                "author": {
                    "name": "Qwen",
                    "url": "https://huggingface.co/Qwen/Qwen3-0.6B",
                    "blurb": ""
                },
                "resources": {
                    "canonicalUrl": "https://huggingface.co/Qwen/Qwen3-0.6B",
                    "downloadUrl": "https://huggingface.co/Qwen/Qwen3-0.6B",
                    "paperUrl": "?"
                },
                "model_group": "qwen"
            },
            {
                "uniqueID": "Qwen/Qwen3-235B-A22B-FP8",
                "name": "Qwen3 235B A22B FP8",
                "description": "",
                "added": "2025-04-28",
                "tags": [],
                "parameters": "235B",
                "context": "40960",
                "architecture": "Qwen3MoeForCausalLM",
                "formats": [
                    "Safetensors"
                ],
                "huggingface_repo": "Qwen/Qwen3-235B-A22B-FP8",
                "transformers_version": "4.51.0",
                "gated": false,
                "license": "apache-2.0",
                "logo": "https://cdn-avatars.huggingface.co/v1/production/uploads/62088594a5943c8a8fc94560/y5SEKiE8TkjBKs9xfjCx5.png",
                "size_of_model_in_mb": 227990.4952917099,
                "author": {
                    "name": "Qwen",
                    "url": "https://huggingface.co/Qwen/Qwen3-235B-A22B-FP8",
                    "blurb": ""
                },
                "resources": {
                    "canonicalUrl": "https://huggingface.co/Qwen/Qwen3-235B-A22B-FP8",
                    "downloadUrl": "https://huggingface.co/Qwen/Qwen3-235B-A22B-FP8",
                    "paperUrl": "?"
                },
                "model_group": "qwen"
            },
            {
                "uniqueID": "Qwen/Qwen3-30B-A3B-FP8",
                "name": "Qwen3 30B A3B FP8",
                "description": "",
                "added": "2025-04-28",
                "tags": [],
                "parameters": "30B",
                "context": "40960",
                "architecture": "Qwen3MoeForCausalLM",
                "formats": [
                    "Safetensors"
                ],
                "huggingface_repo": "Qwen/Qwen3-30B-A3B-FP8",
                "transformers_version": "4.51.0",
                "gated": false,
                "license": "apache-2.0",
                "logo": "https://cdn-avatars.huggingface.co/v1/production/uploads/62088594a5943c8a8fc94560/y5SEKiE8TkjBKs9xfjCx5.png",
                "size_of_model_in_mb": 30963.201493263245,
                "author": {
                    "name": "Qwen",
                    "url": "https://huggingface.co/Qwen/Qwen3-30B-A3B-FP8",
                    "blurb": ""
                },
                "resources": {
                    "canonicalUrl": "https://huggingface.co/Qwen/Qwen3-30B-A3B-FP8",
                    "downloadUrl": "https://huggingface.co/Qwen/Qwen3-30B-A3B-FP8",
                    "paperUrl": "?"
                },
                "model_group": "qwen"
            },
            {
                "uniqueID": "Qwen/Qwen3-32B-FP8",
                "name": "Qwen3 32B FP8",
                "description": "",
                "added": "2025-04-28",
                "tags": [],
                "parameters": "32B",
                "context": "40960",
                "architecture": "Qwen3ForCausalLM",
                "formats": [
                    "Safetensors"
                ],
                "huggingface_repo": "Qwen/Qwen3-32B-FP8",
                "transformers_version": "4.51.0",
                "gated": false,
                "license": "apache-2.0",
                "logo": "https://cdn-avatars.huggingface.co/v1/production/uploads/62088594a5943c8a8fc94560/y5SEKiE8TkjBKs9xfjCx5.png",
                "size_of_model_in_mb": 32746.197826385498,
                "author": {
                    "name": "Qwen",
                    "url": "https://huggingface.co/Qwen/Qwen3-32B-FP8",
                    "blurb": ""
                },
                "resources": {
                    "canonicalUrl": "https://huggingface.co/Qwen/Qwen3-32B-FP8",
                    "downloadUrl": "https://huggingface.co/Qwen/Qwen3-32B-FP8",
                    "paperUrl": "?"
                },
                "model_group": "qwen"
            },
            {
                "uniqueID": "Qwen/Qwen3-14B-FP8",
                "name": "Qwen3 14B FP8",
                "description": "",
                "added": "2025-04-28",
                "tags": [],
                "parameters": "14B",
                "context": "40960",
                "architecture": "Qwen3ForCausalLM",
                "formats": [
                    "Safetensors"
                ],
                "huggingface_repo": "Qwen/Qwen3-14B-FP8",
                "transformers_version": "4.51.0",
                "gated": false,
                "license": "apache-2.0",
                "logo": "https://cdn-avatars.huggingface.co/v1/production/uploads/62088594a5943c8a8fc94560/y5SEKiE8TkjBKs9xfjCx5.png",
                "size_of_model_in_mb": 15583.539617538452,
                "author": {
                    "name": "Qwen",
                    "url": "https://huggingface.co/Qwen/Qwen3-14B-FP8",
                    "blurb": ""
                },
                "resources": {
                    "canonicalUrl": "https://huggingface.co/Qwen/Qwen3-14B-FP8",
                    "downloadUrl": "https://huggingface.co/Qwen/Qwen3-14B-FP8",
                    "paperUrl": "?"
                },
                "model_group": "qwen"
            },
            {
                "uniqueID": "Qwen/Qwen3-8B-FP8",
                "name": "Qwen3 8B FP8",
                "description": "",
                "added": "2025-04-28",
                "tags": [],
                "parameters": "8B",
                "context": "40960",
                "architecture": "Qwen3ForCausalLM",
                "formats": [
                    "Safetensors"
                ],
                "huggingface_repo": "Qwen/Qwen3-8B-FP8",
                "transformers_version": "4.51.0",
                "gated": false,
                "license": "apache-2.0",
                "logo": "https://cdn-avatars.huggingface.co/v1/production/uploads/62088594a5943c8a8fc94560/y5SEKiE8TkjBKs9xfjCx5.png",
                "size_of_model_in_mb": 9013.075735092163,
                "author": {
                    "name": "Qwen",
                    "url": "https://huggingface.co/Qwen/Qwen3-8B-FP8",
                    "blurb": ""
                },
                "resources": {
                    "canonicalUrl": "https://huggingface.co/Qwen/Qwen3-8B-FP8",
                    "downloadUrl": "https://huggingface.co/Qwen/Qwen3-8B-FP8",
                    "paperUrl": "?"
                },
                "model_group": "qwen"
            },
            {
                "uniqueID": "Qwen/Qwen3-4B-FP8",
                "name": "Qwen3 4B FP8",
                "description": "",
                "added": "2025-04-28",
                "tags": [],
                "parameters": "4B",
                "context": "40960",
                "architecture": "Qwen3ForCausalLM",
                "formats": [
                    "Safetensors"
                ],
                "huggingface_repo": "Qwen/Qwen3-4B-FP8",
                "transformers_version": "4.51.0",
                "gated": false,
                "license": "apache-2.0",
                "logo": "https://cdn-avatars.huggingface.co/v1/production/uploads/62088594a5943c8a8fc94560/y5SEKiE8TkjBKs9xfjCx5.png",
                "size_of_model_in_mb": 4963.225378036499,
                "author": {
                    "name": "Qwen",
                    "url": "https://huggingface.co/Qwen/Qwen3-4B-FP8",
                    "blurb": ""
                },
                "resources": {
                    "canonicalUrl": "https://huggingface.co/Qwen/Qwen3-4B-FP8",
                    "downloadUrl": "https://huggingface.co/Qwen/Qwen3-4B-FP8",
                    "paperUrl": "?"
                },
                "model_group": "qwen"
            },
            {
                "uniqueID": "Qwen/Qwen3-1.7B-FP8",
                "name": "Qwen3 1.7B FP8",
                "description": "",
                "added": "2025-04-28",
                "tags": [],
                "parameters": "1.7B",
                "context": "40960",
                "architecture": "Qwen3ForCausalLM",
                "formats": [
                    "Safetensors"
                ],
                "huggingface_repo": "Qwen/Qwen3-1.7B-FP8",
                "transformers_version": "4.51.0",
                "gated": false,
                "license": "apache-2.0",
                "logo": "https://cdn-avatars.huggingface.co/v1/production/uploads/62088594a5943c8a8fc94560/y5SEKiE8TkjBKs9xfjCx5.png",
                "size_of_model_in_mb": 2545.0098791122437,
                "author": {
                    "name": "Qwen",
                    "url": "https://huggingface.co/Qwen/Qwen3-1.7B-FP8",
                    "blurb": ""
                },
                "resources": {
                    "canonicalUrl": "https://huggingface.co/Qwen/Qwen3-1.7B-FP8",
                    "downloadUrl": "https://huggingface.co/Qwen/Qwen3-1.7B-FP8",
                    "paperUrl": "?"
                },
                "model_group": "qwen"
            },
            {
                "uniqueID": "Qwen/Qwen3-0.6B-FP8",
                "name": "Qwen3 0.6B FP8",
                "description": "",
                "added": "2025-04-28",
                "tags": [],
                "parameters": "0.6B",
                "context": "40960",
                "architecture": "Qwen3ForCausalLM",
                "formats": [
                    "Safetensors"
                ],
                "huggingface_repo": "Qwen/Qwen3-0.6B-FP8",
                "transformers_version": "4.51.0",
                "gated": false,
                "license": "apache-2.0",
                "logo": "https://cdn-avatars.huggingface.co/v1/production/uploads/62088594a5943c8a8fc94560/y5SEKiE8TkjBKs9xfjCx5.png",
                "size_of_model_in_mb": 1027.2845468521118,
                "author": {
                    "name": "Qwen",
                    "url": "https://huggingface.co/Qwen/Qwen3-0.6B-FP8",
                    "blurb": ""
                },
                "resources": {
                    "canonicalUrl": "https://huggingface.co/Qwen/Qwen3-0.6B-FP8",
                    "downloadUrl": "https://huggingface.co/Qwen/Qwen3-0.6B-FP8",
                    "paperUrl": "?"
                },
                "model_group": "qwen"
            },
            {
                "uniqueID": "Qwen/Qwen3-30B-A3B-Base",
                "name": "Qwen3 30B A3B Base",
                "description": "",
                "added": "2025-04-28",
                "tags": [],
                "parameters": "30B",
                "context": "32768",
                "architecture": "Qwen3MoeForCausalLM",
                "formats": [
                    "Safetensors"
                ],
                "huggingface_repo": "Qwen/Qwen3-30B-A3B-Base",
                "transformers_version": "4.51.0",
                "gated": false,
                "license": "apache-2.0",
                "logo": "https://cdn-avatars.huggingface.co/v1/production/uploads/62088594a5943c8a8fc94560/y5SEKiE8TkjBKs9xfjCx5.png",
                "size_of_model_in_mb": 58248.6101732254,
                "author": {
                    "name": "Qwen",
                    "url": "https://huggingface.co/Qwen/Qwen3-30B-A3B-Base",
                    "blurb": ""
                },
                "resources": {
                    "canonicalUrl": "https://huggingface.co/Qwen/Qwen3-30B-A3B-Base",
                    "downloadUrl": "https://huggingface.co/Qwen/Qwen3-30B-A3B-Base",
                    "paperUrl": "?"
                },
                "model_group": "qwen"
            },
            {
                "uniqueID": "Qwen/Qwen3-14B-Base",
                "name": "Qwen3 14B Base",
                "description": "",
                "added": "2025-04-28",
                "tags": [],
                "parameters": "14B",
                "context": "32768",
                "architecture": "Qwen3ForCausalLM",
                "formats": [
                    "Safetensors"
                ],
                "huggingface_repo": "Qwen/Qwen3-14B-Base",
                "transformers_version": "4.51.0",
                "gated": false,
                "license": "apache-2.0",
                "logo": "https://cdn-avatars.huggingface.co/v1/production/uploads/62088594a5943c8a8fc94560/y5SEKiE8TkjBKs9xfjCx5.png",
                "size_of_model_in_mb": 28177.75838279724,
                "author": {
                    "name": "Qwen",
                    "url": "https://huggingface.co/Qwen/Qwen3-14B-Base",
                    "blurb": ""
                },
                "resources": {
                    "canonicalUrl": "https://huggingface.co/Qwen/Qwen3-14B-Base",
                    "downloadUrl": "https://huggingface.co/Qwen/Qwen3-14B-Base",
                    "paperUrl": "?"
                },
                "model_group": "qwen"
            },
            {
                "uniqueID": "Qwen/Qwen3-8B-Base",
                "name": "Qwen3 8B Base",
                "description": "",
                "added": "2025-04-28",
                "tags": [],
                "parameters": "8B",
                "context": "32768",
                "architecture": "Qwen3ForCausalLM",
                "formats": [
                    "Safetensors"
                ],
                "huggingface_repo": "Qwen/Qwen3-8B-Base",
                "transformers_version": "4.51.0",
                "gated": false,
                "license": "apache-2.0",
                "logo": "https://cdn-avatars.huggingface.co/v1/production/uploads/62088594a5943c8a8fc94560/y5SEKiE8TkjBKs9xfjCx5.png",
                "size_of_model_in_mb": 15632.02731513977,
                "author": {
                    "name": "Qwen",
                    "url": "https://huggingface.co/Qwen/Qwen3-8B-Base",
                    "blurb": ""
                },
                "resources": {
                    "canonicalUrl": "https://huggingface.co/Qwen/Qwen3-8B-Base",
                    "downloadUrl": "https://huggingface.co/Qwen/Qwen3-8B-Base",
                    "paperUrl": "?"
                },
                "model_group": "qwen"
            },
            {
                "uniqueID": "Qwen/Qwen3-4B-Base",
                "name": "Qwen3 4B Base",
                "description": "",
                "added": "2025-04-28",
                "tags": [],
                "parameters": "4B",
                "context": "32768",
                "architecture": "Qwen3ForCausalLM",
                "formats": [
                    "Safetensors"
                ],
                "huggingface_repo": "Qwen/Qwen3-4B-Base",
                "transformers_version": "4.51.0",
                "gated": false,
                "license": "apache-2.0",
                "logo": "https://cdn-avatars.huggingface.co/v1/production/uploads/62088594a5943c8a8fc94560/y5SEKiE8TkjBKs9xfjCx5.png",
                "size_of_model_in_mb": 7681.688153266907,
                "author": {
                    "name": "Qwen",
                    "url": "https://huggingface.co/Qwen/Qwen3-4B-Base",
                    "blurb": ""
                },
                "resources": {
                    "canonicalUrl": "https://huggingface.co/Qwen/Qwen3-4B-Base",
                    "downloadUrl": "https://huggingface.co/Qwen/Qwen3-4B-Base",
                    "paperUrl": "?"
                },
                "model_group": "qwen"
            },
            {
                "uniqueID": "Qwen/Qwen3-1.7B-Base",
                "name": "Qwen3 1.7B Base",
                "description": "",
                "added": "2025-04-28",
                "tags": [],
                "parameters": "1.7B",
                "context": "32768",
                "architecture": "Qwen3ForCausalLM",
                "formats": [
                    "Safetensors"
                ],
                "huggingface_repo": "Qwen/Qwen3-1.7B-Base",
                "transformers_version": "4.51.0",
                "gated": false,
                "license": "apache-2.0",
                "logo": "https://cdn-avatars.huggingface.co/v1/production/uploads/62088594a5943c8a8fc94560/y5SEKiE8TkjBKs9xfjCx5.png",
                "size_of_model_in_mb": 3291.1344776153564,
                "author": {
                    "name": "Qwen",
                    "url": "https://huggingface.co/Qwen/Qwen3-1.7B-Base",
                    "blurb": ""
                },
                "resources": {
                    "canonicalUrl": "https://huggingface.co/Qwen/Qwen3-1.7B-Base",
                    "downloadUrl": "https://huggingface.co/Qwen/Qwen3-1.7B-Base",
                    "paperUrl": "?"
                },
                "model_group": "qwen"
            },
            {
                "uniqueID": "Qwen/Qwen3-0.6B-Base",
                "name": "Qwen3 0.6B Base",
                "description": "",
                "added": "2025-04-28",
                "tags": [],
                "parameters": "0.6B",
                "context": "32768",
                "architecture": "Qwen3ForCausalLM",
                "formats": [
                    "Safetensors"
                ],
                "huggingface_repo": "Qwen/Qwen3-0.6B-Base",
                "transformers_version": "4.51.0",
                "gated": false,
                "license": "apache-2.0",
                "logo": "https://cdn-avatars.huggingface.co/v1/production/uploads/62088594a5943c8a8fc94560/y5SEKiE8TkjBKs9xfjCx5.png",
                "size_of_model_in_mb": 1146.2727680206299,
                "author": {
                    "name": "Qwen",
                    "url": "https://huggingface.co/Qwen/Qwen3-0.6B-Base",
                    "blurb": ""
                },
                "resources": {
                    "canonicalUrl": "https://huggingface.co/Qwen/Qwen3-0.6B-Base",
                    "downloadUrl": "https://huggingface.co/Qwen/Qwen3-0.6B-Base",
                    "paperUrl": "?"
                },
                "model_group": "qwen"
            },
            {
                "uniqueID": "Qwen/QwQ-32B",
                "name": "Qwen QwQ 32B",
                "description": "QwQ is the reasoning model of the Qwen series. Compared with conventional instruction-tuned models, QwQ, which is capable of thinking and reasoning, can achieve significantly enhanced performance in downstream tasks, especially hard problems.",
                "added": "2025-03-25",
                "tags": [
                    "Reasoning"
                ],
                "parameters": "32B",
                "context": "40960",
                "architecture": "Qwen2ForCausalLM",
                "formats": [
                    "Safetensors"
                ],
                "huggingface_repo": "Qwen/QwQ-32B",
                "transformers_version": "4.43.1",
                "gated": false,
                "license": "apache-2.0",
                "logo": "https://cdn-avatars.huggingface.co/v1/production/uploads/62088594a5943c8a8fc94560/y5SEKiE8TkjBKs9xfjCx5.png",
                "size_of_model_in_mb": 62501.64478683472,
                "author": {
                    "name": "Qwen",
                    "url": "https://huggingface.co/Qwen/QwQ-32B",
                    "blurb": ""
                },
                "resources": {
                    "canonicalUrl": "https://huggingface.co/Qwen/QwQ-32B",
                    "downloadUrl": "https://huggingface.co/Qwen/QwQ-32B",
                    "paperUrl": "?"
                },
                "model_group": "qwen"
            },
            {
                "uniqueID": "mlx-community/QwQ-32B-4bit",
                "name": "Qwen QwQ 32B (MLX 4bit)",
                "description": "MLX-formatted version of Qwen's QwQ model quantized to 4 bits.",
                "added": "2025-03-25",
                "tags": [
                    "Reasoning"
                ],
                "parameters": "32B",
                "context": "131072",
                "architecture": "Qwen2ForCausalLM",
                "formats": [
                    "Safetensors"
                ],
                "huggingface_repo": "mlx-community/QwQ-32B-4bit",
                "transformers_version": "4.43.1",
                "gated": false,
                "license": "apache-2.0",
                "logo": "https://cdn-avatars.huggingface.co/v1/production/uploads/62088594a5943c8a8fc94560/y5SEKiE8TkjBKs9xfjCx5.png",
                "size_of_model_in_mb": 17591.31551837921,
                "author": {
                    "name": "mlx-community",
                    "url": "https://huggingface.co/mlx-community/QwQ-32B-4bit",
                    "blurb": ""
                },
                "resources": {
                    "canonicalUrl": "https://huggingface.co/mlx-community/QwQ-32B-4bit",
                    "downloadUrl": "https://huggingface.co/mlx-community/QwQ-32B-4bit",
                    "paperUrl": "?"
                },
                "model_group": "qwen"
            },
            {
                "uniqueID": "Qwen/QwQ-32B-GGUF",
                "name": "QwQ 32B (GGUF Q4_K_M)",
                "description": "GGUF formatted version of Qwen's QwQ model quantized to 4-bits.",
                "added": "2025-03-25",
                "tags": [
                    "Reasoning"
                ],
                "parameters": "32B",
                "context": "131072",
                "architecture": "Qwen2ForCausalLM",
                "formats": [
                    "GGUF"
                ],
                "huggingface_repo": "Qwen/QwQ-32B-GGUF",
                "huggingface_filename": "qwq-32b-q4_k_m.gguf",
                "transformers_version": "4.43.1",
                "gated": false,
                "license": "apache-2.0",
                "logo": "https://cdn-avatars.huggingface.co/v1/production/uploads/62088594a5943c8a8fc94560/y5SEKiE8TkjBKs9xfjCx5.png",
                "size_of_model_in_mb": 20377.6,
                "author": {
                    "name": "Qwen",
                    "url": "https://huggingface.co/Qwen/QwQ-32B-GGUF",
                    "blurb": ""
                },
                "resources": {
                    "canonicalUrl": "https://huggingface.co/Qwen/QwQ-32B-GGUF",
                    "downloadUrl": "https://huggingface.co/Qwen/QwQ-32B-GGUF",
                    "paperUrl": "?"
                },
                "model_group": "qwen"
            }
        ]
    },
    {
        "name": "starling",
        "description": "General-purpose, designed for engaging instruction-following dialogues.",
        "tags": [],
        "models": [
            {
                "uniqueID": "berkeley-nest/Starling-LM-7B-alpha",
                "name": "Starling-LM-7B-alpha",
                "description": "We introduce Starling-7B, an open large language model (LLM) trained by Reinforcement Learning from AI Feedback (RLAIF). The model harnesses the power of our new GPT-4 labeled ranking dataset, berkeley-nest/Nectar, and our new reward training and policy tuning pipeline. Starling-7B-alpha scores 8.09 in MT Bench with GPT-4 as a judge, outperforming every model to date on MT-Bench except for OpenAI's GPT-4 and GPT-4 Turbo.",
                "parameters": "7B",
                "context": "4k",
                "architecture": "MistralForCausalLM",
                "formats": [
                    "Safetensors"
                ],
                "huggingface_repo": "berkeley-nest/Starling-LM-7B-alpha",
                "transformers_version": "4.35.0",
                "gated": false,
                "license": "Non commercial license",
                "logo": "https://starling.cs.berkeley.edu/starling.png",
                "size_of_model_in_mb": 13814.78,
                "author": {
                    "name": "Berkeley",
                    "url": "https://starling.cs.berkeley.edu/",
                    "blurb": ""
                },
                "resources": {
                    "canonicalUrl": "https://starling.cs.berkeley.edu/",
                    "downloadUrl": "https://huggingface.co/berkeley-nest/Starling-LM-7B-alpha",
                    "paperUrl": "?"
                },
                "model_group": "starling"
            }
        ]
    },
    {
        "name": "fastchat-t5",
        "description": "Lightweight chatbot model optimized for fast response generation using the T5 architecture.",
        "tags": [],
        "models": [
            {
                "uniqueID": "lmsys/fastchat-t5-3b-v1.0",
                "name": "FastChat T5",
                "description": "Open-source chatbot trained by fine-tuning Flan-t5-xl (3B parameters) on user-shared conversations collected from ShareGPT. It is based on an encoder-decoder transformer architecture, and can autoregressively generate responses to users' inputs.",
                "parameters": "3B",
                "context": "4k",
                "architecture": "T5ForConditionalGeneration",
                "formats": [
                    "PyTorch"
                ],
                "huggingface_repo": "lmsys/fastchat-t5-3b-v1.0",
                "transformers_version": "4.28.1",
                "gated": false,
                "license": "Apache 2.0",
                "logo": "https://lmsys.org/images/blog/vicuna/vicuna.jpeg",
                "size_of_model_in_mb": 6395.57,
                "author": {
                    "name": "LMSYS Org",
                    "url": "https://lmsys.org/",
                    "blurb": "Large Model Systems Organization (LMSYS Org) is an open research organization founded by students and faculty from UC Berkeley in collaboration with UCSD and CMU."
                },
                "resources": {
                    "canonicalUrl": "https://huggingface.co/lmsys/fastchat-t5-3b-v1.0",
                    "downloadUrl": "https://huggingface.co/lmsys/fastchat-t5-3b-v1.0",
                    "paperUrl": "?"
                },
                "model_group": "fastchat-t5"
            }
        ]
    },
    {
        "name": "bytedance",
        "description": "Parameter efficient family for reasoning and coding",
        "tags": [
            "Coding",
            "Reasoning"
        ],
        "models": [
            {
                "uniqueID": "ByteDance-Seed/Seed-Coder-8B-Reasoning-bf16",
                "name": "Seed-Coder-8B-Reasoning-bf16",
                "description": "A powerful, transparent, and parameter-efficient open-source code model",
                "added": "2025-05-21",
                "tags": [
                    "Reasoning",
                    "Coding"
                ],
                "parameters": "8B",
                "context": "65536",
                "architecture": "LlamaForCausalLM",
                "formats": [
                    "Safetensors"
                ],
                "huggingface_repo": "ByteDance-Seed/Seed-Coder-8B-Reasoning-bf16",
                "transformers_version": "4.51.3",
                "gated": false,
                "license": "mit",
                "logo": "",
                "size_of_model_in_mb": 15747.93223285675,
                "author": {
                    "name": "ByteDance-Seed",
                    "url": "https://huggingface.co/ByteDance-Seed/Seed-Coder-8B-Reasoning-bf16",
                    "blurb": ""
                },
                "resources": {
                    "canonicalUrl": "https://huggingface.co/ByteDance-Seed/Seed-Coder-8B-Reasoning-bf16",
                    "downloadUrl": "https://huggingface.co/ByteDance-Seed/Seed-Coder-8B-Reasoning-bf16",
                    "paperUrl": "?"
                },
                "model_group": "bytedance"
            }
        ]
    },
    {
        "name": "command",
        "description": "Robust and instruction-following. Designed for enterprise-scale NLP tasks.",
        "tags": [
            "Enterprise Focus"
        ],
        "models": [
            {
                "uniqueID": "CohereForAI/c4ai-command-a-03-2025",
                "name": "C4AI Command A 03-2025",
                "description": "C4AI Command A is an open weights research release of a 111 billion parameter model optimized for demanding enterprises that require fast, secure, and high-quality AI. Compared to other leading proprietary and open-weights models Command A delivers maximum performance with minimum hardware costs, excelling on business-critical agentic and multilingual tasks while being deployable on just two GPUs.",
                "parameters": "111B",
                "context": "16384",
                "architecture": "Cohere2ForCausalLM",
                "formats": [
                    "Safetensors"
                ],
                "huggingface_repo": "CohereForAI/c4ai-command-a-03-2025",
                "transformers_version": "4.48.0.dev0",
                "gated": "manual",
                "license": "cc-by-nc-4.0",
                "logo": "https://avatars.githubusercontent.com/u/54850923?s=200&v=4",
                "size_of_model_in_mb": 222135,
                "author": {
                    "name": "CohereForAI",
                    "url": "https://huggingface.co/CohereForAI/c4ai-command-a-03-2025",
                    "blurb": ""
                },
                "resources": {
                    "canonicalUrl": "https://huggingface.co/CohereForAI/c4ai-command-a-03-2025",
                    "downloadUrl": "https://huggingface.co/CohereForAI/c4ai-command-a-03-2025",
                    "paperUrl": "?"
                },
                "model_group": "command"
            },
            {
                "uniqueID": "CohereForAI/c4ai-command-r-v01",
                "name": "C4AI Command-R",
                "description": "C4AI Command-R is a research release of a 35 billion parameter highly performant generative model. Command-R is a large language model with open weights optimized for a variety of use cases including reasoning, summarization, and question answering. Command-R has the capability for multilingual generation evaluated in 10 languages and highly performant RAG capabilities.",
                "parameters": "35B",
                "context": "8192",
                "architecture": "CohereForCausalLM",
                "formats": [
                    "Safetensors"
                ],
                "huggingface_repo": "CohereForAI/c4ai-command-r-v01",
                "transformers_version": "4.38.2",
                "gated": true,
                "license": "CC-BY-NC-4.0",
                "logo": "https://cohere-ai.ghost.io/content/images/2024/03/CMDR--1--1-1.png",
                "size_of_model_in_mb": 66732.91,
                "author": {
                    "name": "Cohere 4 AI",
                    "url": "https://cohere.com/research",
                    "blurb": ""
                },
                "resources": {
                    "canonicalUrl": "https://huggingface.co/CohereForAI/c4ai-command-r-v01",
                    "downloadUrl": "https://huggingface.co/CohereForAI/c4ai-command-r-v01",
                    "paperUrl": "?"
                },
                "model_group": "command"
            },
            {
                "uniqueID": "mlx-community/c4ai-command-r-v01-4bit",
                "name": "C4AI Command-R (MLX 4-bit)",
                "description": "C4AI Command-R 4bit for MLX is a research release of a 35 billion parameter highly performant generative model. Command-R is a large language model with open weights optimized for a variety of use cases including reasoning, summarization, and question answering. Command-R has the capability for multilingual generation evaluated in 10 languages and highly performant RAG capabilities.",
                "parameters": "35B",
                "context": "8192",
                "architecture": "MLX",
                "formats": [
                    "Safetensors"
                ],
                "huggingface_repo": "mlx-community/c4ai-command-r-v01-4bit",
                "transformers_version": "4.38.2",
                "gated": false,
                "license": "CC-BY-NC-4.0",
                "logo": "https://cohere-ai.ghost.io/content/images/2024/03/CMDR--1--1-1.png",
                "size_of_model_in_mb": 21653.11,
                "author": {
                    "name": "Cohere 4 AI",
                    "url": "https://cohere.com/research",
                    "blurb": ""
                },
                "resources": {
                    "canonicalUrl": "https://huggingface.co/mlx-community/c4ai-command-r-v01-4bit",
                    "downloadUrl": "https://huggingface.co/mlx-community/c4ai-command-r-v01-4bit",
                    "paperUrl": "?"
                },
                "model_group": "command"
            },
            {
                "uniqueID": "CohereForAI/c4ai-command-r-v01-4bit",
                "name": "C4AI Command-R 4bit Bits and Bytes",
                "description": "C4AI Command-R 4bit for MLX is a research release of a 35 billion parameter highly performant generative model. Command-R is a large language model with open weights optimized for a variety of use cases including reasoning, summarization, and question answering. Command-R has the capability for multilingual generation evaluated in 10 languages and highly performant RAG capabilities.",
                "parameters": "35B",
                "context": "8192",
                "architecture": "CohereForCausalLM",
                "formats": [
                    "Safetensors"
                ],
                "huggingface_repo": "CohereForAI/c4ai-command-r-v01-4bit",
                "transformers_version": "4.38.2",
                "gated": true,
                "license": "CC-BY-NC-4.0",
                "logo": "https://cohere-ai.ghost.io/content/images/2024/03/CMDR--1--1-1.png",
                "size_of_model_in_mb": 21653.17,
                "author": {
                    "name": "Cohere 4 AI",
                    "url": "https://cohere.com/research",
                    "blurb": ""
                },
                "resources": {
                    "canonicalUrl": "https://huggingface.co/CohereForAI/c4ai-command-r-v01-4bit",
                    "downloadUrl": "https://huggingface.co/CohereForAI/c4ai-command-r-v01-4bit",
                    "paperUrl": "?"
                },
                "model_group": "command"
            }
        ]
    },
    {
        "name": "zyphra",
        "description": "Small yet powerful, excelling in code and math reasoning with high token efficiency.",
        "tags": [
            "Reasoning",
            "Math",
            "Coding"
        ],
        "models": [
            {
                "uniqueID": "Zyphra/ZR1-1.5B",
                "name": "ZR1-1.5B",
                "description": "This small but powerful reasoning model excels at both math and code, making it one of the best models in these categories for its size. It also uses 60% less reasoning tokens than comparable models.",
                "added": "2025-04-10",
                "tags": [],
                "parameters": "1.5B",
                "context": "131072",
                "architecture": "Qwen2ForCausalLM",
                "formats": [
                    "Safetensors"
                ],
                "huggingface_repo": "Zyphra/ZR1-1.5B",
                "transformers_version": "4.50.3",
                "gated": false,
                "license": "mit",
                "logo": "https://cdn-avatars.huggingface.co/v1/production/uploads/62088594a5943c8a8fc94560/y5SEKiE8TkjBKs9xfjCx5.png",
                "size_of_model_in_mb": 6790.015851974487,
                "author": {
                    "name": "Zyphra",
                    "url": "https://huggingface.co/Zyphra/ZR1-1.5B",
                    "blurb": ""
                },
                "resources": {
                    "canonicalUrl": "https://huggingface.co/Zyphra/ZR1-1.5B",
                    "downloadUrl": "https://huggingface.co/Zyphra/ZR1-1.5B",
                    "paperUrl": "?"
                },
                "model_group": "zyphra"
            }
        ]
    },
    {
        "name": "phi",
        "description": "Small and efficient. Ideal for edge deployment and low-resource inference.",
        "tags": [],
        "models": [
            {
                "uniqueID": "microsoft/Phi-3-mini-128k-instruct",
                "name": "Phi 3 Mini 128K Instruct",
                "description": "The Phi-3-Mini-128K-Instruct is a 3.8B parameters, lightweight, state-of-the-art open model trained with the Phi-3 datasets that includes both synthetic data and the filtered publicly available websites data with a focus on high-quality and reasoning dense properties. The model belongs to the Phi-3 family with the Mini version in two variants 4K and 128K which is the context length (in tokens) that it can support. The model has underwent a post-training process that incorporates both supervised fine-tuning and direct preference optimization for the instruction following and safety measures. When assessed against benchmarks testing common sense, language understanding, math, code, long context and logical reasoning, Phi-3 Mini-4K-Instruct showcased a robust and state-of-the-art performance among models with less than 13 billion parameters.",
                "parameters": "3.8B",
                "context": "128K",
                "architecture": "Phi3ForCausalLM",
                "formats": [
                    "Safetensors"
                ],
                "huggingface_repo": "microsoft/Phi-3-mini-128k-instruct",
                "transformers_version": "4.39.3",
                "gated": "auto",
                "license": "MIT",
                "logo": "https://blogs.microsoft.com/wp-content/uploads/prod/2012/08/8867.Microsoft_5F00_Logo_2D00_for_2D00_screen.jpg",
                "size_of_model_in_mb": 7290.59,
                "author": {
                    "name": "Microsoft",
                    "url": "https://huggingface.co/papers/2404.14219",
                    "blurb": ""
                },
                "resources": {
                    "canonicalUrl": "https://huggingface.co/microsoft/Phi-3-mini-128k-instruct",
                    "downloadUrl": "https://huggingface.co/microsoft/Phi-3-mini-128k-instruct",
                    "paperUrl": "https://huggingface.co/papers/2404.14219"
                },
                "model_group": "phi"
            },
            {
                "uniqueID": "microsoft/Phi-3-mini-4k-instruct",
                "name": "Phi 3 Mini 4K Instruct",
                "description": "The Phi-3-Mini-4K-Instruct is a 3.8B parameters, lightweight, state-of-the-art open model trained with the Phi-3 datasets that includes both synthetic data and the filtered publicly available websites data with a focus on high-quality and reasoning dense properties. The model belongs to the Phi-3 family with the Mini version in two variants 4K and 128K which is the context length (in tokens) that it can support. The model has underwent a post-training process that incorporates both supervised fine-tuning and direct preference optimization for the instruction following and safety measures. When assessed against benchmarks testing common sense, language understanding, math, code, long context and logical reasoning, Phi-3 Mini-4K-Instruct showcased a robust and state-of-the-art performance among models with less than 13 billion parameters.",
                "parameters": "3.8B",
                "context": "4K",
                "architecture": "Phi3ForCausalLM",
                "formats": [
                    "Safetensors"
                ],
                "huggingface_repo": "microsoft/Phi-3-mini-4k-instruct",
                "transformers_version": "4.39.3",
                "gated": "auto",
                "license": "MIT",
                "logo": "https://blogs.microsoft.com/wp-content/uploads/prod/2012/08/8867.Microsoft_5F00_Logo_2D00_for_2D00_screen.jpg",
                "size_of_model_in_mb": 7290.58,
                "author": {
                    "name": "Microsoft",
                    "url": "https://huggingface.co/papers/2404.14219",
                    "blurb": ""
                },
                "resources": {
                    "canonicalUrl": "https://huggingface.co/microsoft/Phi-3-mini-128k-instruct",
                    "downloadUrl": "https://huggingface.co/microsoft/Phi-3-mini-128k-instruct",
                    "paperUrl": "https://huggingface.co/papers/2404.14219"
                },
                "model_group": "phi"
            },
            {
                "uniqueID": "mlx-community/phi-4-4bit",
                "name": "Phi-4 (MLX 4-bit)",
                "description": "MLX export of Microsoft's Phi-4 using 4-bit quantization.",
                "added": "2025-1-20",
                "parameters": "14B",
                "context": "16384",
                "architecture": "MLX",
                "formats": [
                    "Safetensors"
                ],
                "huggingface_repo": "mlx-community/phi-4-4bit",
                "transformers_version": "4.47.0",
                "gated": false,
                "license": "MIT",
                "logo": "https://blogs.microsoft.com/wp-content/uploads/prod/2012/08/8867.Microsoft_5F00_Logo_2D00_for_2D00_screen.jpg",
                "size_of_model_in_mb": 7873.03,
                "author": {
                    "name": "mlx-community",
                    "url": "https://huggingface.co/mlx-community/phi-4-4bit",
                    "blurb": ""
                },
                "resources": {
                    "canonicalUrl": "https://huggingface.co/mlx-community/phi-4-4bit",
                    "downloadUrl": "https://huggingface.co/mlx-community/phi-4-4bit",
                    "paperUrl": "?"
                },
                "model_group": "phi"
            },
            {
                "uniqueID": "phi-4-q4.gguf",
                "name": "Phi-4 GGUF - 4bit",
                "description": "GGUF export of Microsoft's Phi-4 using 4-bit quantization.",
                "added": "2025-1-20",
                "parameters": "14B",
                "context": "16384",
                "architecture": "GGUF",
                "formats": [
                    "GGUF"
                ],
                "huggingface_repo": "microsoft/phi-4-gguf",
                "huggingface_filename": "phi-4-q4.gguf",
                "gated": false,
                "license": "MIT",
                "logo": "https://blogs.microsoft.com/wp-content/uploads/prod/2012/08/8867.Microsoft_5F00_Logo_2D00_for_2D00_screen.jpg",
                "size_of_model_in_mb": 9267.2,
                "author": {
                    "name": "microsoft",
                    "url": "https://huggingface.co/microsoft/phi-4",
                    "blurb": ""
                },
                "resources": {
                    "canonicalUrl": "https://huggingface.co/microsoft/phi-4",
                    "downloadUrl": "https://huggingface.co/microsoft/phi-4",
                    "paperUrl": "?"
                },
                "model_group": "phi"
            },
            {
                "uniqueID": "microsoft/Phi-4-reasoning",
                "name": "Phi-4-reasoning",
                "description": "Phi-4-reasoning is a state-of-the-art open-weight reasoning model finetuned from Phi-4 using supervised fine-tuning on a dataset of chain-of-thought traces and reinforcement learning. The supervised fine-tuning dataset includes a blend of synthetic prompts and high-quality filtered data from public domain websites, focused on math, science, and coding skills as well as alignment data for safety and Responsible AI. The goal of this approach was to ensure that small capable models were trained with data focused on high quality and advanced reasoning.",
                "added": "2025-05-01",
                "tags": [
                    "Reasoning"
                ],
                "parameters": "",
                "context": "32768",
                "architecture": "Phi3ForCausalLM",
                "formats": [
                    "Safetensors"
                ],
                "huggingface_repo": "microsoft/Phi-4-reasoning",
                "transformers_version": "4.51.1",
                "gated": false,
                "license": "mit",
                "logo": "https://blogs.microsoft.com/wp-content/uploads/prod/2012/08/8867.Microsoft_5F00_Logo_2D00_for_2D00_screen.jpg",
                "size_of_model_in_mb": 27969.21728038788,
                "author": {
                    "name": "microsoft",
                    "url": "https://huggingface.co/microsoft/Phi-4-reasoning",
                    "blurb": ""
                },
                "resources": {
                    "canonicalUrl": "https://huggingface.co/microsoft/Phi-4-reasoning",
                    "downloadUrl": "https://huggingface.co/microsoft/Phi-4-reasoning",
                    "paperUrl": "?"
                },
                "model_group": "phi"
            },
            {
                "uniqueID": "microsoft/Phi-4-reasoning-plus",
                "name": "Phi-4-reasoning-plus",
                "description": "On top of Phi-4-reasoning, Phi-4-reasoning-plus has been trained additionally with Reinforcement Learning, hence, it has higher accuracy but generates on average 50% more tokens, thus having higher latency.",
                "added": "2025-05-01",
                "tags": [
                    "Reasoning"
                ],
                "parameters": "",
                "context": "32768",
                "architecture": "Phi3ForCausalLM",
                "formats": [
                    "Safetensors"
                ],
                "huggingface_repo": "microsoft/Phi-4-reasoning-plus",
                "transformers_version": "4.51.1",
                "gated": false,
                "license": "mit",
                "logo": "https://blogs.microsoft.com/wp-content/uploads/prod/2012/08/8867.Microsoft_5F00_Logo_2D00_for_2D00_screen.jpg",
                "size_of_model_in_mb": 27969.21728038788,
                "author": {
                    "name": "microsoft",
                    "url": "https://huggingface.co/microsoft/Phi-4-reasoning-plus",
                    "blurb": ""
                },
                "resources": {
                    "canonicalUrl": "https://huggingface.co/microsoft/Phi-4-reasoning-plus",
                    "downloadUrl": "https://huggingface.co/microsoft/Phi-4-reasoning-plus",
                    "paperUrl": "?"
                },
                "model_group": "phi"
            },
            {
                "uniqueID": "microsoft/phi-4",
                "name": "Phi-4",
                "description": "Fourth generation of Microsoft's Phi model. Text, best suited for prompts in the chat format.",
                "added": "2025-1-20",
                "parameters": "14B",
                "context": "16384",
                "architecture": "Phi3ForCausalLM",
                "formats": [
                    "Safetensors"
                ],
                "huggingface_repo": "microsoft/phi-4",
                "transformers_version": "4.47.0",
                "gated": false,
                "license": "MIT",
                "logo": "https://blogs.microsoft.com/wp-content/uploads/prod/2012/08/8867.Microsoft_5F00_Logo_2D00_for_2D00_screen.jpg",
                "size_of_model_in_mb": 27966.45,
                "author": {
                    "name": "microsoft",
                    "url": "https://huggingface.co/microsoft/phi-4",
                    "blurb": ""
                },
                "resources": {
                    "canonicalUrl": "https://huggingface.co/microsoft/phi-4",
                    "downloadUrl": "https://huggingface.co/microsoft/phi-4",
                    "paperUrl": "?"
                },
                "model_group": "phi"
            },
            {
                "uniqueID": "microsoft/phi-2",
                "name": "Phi 2",
                "archived": true,
                "description": "Phi-2 is a Transformer with 2.7 billion parameters. The model hasn't been fine-tuned through reinforcement learning from human feedback. The intention behind crafting this open-source model is to provide the research community with a non-restricted small model to explore vital safety challenges, such as reducing toxicity, understanding societal biases, enhancing controllability, and more.",
                "parameters": "2B",
                "context": "?k",
                "architecture": "PhiForCausalLM",
                "formats": [
                    "Safetensors"
                ],
                "huggingface_repo": "microsoft/phi-2",
                "transformers_version": "4.37.0.dev0",
                "gated": false,
                "license": "MIT",
                "logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRTwt9xAK7ih0gG74r3XLTpbiWmcd-9PTwXAQ&usqp=CAU",
                "size_of_model_in_mb": 5304.69,
                "author": {
                    "name": "Microsoft",
                    "url": "https://www.microsoft.com/en-us/research/blog/phi-2-the-surprising-power-of-small-language-models/",
                    "blurb": ""
                },
                "resources": {
                    "canonicalUrl": "https://huggingface.co/microsoft/phi-2",
                    "downloadUrl": "https://huggingface.co/microsoft/phi-2",
                    "paperUrl": "https://www.microsoft.com/en-us/research/blog/phi-2-the-surprising-power-of-small-language-models/"
                },
                "model_group": "phi"
            }
        ]
    },
    {
        "name": "nous-hermes",
        "description": "Refined instruction-tuned, capable of high-quality chat and reasoning outputs.",
        "tags": [
            "Reasoning"
        ],
        "models": [
            {
                "uniqueID": "Nous-Hermes-13b",
                "name": "Nous Hermes 13b",
                "description": "Extremely good model. Instruction based. Gives long responses. Curated with 300,000 uncensored instructions. Trained by Nous Research. Cannot be used commercially",
                "parameters": "13B",
                "context": "?",
                "architecture": "LlamaForCausalLM",
                "formats": [
                    "PyTorch"
                ],
                "huggingface_repo": "NousResearch/Nous-Hermes-13b",
                "transformers_version": "4.29.2",
                "gated": false,
                "license": "GPL",
                "logo": "https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62fc06a221c444a56f7cc595/ngyQKdhaykfC8lN6Jfqlz.png?w=200&h=200&f=face",
                "size_of_model_in_mb": 24826.4,
                "author": {
                    "name": "Nous Research",
                    "url": "https://nousresearch.com/",
                    "blurb": "We are dedicated to advancing the field of natural language processing, in collaboration with the open-source community, through bleeding-edge research and a commitment to symbiotic development."
                },
                "resources": {
                    "canonicalUrl": "https://huggingface.co/NousResearch/Nous-Hermes-13b",
                    "downloadUrl": "https://huggingface.co/NousResearch/Nous-Hermes-13b",
                    "paperUrl": "?"
                },
                "model_group": "nous-hermes"
            },
            {
                "uniqueID": "NousResearch/Nous-Hermes-2-Mixtral-8x7B-DPO",
                "name": "Nous Hermes 2 - Mixtral 8x7B - DPO",
                "description": "Nous Hermes 2 Mixtral 8x7B DPO is the new flagship Nous Research model trained over the Mixtral 8x7B MoE LLM.",
                "parameters": "7B",
                "context": "4k",
                "architecture": "MixtralForCausalLM",
                "formats": [
                    "Safetensors"
                ],
                "huggingface_repo": "NousResearch/Nous-Hermes-2-Mixtral-8x7B-DPO",
                "transformers_version": "4.37.0.dev0",
                "gated": false,
                "license": "Apache 2.0",
                "logo": "https://pbs.twimg.com/profile_images/1722061115453272064/dydqIH88_400x400.jpg",
                "size_of_model_in_mb": 89080.94,
                "author": {
                    "name": "Nous Research",
                    "url": "https://nousresearch.com/",
                    "blurb": ""
                },
                "resources": {
                    "canonicalUrl": "https://huggingface.co/NousResearch/Nous-Hermes-2-Mixtral-8x7B-DPO",
                    "downloadUrl": "https://huggingface.co/NousResearch/Nous-Hermes-2-Mixtral-8x7B-DPO",
                    "paperUrl": "?"
                },
                "model_group": "nous-hermes"
            }
        ]
    },
    {
        "name": "tinydolphin",
        "description": "Experimental and compact, trained on curated instruction datasets.",
        "tags": [
            "Tiny"
        ],
        "models": [
            {
                "uniqueID": "mlx-community/TinyDolphin-2.8-1.1b-4bit-mlx",
                "name": "TinyDolphin 2.8 1.1B (MLX 4bit)",
                "description": "This is an experimental model trained on 2 3090's by Kearm on the new Dolphin 2.8 dataset by Eric Hartford https://erichartford.com/dolphin.",
                "parameters": "1.1B",
                "context": "4k",
                "architecture": "MLX",
                "formats": [
                    "Safetensors"
                ],
                "huggingface_repo": "mlx-community/TinyDolphin-2.8-1.1b-4bit-mlx",
                "transformers_version": "4.34.0.dev0",
                "gated": "auto",
                "license": "Apache 2.0",
                "logo": "https://ml-explore.github.io/mlx/build/html/_static/mlx_logo.png",
                "size_of_model_in_mb": 772.23,
                "author": {
                    "name": "",
                    "url": "",
                    "blurb": ""
                },
                "resources": {
                    "canonicalUrl": "",
                    "downloadUrl": "",
                    "paperUrl": "?"
                },
                "model_group": "tinydolphin"
            }
        ]
    },
    {
        "name": "deepseek",
        "description": "General-purpose language models focused on strong downstream task performance.",
        "tags": [],
        "models": [
            {
                "uniqueID": "deepseek-ai/DeepSeek-R1-0528-Qwen3-8B",
                "name": "DeepSeek-R1-0528-Qwen3-8B",
                "description": "The DeepSeek R1 model has undergone a minor version upgrade, with the current version being DeepSeek-R1-0528.",
                "added": "2025-05-30",
                "tags": [
                    "Reasoning"
                ],
                "parameters": "8B",
                "context": "131072",
                "architecture": "Qwen3ForCausalLM",
                "formats": [
                    "Safetensors"
                ],
                "huggingface_repo": "deepseek-ai/DeepSeek-R1-0528-Qwen3-8B",
                "transformers_version": "4.51.0",
                "gated": false,
                "license": "mit",
                "logo": "https://www.deepseek.com/favicon.ico",
                "size_of_model_in_mb": 15629.375182151794,
                "author": {
                    "name": "deepseek-ai",
                    "url": "https://huggingface.co/deepseek-ai/DeepSeek-R1-0528-Qwen3-8B",
                    "blurb": ""
                },
                "resources": {
                    "canonicalUrl": "https://huggingface.co/deepseek-ai/DeepSeek-R1-0528-Qwen3-8B",
                    "downloadUrl": "https://huggingface.co/deepseek-ai/DeepSeek-R1-0528-Qwen3-8B",
                    "paperUrl": "?"
                },
                "model_group": "deepseek"
            },
            {
                "uniqueID": "deepseek-ai/DeepSeek-R1-0528",
                "name": "DeepSeek-R1-0528",
                "description": "The DeepSeek R1 model has undergone a minor version upgrade, with the current version being DeepSeek-R1-0528.",
                "added": "2025-05-30",
                "tags": [
                    "Reasoning"
                ],
                "parameters": "",
                "context": "163840",
                "architecture": "DeepseekV3ForCausalLM",
                "formats": [
                    "Safetensors"
                ],
                "huggingface_repo": "deepseek-ai/DeepSeek-R1-0528",
                "transformers_version": "4.46.3",
                "gated": false,
                "license": "mit",
                "logo": "https://www.deepseek.com/favicon.ico",
                "size_of_model_in_mb": 656703.5342264175,
                "author": {
                    "name": "deepseek-ai",
                    "url": "https://huggingface.co/deepseek-ai/DeepSeek-R1-0528",
                    "blurb": ""
                },
                "resources": {
                    "canonicalUrl": "https://huggingface.co/deepseek-ai/DeepSeek-R1-0528",
                    "downloadUrl": "https://huggingface.co/deepseek-ai/DeepSeek-R1-0528",
                    "paperUrl": "?"
                },
                "model_group": "deepseek"
            },
            {
                "uniqueID": "deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B",
                "name": "DeepSeek R1 Distill Qwen 1.5B",
                "description": "Qwen distillation of DeepSeek chain-of-thought model.",
                "added": "2025-01-27",
                "parameters": "1.5B",
                "tags": [
                    "Chain of Thought"
                ],
                "context": "131072",
                "architecture": "Qwen2ForCausalLM",
                "formats": [
                    "Safetensors"
                ],
                "huggingface_repo": "deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B",
                "transformers_version": "4.44.0",
                "gated": false,
                "license": "MIT",
                "logo": "https://www.deepseek.com/favicon.ico",
                "size_of_model_in_mb": 3554.21,
                "author": {
                    "name": "deepseek-ai",
                    "url": "https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B",
                    "blurb": ""
                },
                "resources": {
                    "canonicalUrl": "https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B",
                    "downloadUrl": "https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B",
                    "paperUrl": "?"
                },
                "model_group": "deepseek"
            },
            {
                "uniqueID": "unsloth/DeepSeek-R1-Distill-Qwen-14B-GGUF",
                "name": "DeepSeek R1 Distill Qwen 14B (GGUF - Q6_K4)",
                "description": "A Q6_K quantized GGUF version of DeepSeek R1 Qwen Distill 14B.",
                "added": "2025-01-27",
                "parameters": "14B",
                "tags": [
                    "Chain of Thought"
                ],
                "context": "131072",
                "architecture": "GGUF",
                "formats": [
                    "GGUF"
                ],
                "huggingface_repo": "unsloth/DeepSeek-R1-Distill-Qwen-14B-GGUF",
                "huggingface_filename": "DeepSeek-R1-Distill-Qwen-14B-Q6_K.gguf",
                "transformers_version": "4.43.1",
                "gated": false,
                "license": "MIT",
                "logo": "https://www.deepseek.com/favicon.ico",
                "size_of_model_in_mb": 12390.4,
                "author": {
                    "name": "unsloth",
                    "url": "hhttps://huggingface.co/unsloth/DeepSeek-R1-Distill-Qwen-14B-GGUF",
                    "blurb": ""
                },
                "resources": {
                    "canonicalUrl": "https://huggingface.co/unsloth/DeepSeek-R1-Distill-Qwen-14B-GGUF",
                    "downloadUrl": "https://huggingface.co/unsloth/DeepSeek-R1-Distill-Qwen-14B-GGUF",
                    "paperUrl": "?"
                },
                "model_group": "deepseek"
            },
            {
                "uniqueID": "mlx-community/DeepSeek-R1-Distill-Qwen-32B-4bit",
                "name": "DeepSeek R1 Distill Qwen 32B (MLX 4bit)",
                "description": "A 4-bit quantized MLX-formatted version of DeeSeek R1 Qwen Distill that is good for running on Apple Silicon.",
                "added": "2025-01-27",
                "parameters": "32B",
                "tags": [
                    "Chain of Thought"
                ],
                "context": "131072",
                "architecture": "Qwen2ForCausalLM",
                "formats": [
                    "Safetensors"
                ],
                "huggingface_repo": "mlx-community/DeepSeek-R1-Distill-Qwen-32B-4bit",
                "transformers_version": "4.43.1",
                "gated": false,
                "license": "MIT",
                "logo": "https://www.deepseek.com/favicon.ico",
                "size_of_model_in_mb": 17588.67,
                "author": {
                    "name": "mlx-community",
                    "url": "https://huggingface.co/mlx-community/DeepSeek-R1-Distill-Qwen-32B-4bit",
                    "blurb": ""
                },
                "resources": {
                    "canonicalUrl": "https://huggingface.co/mlx-community/DeepSeek-R1-Distill-Qwen-32B-4bit",
                    "downloadUrl": "https://huggingface.co/mlx-community/DeepSeek-R1-Distill-Qwen-32B-4bit",
                    "paperUrl": "?"
                },
                "model_group": "deepseek"
            },
            {
                "uniqueID": "deepseek-ai/DeepSeek-R1-Distill-Qwen-7B",
                "name": "DeepSeek R1 Distill Qwen 7B",
                "archived": true,
                "description": "Qwen distillation of DeepSeek chain-of-thought model.",
                "added": "2025-01-27",
                "parameters": "7B",
                "tags": [
                    "Chain of Thought"
                ],
                "context": "131072",
                "architecture": "Qwen2ForCausalLM",
                "formats": [
                    "Safetensors"
                ],
                "huggingface_repo": "deepseek-ai/DeepSeek-R1-Distill-Qwen-7B",
                "transformers_version": "4.44.0",
                "gated": false,
                "license": "MIT",
                "logo": "https://www.deepseek.com/favicon.ico",
                "size_of_model_in_mb": 14532.41,
                "author": {
                    "name": "deepseek-ai",
                    "url": "https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B",
                    "blurb": ""
                },
                "resources": {
                    "canonicalUrl": "https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B",
                    "downloadUrl": "https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B",
                    "paperUrl": "?"
                },
                "model_group": "deepseek"
            },
            {
                "uniqueID": "deepseek-ai/DeepSeek-R1",
                "name": "DeepSeek R1",
                "archived": true,
                "description": "Full version of DeepSeek's flagship Chain-of-Thought model.",
                "added": "2025-01-27",
                "parameters": "671B",
                "tags": [
                    "Chain of Thought"
                ],
                "context": "163840",
                "architecture": "DeepseekV3ForCausalLM",
                "formats": [
                    "Safetensors"
                ],
                "huggingface_repo": "deepseek-ai/DeepSeek-R1",
                "transformers_version": "4.46.3",
                "gated": false,
                "license": "MIT",
                "logo": "https://www.deepseek.com/favicon.ico",
                "size_of_model_in_mb": 656703.53,
                "author": {
                    "name": "deepseek-ai",
                    "url": "https://huggingface.co/deepseek-ai/DeepSeek-R1",
                    "blurb": ""
                },
                "resources": {
                    "canonicalUrl": "https://huggingface.co/deepseek-ai/DeepSeek-R1",
                    "downloadUrl": "https://huggingface.co/deepseek-ai/DeepSeek-R1",
                    "paperUrl": "?"
                },
                "model_group": "deepseek"
            },
            {
                "uniqueID": "deepseek-ai/DeepSeek-V3-0324",
                "name": "DeepSeek V3 0324",
                "description": "March 2025 update to Deepseek's base model that improves in many categories.",
                "added": "2025-03-25",
                "tags": [],
                "parameters": "671B",
                "context": "163840",
                "architecture": "DeepseekV3ForCausalLM",
                "formats": [
                    "Safetensors"
                ],
                "huggingface_repo": "deepseek-ai/DeepSeek-V3-0324",
                "transformers_version": "4.46.3",
                "gated": false,
                "license": "mit",
                "logo": "https://www.deepseek.com/favicon.ico",
                "size_of_model_in_mb": 656703.5347614288,
                "author": {
                    "name": "deepseek-ai",
                    "url": "https://huggingface.co/deepseek-ai/DeepSeek-V3-0324",
                    "blurb": ""
                },
                "resources": {
                    "canonicalUrl": "https://huggingface.co/deepseek-ai/DeepSeek-V3-0324",
                    "downloadUrl": "https://huggingface.co/deepseek-ai/DeepSeek-V3-0324",
                    "paperUrl": "?"
                },
                "model_group": "deepseek"
            }
        ]
    },
    {
        "name": "starcoder",
        "description": "Leading in code-generation. Trained on open-source repositories for development tasks.",
        "tags": [
            "Coding"
        ],
        "models": [
            {
                "uniqueID": "bigcode/starcoder",
                "name": "StarCoder",
                "description": "The StarCoder models are 15.5B parameter models trained on 80+ programming languages from The Stack (v1.2), with opt-out requests excluded. The model uses Multi Query Attention, a context window of 8192 tokens, and was trained using the Fill-in-the-Middle objective on 1 trillion tokens.",
                "archived": true,
                "parameters": "7B",
                "tags": [
                    "Coding"
                ],
                "context": "4k",
                "architecture": "GPTBigCodeForCausalLM",
                "formats": [
                    "Pytorch"
                ],
                "huggingface_repo": "bigcode/starcoder",
                "transformers_version": "4.28.1",
                "gated": "auto",
                "license": "bigcode-openrail-m",
                "logo": "https://www.bigcode-project.org/flow.png",
                "size_of_model_in_mb": 120695.78,
                "author": {
                    "name": "Big Code",
                    "url": "https://www.bigcode-project.org/",
                    "blurb": "BigCode is an open scientific collaboration working on the responsible development and use of large language models for code"
                },
                "resources": {
                    "canonicalUrl": "https://www.bigcode-project.org/",
                    "downloadUrl": "https://huggingface.co/bigcode/starcoder",
                    "paperUrl": "?"
                },
                "model_group": "starcoder"
            }
        ]
    },
    {
        "name": "deepcoder",
        "description": "Neural program synthesis model built to generate and complete code efficiently.",
        "tags": [
            "Coding"
        ],
        "models": [
            {
                "uniqueID": "agentica-org/DeepCoder-14B-Preview",
                "name": "DeepCoder 14B Preview",
                "description": "DeepCoder is a code reasoning LLM fine-tuned from DeepSeek-R1-Distilled-Qwen-14B that achieves similar performance to OpenAI's o3-mini with just 14B parameters.",
                "added": "2025-04-25",
                "tags": [
                    "Coding"
                ],
                "parameters": "14B",
                "context": "131072",
                "architecture": "Qwen2ForCausalLM",
                "formats": [
                    "Safetensors"
                ],
                "huggingface_repo": "agentica-org/DeepCoder-14B-Preview",
                "transformers_version": "4.47.1",
                "gated": false,
                "license": "mit",
                "logo": "https://cdn-avatars.huggingface.co/v1/production/uploads/62088594a5943c8a8fc94560/y5SEKiE8TkjBKs9xfjCx5.png",
                "size_of_model_in_mb": 56354.21689414978,
                "author": {
                    "name": "agentica-org",
                    "url": "https://huggingface.co/agentica-org/DeepCoder-14B-Preview",
                    "blurb": ""
                },
                "resources": {
                    "canonicalUrl": "https://huggingface.co/agentica-org/DeepCoder-14B-Preview",
                    "downloadUrl": "https://huggingface.co/agentica-org/DeepCoder-14B-Preview",
                    "paperUrl": "?"
                },
                "model_group": "deepcoder"
            },
            {
                "uniqueID": "agentica-org/DeepCoder-1.5B-Preview",
                "name": "DeepCoder 1.5B Preview",
                "description": "DeepCoder is a code reasoning LLM fine-tuned from DeepSeek-R1-Distilled-Qwen-1.5B using distributed reinforcement learning (RL) to scale up to long context lengths.",
                "added": "2025-04-25",
                "tags": [
                    "Coding"
                ],
                "parameters": "1.5B",
                "context": "131072",
                "architecture": "Qwen2ForCausalLM",
                "formats": [
                    "Safetensors"
                ],
                "huggingface_repo": "agentica-org/DeepCoder-1.5B-Preview",
                "transformers_version": "4.49.0",
                "gated": false,
                "license": "mit",
                "logo": "https://cdn-avatars.huggingface.co/v1/production/uploads/62088594a5943c8a8fc94560/y5SEKiE8TkjBKs9xfjCx5.png",
                "size_of_model_in_mb": 6790.017298698425,
                "author": {
                    "name": "agentica-org",
                    "url": "https://huggingface.co/agentica-org/DeepCoder-1.5B-Preview",
                    "blurb": ""
                },
                "resources": {
                    "canonicalUrl": "https://huggingface.co/agentica-org/DeepCoder-1.5B-Preview",
                    "downloadUrl": "https://huggingface.co/agentica-org/DeepCoder-1.5B-Preview",
                    "paperUrl": "?"
                },
                "model_group": "deepcoder"
            },
            {
                "uniqueID": "bartowski/agentica-org_DeepCoder-14B-Preview-GGUF",
                "name": "DeepCoder 14B Preview (GGUF Q6_K)",
                "description": "DeepCoder is a code reasoning LLM fine-tuned from DeepSeek-R1-Distilled-Qwen-14B that achieves similar performance to OpenAI's o3-mini with just 14B parameters.",
                "added": "2025-04-25",
                "tags": [
                    "Coding"
                ],
                "parameters": "14B",
                "context": "131072",
                "architecture": "GGUF",
                "formats": [
                    "GGUF"
                ],
                "huggingface_repo": "bartowski/agentica-org_DeepCoder-14B-Preview-GGUF",
                "huggingface_filename": "bartowski/agentica-org_DeepCoder-14B-Preview-GGUF",
                "transformers_version": "4.47.1",
                "gated": false,
                "license": "mit",
                "logo": "https://cdn-avatars.huggingface.co/v1/production/uploads/62088594a5943c8a8fc94560/y5SEKiE8TkjBKs9xfjCx5.png",
                "size_of_model_in_mb": 12390.4,
                "author": {
                    "name": "agentica-org",
                    "url": "https://huggingface.co/agentica-org/",
                    "blurb": ""
                },
                "resources": {
                    "canonicalUrl": "https://huggingface.co/bartowski/agentica-org_DeepCoder-14B-Preview-GGUF",
                    "downloadUrl": "https://huggingface.co/bartowski/agentica-org_DeepCoder-14B-Preview-GGUF",
                    "paperUrl": "?"
                },
                "model_group": "deepcoder"
            },
            {
                "uniqueID": "mlx-community/DeepCoder-14B-Preview-6bit",
                "name": "DeepCoder 14B Preview (MLX 6-bit)",
                "description": "DeepCoder is a code reasoning LLM fine-tuned from DeepSeek-R1-Distilled-Qwen-14B that achieves similar performance to OpenAI's o3-mini with just 14B parameters.",
                "added": "2025-04-25",
                "tags": [
                    "Coding"
                ],
                "parameters": "14B",
                "context": "131072",
                "architecture": "MLX",
                "formats": [
                    "Safetensors"
                ],
                "huggingface_repo": "mlx-community/DeepCoder-14B-Preview-6bit",
                "transformers_version": "4.47.1",
                "gated": false,
                "license": "mit",
                "logo": "https://cdn-avatars.huggingface.co/v1/production/uploads/62088594a5943c8a8fc94560/y5SEKiE8TkjBKs9xfjCx5.png",
                "size_of_model_in_mb": 11456.805171966553,
                "author": {
                    "name": "mlx-community",
                    "url": "https://huggingface.co/mlx-community/DeepCoder-14B-Preview-6bit",
                    "blurb": ""
                },
                "resources": {
                    "canonicalUrl": "https://huggingface.co/mlx-community/DeepCoder-14B-Preview-6bit",
                    "downloadUrl": "https://huggingface.co/mlx-community/DeepCoder-14B-Preview-6bit",
                    "paperUrl": "?"
                },
                "model_group": "deepcoder"
            }
        ]
    },
    {
        "name": "apriel",
        "description": "Compact and high-performance reasoning. Optimized for efficiency and low memory usage.",
        "tags": [
            "Reasoning"
        ],
        "models": [
            {
                "uniqueID": "ServiceNow-AI/Apriel-5B-Instruct",
                "name": "Apriel-5B-Instruct",
                "description": "Apriel is a family of models built for versatility, offering high throughput and efficiency across a wide range of tasks.",
                "added": "2025-04-15",
                "tags": [],
                "parameters": "5B",
                "context": "16384",
                "architecture": "AprielForCausalLM",
                "formats": [
                    "Safetensors"
                ],
                "huggingface_repo": "ServiceNow-AI/Apriel-5B-Instruct",
                "transformers_version": "4.48.3",
                "gated": false,
                "license": "mit",
                "logo": "https://cdn-avatars.huggingface.co/v1/production/uploads/63d3095c2727d7888cbb54e2/Uv-Lx8PVGviqokfOyYlCN.png",
                "size_of_model_in_mb": 9233.023426055908,
                "author": {
                    "name": "ServiceNow-AI",
                    "url": "https://huggingface.co/ServiceNow-AI/Apriel-5B-Instruct",
                    "blurb": ""
                },
                "resources": {
                    "canonicalUrl": "https://huggingface.co/ServiceNow-AI/Apriel-5B-Instruct",
                    "downloadUrl": "https://huggingface.co/ServiceNow-AI/Apriel-5B-Instruct",
                    "paperUrl": "?"
                },
                "model_group": "apriel"
            },
            {
                "uniqueID": "ServiceNow-AI/Apriel-5B-Base",
                "name": "Apriel-5B-Base",
                "description": "Apriel is a family of models built for versatility, offering high throughput and efficiency across a wide range of tasks.",
                "added": "2025-04-15",
                "tags": [],
                "parameters": "5B",
                "context": "16384",
                "architecture": "AprielForCausalLM",
                "formats": [
                    "Safetensors"
                ],
                "huggingface_repo": "ServiceNow-AI/Apriel-5B-Base",
                "transformers_version": "4.48.3",
                "gated": false,
                "license": "mit",
                "logo": "https://cdn-avatars.huggingface.co/v1/production/uploads/63d3095c2727d7888cbb54e2/Uv-Lx8PVGviqokfOyYlCN.png",
                "size_of_model_in_mb": 9233.021151542664,
                "author": {
                    "name": "ServiceNow-AI",
                    "url": "https://huggingface.co/ServiceNow-AI/Apriel-5B-Base",
                    "blurb": ""
                },
                "resources": {
                    "canonicalUrl": "https://huggingface.co/ServiceNow-AI/Apriel-5B-Base",
                    "downloadUrl": "https://huggingface.co/ServiceNow-AI/Apriel-5B-Base",
                    "paperUrl": "?"
                },
                "model_group": "apriel"
            },
            {
                "uniqueID": "ServiceNow-AI/Apriel-Nemotron-15b-Thinker",
                "name": "Apriel Nemotron 15B Thinker",
                "description": "Apriel-Nemotron-15b-Thinker is a 15\u202fbillion\u2011parameter reasoning model in ServiceNow\u2019s Apriel SLM series which achieves competitive performance against similarly sized state-of-the-art models like o1\u2011mini, QWQ\u201132b, and EXAONE\u2011Deep\u201132b, all while maintaining only half the memory footprint of those alternatives. It builds upon the Apriel\u201115b\u2011base checkpoint through a three\u2011stage training pipeline (CPT, SFT and GRPO).",
                "added": "2025-05-07",
                "tags": [],
                "parameters": "15B",
                "context": "65536",
                "architecture": "MistralForCausalLM",
                "formats": [
                    "Safetensors"
                ],
                "huggingface_repo": "ServiceNow-AI/Apriel-Nemotron-15b-Thinker",
                "transformers_version": "4.51.0",
                "gated": false,
                "license": "mit",
                "logo": "https://cdn-avatars.huggingface.co/v1/production/uploads/63d3095c2727d7888cbb54e2/Uv-Lx8PVGviqokfOyYlCN.png",
                "size_of_model_in_mb": 28570.078763961792,
                "author": {
                    "name": "ServiceNow-AI",
                    "url": "https://huggingface.co/ServiceNow-AI/Apriel-Nemotron-15b-Thinker",
                    "blurb": ""
                },
                "resources": {
                    "canonicalUrl": "https://huggingface.co/ServiceNow-AI/Apriel-Nemotron-15b-Thinker",
                    "downloadUrl": "https://huggingface.co/ServiceNow-AI/Apriel-Nemotron-15b-Thinker",
                    "paperUrl": "?"
                },
                "model_group": "apriel"
            }
        ]
    },
    {
        "name": "others",
        "description": "",
        "tags": [],
        "models": [
            {
                "uniqueID": "nvidia/Llama-3.1-Nemotron-Nano-4B-v1.1",
                "name": "Llama-3.1-Nemotron-Nano-4B-v1.1",
                "description": "Offers a great tradeoff between model accuracy and efficiency. The model fits on a single RTX GPU and can be used locally",
                "added": "2025-05-24",
                "tags": [
                    "Reasoning",
                    "RAG"
                ],
                "parameters": "4B",
                "context": "131072",
                "architecture": "LlamaForCausalLM",
                "formats": [
                    "Safetensors"
                ],
                "huggingface_repo": "nvidia/Llama-3.1-Nemotron-Nano-4B-v1.1",
                "transformers_version": "4.47.1",
                "gated": false,
                "license": "other",
                "logo": "https://upload.wikimedia.org/wikipedia/commons/a/ab/Meta-Logo.png",
                "size_of_model_in_mb": 8623.903450965881,
                "author": {
                    "name": "nvidia",
                    "url": "https://huggingface.co/nvidia/Llama-3.1-Nemotron-Nano-4B-v1.1",
                    "blurb": ""
                },
                "resources": {
                    "canonicalUrl": "https://huggingface.co/nvidia/Llama-3.1-Nemotron-Nano-4B-v1.1",
                    "downloadUrl": "https://huggingface.co/nvidia/Llama-3.1-Nemotron-Nano-4B-v1.1",
                    "paperUrl": "?"
                },
                "model_group": "nvidia"
            },
            {
                "uniqueID": "nvidia/Llama-3.1-Nemotron-Nano-8B-v1",
                "name": "Llama-3.1-Nemotron-Nano-8B-v1",
                "description": "Offers a great tradeoff between model accuracy and efficiency. The model fits on a single RTX GPU and can be used locally",
                "added": "2025-05-24",
                "tags": [
                    "Reasoning",
                    "RAG"
                ],
                "parameters": "8B",
                "context": "131072",
                "architecture": "LlamaForCausalLM",
                "formats": [
                    "Safetensors"
                ],
                "huggingface_repo": "nvidia/Llama-3.1-Nemotron-Nano-8B-v1",
                "transformers_version": "4.47.1",
                "gated": false,
                "license": "other",
                "logo": "https://upload.wikimedia.org/wikipedia/commons/a/ab/Meta-Logo.png",
                "size_of_model_in_mb": 15333.027109146118,
                "author": {
                    "name": "nvidia",
                    "url": "https://huggingface.co/nvidia/Llama-3.1-Nemotron-Nano-8B-v1",
                    "blurb": ""
                },
                "resources": {
                    "canonicalUrl": "https://huggingface.co/nvidia/Llama-3.1-Nemotron-Nano-8B-v1",
                    "downloadUrl": "https://huggingface.co/nvidia/Llama-3.1-Nemotron-Nano-8B-v1",
                    "paperUrl": "?"
                },
                "model_group": "nvidia"
            }
        ]
    },
    {
        "name": "zephyr",
        "description": "Hugging Face instruction-tuned model known for clean chat behavior and fast performance.",
        "tags": [],
        "models": [
            {
                "uniqueID": "HuggingFaceH4/zephyr-7b-alpha",
                "name": "Zephyr 7b Alpha",
                "description": "Zephyr is a series of language models that are trained to act as helpful assistants. Zephyr-7B-\u03b1 is the first model in the series, and is a fine-tuned version of mistralai/Mistral-7B-v0.1 that was trained on on a mix of publicly available, synthetic datasets using Direct Preference Optimization (DPO).",
                "parameters": "7B",
                "tags": [
                    "RAG"
                ],
                "context": "4k",
                "architecture": "MistralForCausalLM",
                "formats": [
                    "PyTorch"
                ],
                "huggingface_repo": "HuggingFaceH4/zephyr-7b-alpha",
                "transformers_version": "4.34.0",
                "gated": "auto",
                "license": "MIT",
                "logo": "https://huggingface.co/HuggingFaceH4/zephyr-7b-alpha/resolve/main/thumbnail.png",
                "size_of_model_in_mb": 27627.48,
                "author": {
                    "name": "HuggingFace H4",
                    "url": "https://huggingface.co/HuggingFaceH4/zephyr-7b-alpha",
                    "blurb": ""
                },
                "resources": {
                    "canonicalUrl": "https://huggingface.co/HuggingFaceH4/zephyr-7b-alpha",
                    "downloadUrl": "https://huggingface.co/HuggingFaceH4/zephyr-7b-alpha",
                    "paperUrl": "?"
                },
                "model_group": "zephyr"
            }
        ]
    },
    {
        "name": "reflection",
        "description": "Large-scale, optimized for self-improvement and reflective reasoning.",
        "tags": [
            "Reasoning"
        ],
        "models": [
            {
                "uniqueID": "mattshumer/Reflection-Llama-3.1-70B",
                "name": "Reflection-Llama-3.1-70B",
                "description": "",
                "parameters": "",
                "context": "8192",
                "architecture": "LlamaForCausalLM",
                "formats": [
                    "PyTorch"
                ],
                "huggingface_repo": "mattshumer/Reflection-Llama-3.1-70B",
                "transformers_version": "4.40.0",
                "gated": false,
                "license": "llama3.1",
                "logo": "https://pbs.twimg.com/profile_images/1669714060269461506/9LY7fML7_400x400.jpg",
                "size_of_model_in_mb": 269150.26,
                "author": {
                    "name": "mattshumer",
                    "url": "https://huggingface.co/mattshumer/Reflection-Llama-3.1-70B",
                    "blurb": ""
                },
                "resources": {
                    "canonicalUrl": "https://huggingface.co/mattshumer/Reflection-Llama-3.1-70B",
                    "downloadUrl": "https://huggingface.co/mattshumer/Reflection-Llama-3.1-70B",
                    "paperUrl": "?"
                },
                "model_group": "reflection"
            }
        ]
    },
    {
        "name": "opencoder",
        "description": "Reasoning-enabled code generators optimized for programming assistance.",
        "tags": [
            "Coding",
            "Reasoning"
        ],
        "models": [
            {
                "uniqueID": "infly/OpenCoder-8B-Instruct",
                "name": "OpenCoder 8B Instruct",
                "description": "OpenCoder is pretrained on 2.5 trillion tokens composed of 90% raw code and 10% code-related web data, and supervised finetuned on over 4.5M high-quality SFT examples.",
                "added": "2024-11-19",
                "parameters": "8B",
                "tags": [
                    "Coding"
                ],
                "context": "8192",
                "architecture": "LlamaForCausalLM",
                "formats": [
                    "Safetensors"
                ],
                "huggingface_repo": "infly/OpenCoder-8B-Instruct",
                "transformers_version": "4.37.0",
                "gated": false,
                "license": "other",
                "logo": "https://raw.githubusercontent.com/OpenCoder-llm/opencoder-llm.github.io/refs/heads/main/static/images/opencoder_icon.jpg",
                "size_of_model_in_mb": 14824.05,
                "author": {
                    "name": "infly",
                    "url": "https://huggingface.co/infly/OpenCoder-8B-Instruct",
                    "blurb": ""
                },
                "resources": {
                    "canonicalUrl": "https://huggingface.co/infly/OpenCoder-8B-Instruct",
                    "downloadUrl": "https://huggingface.co/infly/OpenCoder-8B-Instruct",
                    "paperUrl": "?"
                },
                "model_group": "opencoder"
            }
        ]
    },
    {
        "name": "granite",
        "description": "Enterprise-grade. Tailored for secure, reliable, and general-purpose deployments.",
        "tags": [
            "Enterprise"
        ],
        "models": [
            {
                "uniqueID": "ibm-granite/granite-3.3-2b-base",
                "name": "granite-3.3-2b-base",
                "description": "2B parameter base version of IBM's open-source language model.",
                "added": "2025-04-21",
                "tags": [],
                "parameters": "2B",
                "context": "131072",
                "architecture": "GraniteForCausalLM",
                "formats": [
                    "Safetensors"
                ],
                "huggingface_repo": "ibm-granite/granite-3.3-2b-base",
                "transformers_version": "4.46.0",
                "gated": false,
                "license": "apache-2.0",
                "logo": "https://cdn-avatars.huggingface.co/v1/production/uploads/639bcaa2445b133a4e942436/CEW-OjXkRkDNmTxSu8Egh.png",
                "size_of_model_in_mb": 4834.352711677551,
                "author": {
                    "name": "ibm-granite",
                    "url": "https://huggingface.co/ibm-granite/granite-3.3-2b-base",
                    "blurb": ""
                },
                "resources": {
                    "canonicalUrl": "https://huggingface.co/ibm-granite/granite-3.3-2b-base",
                    "downloadUrl": "https://huggingface.co/ibm-granite/granite-3.3-2b-base",
                    "paperUrl": "?"
                },
                "model_group": "granite"
            },
            {
                "uniqueID": "ibm-granite/granite-3.3-2b-instruct",
                "name": "granite-3.3-2b-instruct",
                "description": "2B parameter instruction-tuned version of IBM's open-source language model.",
                "added": "2025-04-21",
                "tags": [],
                "parameters": "2B",
                "context": "131072",
                "architecture": "GraniteForCausalLM",
                "formats": [
                    "Safetensors"
                ],
                "huggingface_repo": "ibm-granite/granite-3.3-2b-instruct",
                "transformers_version": "4.49.0",
                "gated": false,
                "license": "apache-2.0",
                "logo": "https://cdn-avatars.huggingface.co/v1/production/uploads/639bcaa2445b133a4e942436/CEW-OjXkRkDNmTxSu8Egh.png",
                "size_of_model_in_mb": 4836.479970932007,
                "author": {
                    "name": "ibm-granite",
                    "url": "https://huggingface.co/ibm-granite/granite-3.3-2b-instruct",
                    "blurb": ""
                },
                "resources": {
                    "canonicalUrl": "https://huggingface.co/ibm-granite/granite-3.3-2b-instruct",
                    "downloadUrl": "https://huggingface.co/ibm-granite/granite-3.3-2b-instruct",
                    "paperUrl": "?"
                },
                "model_group": "granite"
            },
            {
                "uniqueID": "ibm-granite/granite-3.3-8b-base",
                "name": "granite-3.3-8b-base",
                "description": "8B parameter base version of IBM's open-source language model.",
                "added": "2025-04-21",
                "tags": [],
                "parameters": "8B",
                "context": "131072",
                "architecture": "GraniteForCausalLM",
                "formats": [
                    "Safetensors"
                ],
                "huggingface_repo": "ibm-granite/granite-3.3-8b-base",
                "transformers_version": "4.46.0",
                "gated": false,
                "license": "apache-2.0",
                "logo": "https://cdn-avatars.huggingface.co/v1/production/uploads/639bcaa2445b133a4e942436/CEW-OjXkRkDNmTxSu8Egh.png",
                "size_of_model_in_mb": 15586.66934299469,
                "author": {
                    "name": "ibm-granite",
                    "url": "https://huggingface.co/ibm-granite/granite-3.3-8b-base",
                    "blurb": ""
                },
                "resources": {
                    "canonicalUrl": "https://huggingface.co/ibm-granite/granite-3.3-8b-base",
                    "downloadUrl": "https://huggingface.co/ibm-granite/granite-3.3-8b-base",
                    "paperUrl": "?"
                },
                "model_group": "granite"
            },
            {
                "uniqueID": "ibm-granite/granite-3.3-8b-instruct",
                "name": "granite-3.3-8b-instruct",
                "description": "8B parameter instruction-tuned version of IBM's open-source language model.",
                "added": "2025-04-21",
                "tags": [],
                "parameters": "8B",
                "context": "131072",
                "architecture": "GraniteForCausalLM",
                "formats": [
                    "Safetensors"
                ],
                "huggingface_repo": "ibm-granite/granite-3.3-8b-instruct",
                "transformers_version": "4.49.0",
                "gated": false,
                "license": "apache-2.0",
                "logo": "https://cdn-avatars.huggingface.co/v1/production/uploads/639bcaa2445b133a4e942436/CEW-OjXkRkDNmTxSu8Egh.png",
                "size_of_model_in_mb": 15588.823945999146,
                "author": {
                    "name": "ibm-granite",
                    "url": "https://huggingface.co/ibm-granite/granite-3.3-8b-instruct",
                    "blurb": ""
                },
                "resources": {
                    "canonicalUrl": "https://huggingface.co/ibm-granite/granite-3.3-8b-instruct",
                    "downloadUrl": "https://huggingface.co/ibm-granite/granite-3.3-8b-instruct",
                    "paperUrl": "?"
                },
                "model_group": "granite"
            }
        ]
    },
    {
        "name": "tinyllama",
        "description": "Miniature variant of LLaMA suitable for testing and fast inference scenarios.",
        "tags": [
            "Tiny"
        ],
        "models": [
            {
                "uniqueID": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "name": "Tiny Llama 1.1B Chat",
                "description": "The TinyLlama project aims to pretrain a 1.1B Llama model on 3 trillion tokens. With some proper optimization, we can achieve this within a span of 90 days using 16 A100-40G GPUs \ud83d\ude80\ud83d\ude80. The training has started on 2023-09-01.",
                "parameters": "1.1B",
                "context": "4k",
                "architecture": "LlamaForCausalLM",
                "formats": [
                    "Safetensors"
                ],
                "huggingface_repo": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "transformers_version": "4.35.0",
                "gated": false,
                "license": "Apache 2.0",
                "logo": "https://eu-images.contentstack.com/v3/assets/blt6b0f74e5591baa03/bltf495f117670d330a/65985db56b9c04040d633f56/image.png?width=700&auto=webp&quality=80&disable=upscale",
                "size_of_model_in_mb": 2100.43,
                "author": {
                    "name": "TinyLlama",
                    "url": "https://github.com/jzhang38/TinyLlama",
                    "blurb": ""
                },
                "resources": {
                    "canonicalUrl": "https://github.com/jzhang38/TinyLlama",
                    "downloadUrl": "https://github.com/jzhang38/TinyLlama",
                    "paperUrl": "?"
                },
                "model_group": "tinyllama"
            }
        ]
    }
]